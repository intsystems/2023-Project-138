{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sedov\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:148: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorCompare.cpp:652.)\n",
            "  x_min, x_max = torch._aminmax(y, 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reconstruction for layer conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sedov\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:179: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceAllOps.cpp:66.)\n",
            "  x_min, x_max = torch._aminmax(x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Init alpha to be FP32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sedov\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\layer_recon.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  layer.act_quantizer.delta = torch.nn.Parameter(torch.tensor(layer.act_quantizer.delta))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total loss:\t0.009 (rec:0.009, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.010 (rec:0.010, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.008 (rec:0.008, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t0.009 (rec:0.009, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t0.010 (rec:0.010, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t0.010 (rec:0.010, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t0.012 (rec:0.012, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t84.043 (rec:0.011, round:84.032)\tb=20.00\tcount=4000\n",
            "Total loss:\t36.074 (rec:0.009, round:36.066)\tb=19.44\tcount=4500\n",
            "Total loss:\t33.342 (rec:0.010, round:33.332)\tb=18.88\tcount=5000\n",
            "Total loss:\t31.897 (rec:0.010, round:31.887)\tb=18.31\tcount=5500\n",
            "Total loss:\t30.478 (rec:0.010, round:30.467)\tb=17.75\tcount=6000\n",
            "Total loss:\t29.190 (rec:0.011, round:29.179)\tb=17.19\tcount=6500\n",
            "Total loss:\t27.972 (rec:0.010, round:27.962)\tb=16.62\tcount=7000\n",
            "Total loss:\t26.774 (rec:0.009, round:26.765)\tb=16.06\tcount=7500\n",
            "Total loss:\t25.672 (rec:0.017, round:25.655)\tb=15.50\tcount=8000\n",
            "Total loss:\t24.630 (rec:0.008, round:24.622)\tb=14.94\tcount=8500\n",
            "Total loss:\t23.370 (rec:0.013, round:23.357)\tb=14.38\tcount=9000\n",
            "Total loss:\t22.054 (rec:0.009, round:22.046)\tb=13.81\tcount=9500\n",
            "Total loss:\t20.801 (rec:0.009, round:20.792)\tb=13.25\tcount=10000\n",
            "Total loss:\t19.403 (rec:0.009, round:19.394)\tb=12.69\tcount=10500\n",
            "Total loss:\t17.821 (rec:0.009, round:17.811)\tb=12.12\tcount=11000\n",
            "Total loss:\t16.621 (rec:0.011, round:16.609)\tb=11.56\tcount=11500\n",
            "Total loss:\t15.255 (rec:0.011, round:15.244)\tb=11.00\tcount=12000\n",
            "Total loss:\t13.762 (rec:0.008, round:13.754)\tb=10.44\tcount=12500\n",
            "Total loss:\t12.441 (rec:0.010, round:12.431)\tb=9.88\tcount=13000\n",
            "Total loss:\t10.960 (rec:0.009, round:10.951)\tb=9.31\tcount=13500\n",
            "Total loss:\t9.496 (rec:0.009, round:9.486)\tb=8.75\tcount=14000\n",
            "Total loss:\t7.837 (rec:0.009, round:7.828)\tb=8.19\tcount=14500\n",
            "Total loss:\t6.291 (rec:0.012, round:6.279)\tb=7.62\tcount=15000\n",
            "Total loss:\t4.664 (rec:0.009, round:4.655)\tb=7.06\tcount=15500\n",
            "Total loss:\t2.697 (rec:0.009, round:2.688)\tb=6.50\tcount=16000\n",
            "Total loss:\t1.246 (rec:0.008, round:1.237)\tb=5.94\tcount=16500\n",
            "Total loss:\t0.524 (rec:0.010, round:0.514)\tb=5.38\tcount=17000\n",
            "Total loss:\t0.180 (rec:0.016, round:0.164)\tb=4.81\tcount=17500\n",
            "Total loss:\t0.023 (rec:0.008, round:0.014)\tb=4.25\tcount=18000\n",
            "Total loss:\t0.013 (rec:0.013, round:0.000)\tb=3.69\tcount=18500\n",
            "Total loss:\t0.009 (rec:0.009, round:0.000)\tb=3.12\tcount=19000\n",
            "Total loss:\t0.011 (rec:0.011, round:0.000)\tb=2.56\tcount=19500\n",
            "Total loss:\t0.011 (rec:0.011, round:0.000)\tb=2.00\tcount=20000\n",
            "Reconstruction for block 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sedov\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\block_recon.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  module.act_quantizer.delta = torch.nn.Parameter(torch.tensor(module.act_quantizer.delta))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.104 (rec:0.104, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.119 (rec:0.119, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.138 (rec:0.138, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t0.105 (rec:0.105, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t0.120 (rec:0.120, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t0.101 (rec:0.101, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t0.106 (rec:0.106, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t651.739 (rec:0.103, round:651.636)\tb=20.00\tcount=4000\n",
            "Total loss:\t359.001 (rec:0.107, round:358.893)\tb=19.44\tcount=4500\n",
            "Total loss:\t332.963 (rec:0.107, round:332.856)\tb=18.88\tcount=5000\n",
            "Total loss:\t316.495 (rec:0.114, round:316.381)\tb=18.31\tcount=5500\n",
            "Total loss:\t302.728 (rec:0.103, round:302.625)\tb=17.75\tcount=6000\n",
            "Total loss:\t290.363 (rec:0.106, round:290.257)\tb=17.19\tcount=6500\n",
            "Total loss:\t278.641 (rec:0.096, round:278.544)\tb=16.62\tcount=7000\n",
            "Total loss:\t266.920 (rec:0.103, round:266.817)\tb=16.06\tcount=7500\n",
            "Total loss:\t256.017 (rec:0.116, round:255.901)\tb=15.50\tcount=8000\n",
            "Total loss:\t244.910 (rec:0.100, round:244.810)\tb=14.94\tcount=8500\n",
            "Total loss:\t233.519 (rec:0.110, round:233.409)\tb=14.38\tcount=9000\n",
            "Total loss:\t222.322 (rec:0.112, round:222.210)\tb=13.81\tcount=9500\n",
            "Total loss:\t210.740 (rec:0.099, round:210.641)\tb=13.25\tcount=10000\n",
            "Total loss:\t199.311 (rec:0.108, round:199.203)\tb=12.69\tcount=10500\n",
            "Total loss:\t187.722 (rec:0.107, round:187.615)\tb=12.12\tcount=11000\n",
            "Total loss:\t175.577 (rec:0.106, round:175.471)\tb=11.56\tcount=11500\n",
            "Total loss:\t162.850 (rec:0.111, round:162.739)\tb=11.00\tcount=12000\n",
            "Total loss:\t149.598 (rec:0.110, round:149.488)\tb=10.44\tcount=12500\n",
            "Total loss:\t136.369 (rec:0.137, round:136.232)\tb=9.88\tcount=13000\n",
            "Total loss:\t122.162 (rec:0.118, round:122.043)\tb=9.31\tcount=13500\n",
            "Total loss:\t107.853 (rec:0.103, round:107.750)\tb=8.75\tcount=14000\n",
            "Total loss:\t92.800 (rec:0.121, round:92.679)\tb=8.19\tcount=14500\n",
            "Total loss:\t76.924 (rec:0.118, round:76.806)\tb=7.62\tcount=15000\n",
            "Total loss:\t60.672 (rec:0.141, round:60.531)\tb=7.06\tcount=15500\n",
            "Total loss:\t44.791 (rec:0.108, round:44.682)\tb=6.50\tcount=16000\n",
            "Total loss:\t29.985 (rec:0.113, round:29.872)\tb=5.94\tcount=16500\n",
            "Total loss:\t17.305 (rec:0.123, round:17.182)\tb=5.38\tcount=17000\n",
            "Total loss:\t7.929 (rec:0.110, round:7.819)\tb=4.81\tcount=17500\n",
            "Total loss:\t2.396 (rec:0.149, round:2.247)\tb=4.25\tcount=18000\n",
            "Total loss:\t0.449 (rec:0.148, round:0.301)\tb=3.69\tcount=18500\n",
            "Total loss:\t0.137 (rec:0.126, round:0.011)\tb=3.12\tcount=19000\n",
            "Total loss:\t0.105 (rec:0.105, round:0.000)\tb=2.56\tcount=19500\n",
            "Total loss:\t0.128 (rec:0.128, round:0.000)\tb=2.00\tcount=20000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.305 (rec:0.305, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.321 (rec:0.321, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.295 (rec:0.295, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t0.264 (rec:0.264, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t0.295 (rec:0.295, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t0.265 (rec:0.265, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t0.293 (rec:0.293, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t672.344 (rec:0.260, round:672.084)\tb=20.00\tcount=4000\n",
            "Total loss:\t372.624 (rec:0.269, round:372.355)\tb=19.44\tcount=4500\n",
            "Total loss:\t345.107 (rec:0.272, round:344.835)\tb=18.88\tcount=5000\n",
            "Total loss:\t327.337 (rec:0.284, round:327.053)\tb=18.31\tcount=5500\n",
            "Total loss:\t312.556 (rec:0.271, round:312.285)\tb=17.75\tcount=6000\n",
            "Total loss:\t299.676 (rec:0.270, round:299.406)\tb=17.19\tcount=6500\n",
            "Total loss:\t287.475 (rec:0.264, round:287.211)\tb=16.62\tcount=7000\n",
            "Total loss:\t276.159 (rec:0.278, round:275.881)\tb=16.06\tcount=7500\n",
            "Total loss:\t264.765 (rec:0.301, round:264.464)\tb=15.50\tcount=8000\n",
            "Total loss:\t253.203 (rec:0.284, round:252.919)\tb=14.94\tcount=8500\n",
            "Total loss:\t241.893 (rec:0.373, round:241.521)\tb=14.38\tcount=9000\n",
            "Total loss:\t230.139 (rec:0.266, round:229.873)\tb=13.81\tcount=9500\n",
            "Total loss:\t219.036 (rec:0.285, round:218.751)\tb=13.25\tcount=10000\n",
            "Total loss:\t206.864 (rec:0.308, round:206.555)\tb=12.69\tcount=10500\n",
            "Total loss:\t194.114 (rec:0.272, round:193.842)\tb=12.12\tcount=11000\n",
            "Total loss:\t181.364 (rec:0.288, round:181.076)\tb=11.56\tcount=11500\n",
            "Total loss:\t167.956 (rec:0.314, round:167.642)\tb=11.00\tcount=12000\n",
            "Total loss:\t154.288 (rec:0.272, round:154.016)\tb=10.44\tcount=12500\n",
            "Total loss:\t140.449 (rec:0.272, round:140.178)\tb=9.88\tcount=13000\n",
            "Total loss:\t125.436 (rec:0.285, round:125.151)\tb=9.31\tcount=13500\n",
            "Total loss:\t110.307 (rec:0.297, round:110.010)\tb=8.75\tcount=14000\n",
            "Total loss:\t94.497 (rec:0.298, round:94.199)\tb=8.19\tcount=14500\n",
            "Total loss:\t78.045 (rec:0.291, round:77.754)\tb=7.62\tcount=15000\n",
            "Total loss:\t61.009 (rec:0.293, round:60.715)\tb=7.06\tcount=15500\n",
            "Total loss:\t45.587 (rec:0.294, round:45.292)\tb=6.50\tcount=16000\n",
            "Total loss:\t31.253 (rec:0.285, round:30.968)\tb=5.94\tcount=16500\n",
            "Total loss:\t18.687 (rec:0.286, round:18.401)\tb=5.38\tcount=17000\n",
            "Total loss:\t8.754 (rec:0.364, round:8.390)\tb=4.81\tcount=17500\n",
            "Total loss:\t2.957 (rec:0.314, round:2.643)\tb=4.25\tcount=18000\n",
            "Total loss:\t0.743 (rec:0.291, round:0.452)\tb=3.69\tcount=18500\n",
            "Total loss:\t0.314 (rec:0.295, round:0.020)\tb=3.12\tcount=19000\n",
            "Total loss:\t0.291 (rec:0.291, round:0.000)\tb=2.56\tcount=19500\n",
            "Total loss:\t0.300 (rec:0.300, round:0.000)\tb=2.00\tcount=20000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.147 (rec:0.147, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.143 (rec:0.143, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.138 (rec:0.138, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t0.140 (rec:0.140, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t0.147 (rec:0.147, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t0.145 (rec:0.145, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t0.139 (rec:0.139, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t2128.678 (rec:0.140, round:2128.539)\tb=20.00\tcount=4000\n",
            "Total loss:\t1094.638 (rec:0.155, round:1094.483)\tb=19.44\tcount=4500\n",
            "Total loss:\t1008.881 (rec:0.151, round:1008.730)\tb=18.88\tcount=5000\n",
            "Total loss:\t956.184 (rec:0.147, round:956.037)\tb=18.31\tcount=5500\n",
            "Total loss:\t912.155 (rec:0.143, round:912.012)\tb=17.75\tcount=6000\n",
            "Total loss:\t873.364 (rec:0.147, round:873.217)\tb=17.19\tcount=6500\n",
            "Total loss:\t835.715 (rec:0.165, round:835.550)\tb=16.62\tcount=7000\n",
            "Total loss:\t800.015 (rec:0.153, round:799.862)\tb=16.06\tcount=7500\n",
            "Total loss:\t765.414 (rec:0.150, round:765.264)\tb=15.50\tcount=8000\n",
            "Total loss:\t730.042 (rec:0.159, round:729.884)\tb=14.94\tcount=8500\n",
            "Total loss:\t694.933 (rec:0.150, round:694.783)\tb=14.38\tcount=9000\n",
            "Total loss:\t659.335 (rec:0.146, round:659.189)\tb=13.81\tcount=9500\n",
            "Total loss:\t622.893 (rec:0.150, round:622.743)\tb=13.25\tcount=10000\n",
            "Total loss:\t585.572 (rec:0.159, round:585.413)\tb=12.69\tcount=10500\n",
            "Total loss:\t546.573 (rec:0.159, round:546.414)\tb=12.12\tcount=11000\n",
            "Total loss:\t506.802 (rec:0.152, round:506.650)\tb=11.56\tcount=11500\n",
            "Total loss:\t465.558 (rec:0.143, round:465.415)\tb=11.00\tcount=12000\n",
            "Total loss:\t422.438 (rec:0.152, round:422.286)\tb=10.44\tcount=12500\n",
            "Total loss:\t377.536 (rec:0.152, round:377.384)\tb=9.88\tcount=13000\n",
            "Total loss:\t331.673 (rec:0.157, round:331.516)\tb=9.31\tcount=13500\n",
            "Total loss:\t284.133 (rec:0.172, round:283.962)\tb=8.75\tcount=14000\n",
            "Total loss:\t236.435 (rec:0.157, round:236.279)\tb=8.19\tcount=14500\n",
            "Total loss:\t189.606 (rec:0.163, round:189.443)\tb=7.62\tcount=15000\n",
            "Total loss:\t142.834 (rec:0.157, round:142.677)\tb=7.06\tcount=15500\n",
            "Total loss:\t99.683 (rec:0.158, round:99.524)\tb=6.50\tcount=16000\n",
            "Total loss:\t61.442 (rec:0.156, round:61.286)\tb=5.94\tcount=16500\n",
            "Total loss:\t31.131 (rec:0.181, round:30.951)\tb=5.38\tcount=17000\n",
            "Total loss:\t9.873 (rec:0.168, round:9.705)\tb=4.81\tcount=17500\n",
            "Total loss:\t1.579 (rec:0.173, round:1.406)\tb=4.25\tcount=18000\n",
            "Total loss:\t0.320 (rec:0.173, round:0.147)\tb=3.69\tcount=18500\n",
            "Total loss:\t0.182 (rec:0.169, round:0.013)\tb=3.12\tcount=19000\n",
            "Total loss:\t0.178 (rec:0.178, round:0.000)\tb=2.56\tcount=19500\n",
            "Total loss:\t0.171 (rec:0.171, round:0.000)\tb=2.00\tcount=20000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.272 (rec:0.272, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.266 (rec:0.266, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.290 (rec:0.290, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t0.263 (rec:0.263, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t0.272 (rec:0.272, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t0.279 (rec:0.279, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t0.273 (rec:0.273, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t2721.329 (rec:0.273, round:2721.056)\tb=20.00\tcount=4000\n",
            "Total loss:\t1403.261 (rec:0.284, round:1402.977)\tb=19.44\tcount=4500\n",
            "Total loss:\t1298.078 (rec:0.293, round:1297.785)\tb=18.88\tcount=5000\n",
            "Total loss:\t1232.246 (rec:0.286, round:1231.960)\tb=18.31\tcount=5500\n",
            "Total loss:\t1178.278 (rec:0.272, round:1178.006)\tb=17.75\tcount=6000\n",
            "Total loss:\t1129.434 (rec:0.273, round:1129.161)\tb=17.19\tcount=6500\n",
            "Total loss:\t1083.137 (rec:0.288, round:1082.849)\tb=16.62\tcount=7000\n",
            "Total loss:\t1038.512 (rec:0.279, round:1038.233)\tb=16.06\tcount=7500\n",
            "Total loss:\t994.419 (rec:0.277, round:994.143)\tb=15.50\tcount=8000\n",
            "Total loss:\t951.675 (rec:0.290, round:951.385)\tb=14.94\tcount=8500\n",
            "Total loss:\t908.141 (rec:0.259, round:907.882)\tb=14.38\tcount=9000\n",
            "Total loss:\t864.062 (rec:0.276, round:863.786)\tb=13.81\tcount=9500\n",
            "Total loss:\t817.374 (rec:0.299, round:817.075)\tb=13.25\tcount=10000\n",
            "Total loss:\t770.385 (rec:0.281, round:770.104)\tb=12.69\tcount=10500\n",
            "Total loss:\t722.303 (rec:0.290, round:722.012)\tb=12.12\tcount=11000\n",
            "Total loss:\t672.712 (rec:0.310, round:672.402)\tb=11.56\tcount=11500\n",
            "Total loss:\t620.939 (rec:0.277, round:620.662)\tb=11.00\tcount=12000\n",
            "Total loss:\t568.450 (rec:0.287, round:568.163)\tb=10.44\tcount=12500\n",
            "Total loss:\t514.200 (rec:0.301, round:513.899)\tb=9.88\tcount=13000\n",
            "Total loss:\t458.548 (rec:0.284, round:458.264)\tb=9.31\tcount=13500\n",
            "Total loss:\t399.316 (rec:0.292, round:399.024)\tb=8.75\tcount=14000\n",
            "Total loss:\t339.032 (rec:0.286, round:338.746)\tb=8.19\tcount=14500\n",
            "Total loss:\t278.116 (rec:0.281, round:277.836)\tb=7.62\tcount=15000\n",
            "Total loss:\t216.902 (rec:0.284, round:216.617)\tb=7.06\tcount=15500\n",
            "Total loss:\t156.532 (rec:0.295, round:156.237)\tb=6.50\tcount=16000\n",
            "Total loss:\t100.137 (rec:0.279, round:99.858)\tb=5.94\tcount=16500\n",
            "Total loss:\t51.357 (rec:0.284, round:51.073)\tb=5.38\tcount=17000\n",
            "Total loss:\t16.352 (rec:0.288, round:16.064)\tb=4.81\tcount=17500\n",
            "Total loss:\t2.621 (rec:0.279, round:2.343)\tb=4.25\tcount=18000\n",
            "Total loss:\t0.490 (rec:0.308, round:0.182)\tb=3.69\tcount=18500\n",
            "Total loss:\t0.313 (rec:0.305, round:0.008)\tb=3.12\tcount=19000\n",
            "Total loss:\t0.286 (rec:0.286, round:0.000)\tb=2.56\tcount=19500\n",
            "Total loss:\t0.298 (rec:0.298, round:0.000)\tb=2.00\tcount=20000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.179 (rec:0.179, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.167 (rec:0.167, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.171 (rec:0.171, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t0.174 (rec:0.174, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t0.152 (rec:0.152, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t0.160 (rec:0.160, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t0.163 (rec:0.163, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t8571.141 (rec:0.167, round:8570.974)\tb=20.00\tcount=4000\n",
            "Total loss:\t4278.250 (rec:0.171, round:4278.079)\tb=19.44\tcount=4500\n",
            "Total loss:\t3966.366 (rec:0.164, round:3966.202)\tb=18.88\tcount=5000\n",
            "Total loss:\t3772.732 (rec:0.174, round:3772.558)\tb=18.31\tcount=5500\n",
            "Total loss:\t3610.740 (rec:0.168, round:3610.572)\tb=17.75\tcount=6000\n",
            "Total loss:\t3463.945 (rec:0.165, round:3463.780)\tb=17.19\tcount=6500\n",
            "Total loss:\t3324.627 (rec:0.177, round:3324.450)\tb=16.62\tcount=7000\n",
            "Total loss:\t3190.554 (rec:0.163, round:3190.391)\tb=16.06\tcount=7500\n",
            "Total loss:\t3057.828 (rec:0.167, round:3057.661)\tb=15.50\tcount=8000\n",
            "Total loss:\t2923.508 (rec:0.157, round:2923.350)\tb=14.94\tcount=8500\n",
            "Total loss:\t2790.044 (rec:0.163, round:2789.881)\tb=14.38\tcount=9000\n",
            "Total loss:\t2653.975 (rec:0.169, round:2653.806)\tb=13.81\tcount=9500\n",
            "Total loss:\t2515.706 (rec:0.174, round:2515.532)\tb=13.25\tcount=10000\n",
            "Total loss:\t2374.134 (rec:0.173, round:2373.961)\tb=12.69\tcount=10500\n",
            "Total loss:\t2225.152 (rec:0.168, round:2224.985)\tb=12.12\tcount=11000\n",
            "Total loss:\t2074.261 (rec:0.171, round:2074.090)\tb=11.56\tcount=11500\n",
            "Total loss:\t1917.945 (rec:0.164, round:1917.781)\tb=11.00\tcount=12000\n",
            "Total loss:\t1758.714 (rec:0.178, round:1758.536)\tb=10.44\tcount=12500\n",
            "Total loss:\t1591.237 (rec:0.167, round:1591.070)\tb=9.88\tcount=13000\n",
            "Total loss:\t1420.394 (rec:0.173, round:1420.221)\tb=9.31\tcount=13500\n",
            "Total loss:\t1245.639 (rec:0.176, round:1245.463)\tb=8.75\tcount=14000\n",
            "Total loss:\t1067.469 (rec:0.197, round:1067.272)\tb=8.19\tcount=14500\n",
            "Total loss:\t885.221 (rec:0.176, round:885.045)\tb=7.62\tcount=15000\n",
            "Total loss:\t699.694 (rec:0.174, round:699.520)\tb=7.06\tcount=15500\n",
            "Total loss:\t517.782 (rec:0.172, round:517.611)\tb=6.50\tcount=16000\n",
            "Total loss:\t343.030 (rec:0.171, round:342.859)\tb=5.94\tcount=16500\n",
            "Total loss:\t176.258 (rec:0.177, round:176.081)\tb=5.38\tcount=17000\n",
            "Total loss:\t50.766 (rec:0.195, round:50.571)\tb=4.81\tcount=17500\n",
            "Total loss:\t5.193 (rec:0.194, round:5.000)\tb=4.25\tcount=18000\n",
            "Total loss:\t0.488 (rec:0.188, round:0.299)\tb=3.69\tcount=18500\n",
            "Total loss:\t0.215 (rec:0.184, round:0.031)\tb=3.12\tcount=19000\n",
            "Total loss:\t0.184 (rec:0.182, round:0.002)\tb=2.56\tcount=19500\n",
            "Total loss:\t0.193 (rec:0.193, round:0.000)\tb=2.00\tcount=20000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.277 (rec:0.277, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.248 (rec:0.248, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.269 (rec:0.269, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t0.260 (rec:0.260, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t0.252 (rec:0.252, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t0.255 (rec:0.255, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t0.248 (rec:0.248, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t11010.741 (rec:0.246, round:11010.495)\tb=20.00\tcount=4000\n",
            "Total loss:\t5498.102 (rec:0.275, round:5497.827)\tb=19.44\tcount=4500\n",
            "Total loss:\t5107.017 (rec:0.282, round:5106.735)\tb=18.88\tcount=5000\n",
            "Total loss:\t4861.785 (rec:0.244, round:4861.541)\tb=18.31\tcount=5500\n",
            "Total loss:\t4657.282 (rec:0.240, round:4657.042)\tb=17.75\tcount=6000\n",
            "Total loss:\t4470.661 (rec:0.237, round:4470.423)\tb=17.19\tcount=6500\n",
            "Total loss:\t4292.249 (rec:0.243, round:4292.006)\tb=16.62\tcount=7000\n",
            "Total loss:\t4118.010 (rec:0.251, round:4117.759)\tb=16.06\tcount=7500\n",
            "Total loss:\t3945.738 (rec:0.246, round:3945.492)\tb=15.50\tcount=8000\n",
            "Total loss:\t3769.699 (rec:0.238, round:3769.461)\tb=14.94\tcount=8500\n",
            "Total loss:\t3593.066 (rec:0.257, round:3592.809)\tb=14.38\tcount=9000\n",
            "Total loss:\t3417.020 (rec:0.251, round:3416.769)\tb=13.81\tcount=9500\n",
            "Total loss:\t3235.229 (rec:0.280, round:3234.950)\tb=13.25\tcount=10000\n",
            "Total loss:\t3050.199 (rec:0.273, round:3049.926)\tb=12.69\tcount=10500\n",
            "Total loss:\t2863.933 (rec:0.249, round:2863.684)\tb=12.12\tcount=11000\n",
            "Total loss:\t2671.858 (rec:0.257, round:2671.601)\tb=11.56\tcount=11500\n",
            "Total loss:\t2473.989 (rec:0.278, round:2473.711)\tb=11.00\tcount=12000\n",
            "Total loss:\t2272.110 (rec:0.260, round:2271.851)\tb=10.44\tcount=12500\n",
            "Total loss:\t2064.539 (rec:0.260, round:2064.279)\tb=9.88\tcount=13000\n",
            "Total loss:\t1854.326 (rec:0.281, round:1854.045)\tb=9.31\tcount=13500\n",
            "Total loss:\t1636.541 (rec:0.235, round:1636.306)\tb=8.75\tcount=14000\n",
            "Total loss:\t1415.695 (rec:0.257, round:1415.438)\tb=8.19\tcount=14500\n",
            "Total loss:\t1191.300 (rec:0.262, round:1191.038)\tb=7.62\tcount=15000\n",
            "Total loss:\t965.138 (rec:0.240, round:964.897)\tb=7.06\tcount=15500\n",
            "Total loss:\t744.666 (rec:0.246, round:744.420)\tb=6.50\tcount=16000\n",
            "Total loss:\t522.430 (rec:0.254, round:522.176)\tb=5.94\tcount=16500\n",
            "Total loss:\t301.731 (rec:0.261, round:301.470)\tb=5.38\tcount=17000\n",
            "Total loss:\t105.629 (rec:0.256, round:105.373)\tb=4.81\tcount=17500\n",
            "Total loss:\t14.854 (rec:0.296, round:14.558)\tb=4.25\tcount=18000\n",
            "Total loss:\t1.294 (rec:0.261, round:1.034)\tb=3.69\tcount=18500\n",
            "Total loss:\t0.359 (rec:0.262, round:0.097)\tb=3.12\tcount=19000\n",
            "Total loss:\t0.275 (rec:0.267, round:0.008)\tb=2.56\tcount=19500\n",
            "Total loss:\t0.271 (rec:0.271, round:0.000)\tb=2.00\tcount=20000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.380 (rec:0.380, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.359 (rec:0.359, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.377 (rec:0.377, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t0.348 (rec:0.348, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t0.328 (rec:0.328, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t0.316 (rec:0.316, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t0.308 (rec:0.308, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t34232.207 (rec:0.298, round:34231.910)\tb=20.00\tcount=4000\n",
            "Total loss:\t17291.111 (rec:0.324, round:17290.787)\tb=19.44\tcount=4500\n",
            "Total loss:\t16065.154 (rec:0.314, round:16064.841)\tb=18.88\tcount=5000\n",
            "Total loss:\t15287.219 (rec:0.333, round:15286.886)\tb=18.31\tcount=5500\n",
            "Total loss:\t14631.208 (rec:0.303, round:14630.905)\tb=17.75\tcount=6000\n",
            "Total loss:\t14025.353 (rec:0.319, round:14025.033)\tb=17.19\tcount=6500\n",
            "Total loss:\t13439.245 (rec:0.290, round:13438.955)\tb=16.62\tcount=7000\n",
            "Total loss:\t12865.412 (rec:0.285, round:12865.127)\tb=16.06\tcount=7500\n",
            "Total loss:\t12296.141 (rec:0.287, round:12295.854)\tb=15.50\tcount=8000\n",
            "Total loss:\t11725.164 (rec:0.279, round:11724.885)\tb=14.94\tcount=8500\n",
            "Total loss:\t11150.687 (rec:0.292, round:11150.395)\tb=14.38\tcount=9000\n",
            "Total loss:\t10571.399 (rec:0.285, round:10571.114)\tb=13.81\tcount=9500\n",
            "Total loss:\t9990.560 (rec:0.288, round:9990.271)\tb=13.25\tcount=10000\n",
            "Total loss:\t9402.646 (rec:0.295, round:9402.351)\tb=12.69\tcount=10500\n",
            "Total loss:\t8800.641 (rec:0.301, round:8800.340)\tb=12.12\tcount=11000\n",
            "Total loss:\t8186.500 (rec:0.269, round:8186.231)\tb=11.56\tcount=11500\n",
            "Total loss:\t7565.418 (rec:0.304, round:7565.115)\tb=11.00\tcount=12000\n",
            "Total loss:\t6936.305 (rec:0.280, round:6936.025)\tb=10.44\tcount=12500\n",
            "Total loss:\t6299.674 (rec:0.285, round:6299.389)\tb=9.88\tcount=13000\n",
            "Total loss:\t5651.280 (rec:0.284, round:5650.995)\tb=9.31\tcount=13500\n",
            "Total loss:\t4996.313 (rec:0.284, round:4996.029)\tb=8.75\tcount=14000\n",
            "Total loss:\t4339.271 (rec:0.295, round:4338.976)\tb=8.19\tcount=14500\n",
            "Total loss:\t3683.874 (rec:0.285, round:3683.589)\tb=7.62\tcount=15000\n",
            "Total loss:\t3029.244 (rec:0.304, round:3028.940)\tb=7.06\tcount=15500\n",
            "Total loss:\t2383.690 (rec:0.266, round:2383.424)\tb=6.50\tcount=16000\n",
            "Total loss:\t1751.644 (rec:0.280, round:1751.364)\tb=5.94\tcount=16500\n",
            "Total loss:\t1127.842 (rec:0.336, round:1127.506)\tb=5.38\tcount=17000\n",
            "Total loss:\t516.630 (rec:0.308, round:516.322)\tb=4.81\tcount=17500\n",
            "Total loss:\t108.529 (rec:0.323, round:108.206)\tb=4.25\tcount=18000\n",
            "Total loss:\t10.311 (rec:0.316, round:9.995)\tb=3.69\tcount=18500\n",
            "Total loss:\t0.783 (rec:0.327, round:0.456)\tb=3.12\tcount=19000\n",
            "Total loss:\t0.303 (rec:0.290, round:0.012)\tb=2.56\tcount=19500\n",
            "Total loss:\t0.309 (rec:0.309, round:0.000)\tb=2.00\tcount=20000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t34.983 (rec:34.983, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t32.591 (rec:32.591, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t27.543 (rec:27.543, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t25.583 (rec:25.583, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t29.458 (rec:29.458, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t27.745 (rec:27.745, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t24.171 (rec:24.171, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t44261.395 (rec:26.419, round:44234.977)\tb=20.00\tcount=4000\n",
            "Total loss:\t29579.746 (rec:24.698, round:29555.047)\tb=19.44\tcount=4500\n",
            "Total loss:\t27842.035 (rec:25.183, round:27816.852)\tb=18.88\tcount=5000\n",
            "Total loss:\t26790.936 (rec:24.935, round:26766.000)\tb=18.31\tcount=5500\n",
            "Total loss:\t25961.600 (rec:26.006, round:25935.594)\tb=17.75\tcount=6000\n",
            "Total loss:\t25227.348 (rec:22.623, round:25204.725)\tb=17.19\tcount=6500\n",
            "Total loss:\t24542.891 (rec:21.694, round:24521.197)\tb=16.62\tcount=7000\n",
            "Total loss:\t23877.568 (rec:20.460, round:23857.109)\tb=16.06\tcount=7500\n",
            "Total loss:\t23232.432 (rec:26.967, round:23205.465)\tb=15.50\tcount=8000\n",
            "Total loss:\t22579.152 (rec:24.304, round:22554.850)\tb=14.94\tcount=8500\n",
            "Total loss:\t21926.334 (rec:21.314, round:21905.020)\tb=14.38\tcount=9000\n",
            "Total loss:\t21263.824 (rec:23.752, round:21240.072)\tb=13.81\tcount=9500\n",
            "Total loss:\t20588.488 (rec:21.457, round:20567.031)\tb=13.25\tcount=10000\n",
            "Total loss:\t19899.715 (rec:22.462, round:19877.254)\tb=12.69\tcount=10500\n",
            "Total loss:\t19189.943 (rec:24.314, round:19165.629)\tb=12.12\tcount=11000\n",
            "Total loss:\t18448.252 (rec:22.275, round:18425.977)\tb=11.56\tcount=11500\n",
            "Total loss:\t17683.252 (rec:25.701, round:17657.551)\tb=11.00\tcount=12000\n",
            "Total loss:\t16891.609 (rec:23.514, round:16868.096)\tb=10.44\tcount=12500\n",
            "Total loss:\t16064.593 (rec:27.198, round:16037.396)\tb=9.88\tcount=13000\n",
            "Total loss:\t15186.924 (rec:23.728, round:15163.195)\tb=9.31\tcount=13500\n",
            "Total loss:\t14264.614 (rec:21.464, round:14243.150)\tb=8.75\tcount=14000\n",
            "Total loss:\t13295.271 (rec:20.606, round:13274.666)\tb=8.19\tcount=14500\n",
            "Total loss:\t12276.574 (rec:18.934, round:12257.640)\tb=7.62\tcount=15000\n",
            "Total loss:\t11198.477 (rec:24.446, round:11174.030)\tb=7.06\tcount=15500\n",
            "Total loss:\t10045.688 (rec:21.378, round:10024.310)\tb=6.50\tcount=16000\n",
            "Total loss:\t8826.071 (rec:22.617, round:8803.454)\tb=5.94\tcount=16500\n",
            "Total loss:\t7529.432 (rec:22.270, round:7507.161)\tb=5.38\tcount=17000\n",
            "Total loss:\t6159.047 (rec:22.095, round:6136.952)\tb=4.81\tcount=17500\n",
            "Total loss:\t4742.723 (rec:25.068, round:4717.656)\tb=4.25\tcount=18000\n",
            "Total loss:\t3282.990 (rec:25.887, round:3257.103)\tb=3.69\tcount=18500\n",
            "Total loss:\t1844.024 (rec:23.980, round:1820.044)\tb=3.12\tcount=19000\n",
            "Total loss:\t667.783 (rec:22.162, round:645.621)\tb=2.56\tcount=19500\n",
            "Total loss:\t144.656 (rec:25.286, round:119.370)\tb=2.00\tcount=20000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t15.754 (rec:15.754, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t17.519 (rec:17.519, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t15.040 (rec:15.040, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t15.096 (rec:15.096, round:0.000)\tb=0.00\tcount=2000\n",
            "Total loss:\t15.717 (rec:15.717, round:0.000)\tb=0.00\tcount=2500\n",
            "Total loss:\t13.231 (rec:13.231, round:0.000)\tb=0.00\tcount=3000\n",
            "Total loss:\t13.771 (rec:13.771, round:0.000)\tb=0.00\tcount=3500\n",
            "Total loss:\t4776.778 (rec:14.778, round:4762.000)\tb=20.00\tcount=4000\n",
            "Total loss:\t2967.240 (rec:16.730, round:2950.510)\tb=19.44\tcount=4500\n",
            "Total loss:\t2776.345 (rec:15.463, round:2760.882)\tb=18.88\tcount=5000\n",
            "Total loss:\t2655.924 (rec:14.384, round:2641.540)\tb=18.31\tcount=5500\n",
            "Total loss:\t2557.933 (rec:15.733, round:2542.200)\tb=17.75\tcount=6000\n",
            "Total loss:\t2464.083 (rec:13.513, round:2450.570)\tb=17.19\tcount=6500\n",
            "Total loss:\t2377.103 (rec:14.202, round:2362.901)\tb=16.62\tcount=7000\n",
            "Total loss:\t2292.432 (rec:16.129, round:2276.303)\tb=16.06\tcount=7500\n",
            "Total loss:\t2206.521 (rec:15.250, round:2191.272)\tb=15.50\tcount=8000\n",
            "Total loss:\t2121.141 (rec:16.413, round:2104.729)\tb=14.94\tcount=8500\n",
            "Total loss:\t2033.846 (rec:16.153, round:2017.693)\tb=14.38\tcount=9000\n",
            "Total loss:\t1944.714 (rec:16.401, round:1928.313)\tb=13.81\tcount=9500\n",
            "Total loss:\t1853.539 (rec:14.567, round:1838.972)\tb=13.25\tcount=10000\n",
            "Total loss:\t1764.769 (rec:17.316, round:1747.453)\tb=12.69\tcount=10500\n",
            "Total loss:\t1668.079 (rec:13.340, round:1654.739)\tb=12.12\tcount=11000\n",
            "Total loss:\t1577.491 (rec:15.903, round:1561.588)\tb=11.56\tcount=11500\n",
            "Total loss:\t1481.720 (rec:14.398, round:1467.323)\tb=11.00\tcount=12000\n",
            "Total loss:\t1386.223 (rec:14.144, round:1372.079)\tb=10.44\tcount=12500\n",
            "Total loss:\t1289.927 (rec:15.398, round:1274.529)\tb=9.88\tcount=13000\n",
            "Total loss:\t1189.219 (rec:12.444, round:1176.775)\tb=9.31\tcount=13500\n",
            "Total loss:\t1089.745 (rec:15.356, round:1074.389)\tb=8.75\tcount=14000\n",
            "Total loss:\t987.334 (rec:15.565, round:971.770)\tb=8.19\tcount=14500\n",
            "Total loss:\t883.110 (rec:15.813, round:867.297)\tb=7.62\tcount=15000\n",
            "Total loss:\t777.640 (rec:15.540, round:762.099)\tb=7.06\tcount=15500\n",
            "Total loss:\t669.685 (rec:14.094, round:655.592)\tb=6.50\tcount=16000\n",
            "Total loss:\t561.579 (rec:12.113, round:549.465)\tb=5.94\tcount=16500\n",
            "Total loss:\t459.086 (rec:15.295, round:443.791)\tb=5.38\tcount=17000\n",
            "Total loss:\t352.801 (rec:13.132, round:339.669)\tb=4.81\tcount=17500\n",
            "Total loss:\t256.666 (rec:17.317, round:239.348)\tb=4.25\tcount=18000\n",
            "Total loss:\t160.232 (rec:13.750, round:146.483)\tb=3.69\tcount=18500\n",
            "Total loss:\t81.865 (rec:16.189, round:65.677)\tb=3.12\tcount=19000\n",
            "Total loss:\t30.897 (rec:16.159, round:14.738)\tb=2.56\tcount=19500\n",
            "Total loss:\t14.794 (rec:13.623, round:1.171)\tb=2.00\tcount=20000\n",
            "Test: [  0/782]\tTime  8.516 ( 8.516)\tAcc@1  75.00 ( 75.00)\tAcc@5  85.94 ( 85.94)\n",
            "Test: [100/782]\tTime  0.063 ( 0.214)\tAcc@1  73.44 ( 68.21)\tAcc@5  87.50 ( 88.51)\n",
            "Test: [200/782]\tTime  0.061 ( 0.181)\tAcc@1  64.06 ( 69.03)\tAcc@5  87.50 ( 88.77)\n",
            "Test: [300/782]\tTime  0.059 ( 0.169)\tAcc@1  68.75 ( 68.58)\tAcc@5  87.50 ( 88.62)\n",
            "Test: [400/782]\tTime  0.065 ( 0.163)\tAcc@1  75.00 ( 68.71)\tAcc@5  92.19 ( 88.81)\n",
            "Test: [500/782]\tTime  0.062 ( 0.159)\tAcc@1  64.06 ( 68.99)\tAcc@5  84.38 ( 88.87)\n",
            "Test: [600/782]\tTime  0.060 ( 0.156)\tAcc@1  75.00 ( 68.93)\tAcc@5  87.50 ( 88.94)\n",
            "Test: [700/782]\tTime  0.059 ( 0.153)\tAcc@1  65.62 ( 69.05)\tAcc@5  89.06 ( 89.02)\n",
            " * Acc@1 69.004 Acc@5 88.986\n",
            "Full quantization (W4A4) accuracy: 69.00399780273438\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC \\\n",
        "--arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --keep_cpu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Обычная квантизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.244 ( 7.244)\tAcc@1  53.12 ( 53.12)\tAcc@5  71.88 ( 71.88)\n",
            "Test: [100/782]\tTime  0.227 ( 0.176)\tAcc@1  50.00 ( 48.72)\tAcc@5  67.19 ( 73.16)\n",
            "Test: [200/782]\tTime  0.176 ( 0.141)\tAcc@1  50.00 ( 49.21)\tAcc@5  75.00 ( 73.65)\n",
            "Test: [300/782]\tTime  0.199 ( 0.130)\tAcc@1  50.00 ( 49.14)\tAcc@5  73.44 ( 73.55)\n",
            "Test: [400/782]\tTime  0.243 ( 0.124)\tAcc@1  50.00 ( 48.96)\tAcc@5  78.12 ( 73.62)\n",
            "Test: [500/782]\tTime  0.404 ( 0.121)\tAcc@1  46.88 ( 49.02)\tAcc@5  67.19 ( 73.49)\n",
            "Test: [600/782]\tTime  0.282 ( 0.119)\tAcc@1  54.69 ( 48.96)\tAcc@5  75.00 ( 73.51)\n",
            "Test: [700/782]\tTime  0.171 ( 0.118)\tAcc@1  40.62 ( 48.89)\tAcc@5  62.50 ( 73.43)\n",
            " * Acc@1 49.052 Acc@5 73.520\n",
            "Full quantization (W4A4) accuracy: 49.051998138427734\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.226 ( 7.226)\tAcc@1  67.19 ( 67.19)\tAcc@5  87.50 ( 87.50)\n",
            "Test: [100/782]\tTime  0.220 ( 0.179)\tAcc@1  59.38 ( 65.95)\tAcc@5  82.81 ( 86.66)\n",
            "Test: [200/782]\tTime  0.228 ( 0.141)\tAcc@1  60.94 ( 66.43)\tAcc@5  89.06 ( 87.06)\n",
            "Test: [300/782]\tTime  0.077 ( 0.129)\tAcc@1  64.06 ( 66.33)\tAcc@5  87.50 ( 86.90)\n",
            "Test: [400/782]\tTime  0.215 ( 0.123)\tAcc@1  71.88 ( 66.37)\tAcc@5  87.50 ( 86.88)\n",
            "Test: [500/782]\tTime  0.146 ( 0.120)\tAcc@1  65.62 ( 66.34)\tAcc@5  79.69 ( 86.94)\n",
            "Test: [600/782]\tTime  0.062 ( 0.118)\tAcc@1  62.50 ( 66.34)\tAcc@5  81.25 ( 87.00)\n",
            "Test: [700/782]\tTime  0.056 ( 0.117)\tAcc@1  53.12 ( 66.13)\tAcc@5  76.56 ( 86.87)\n",
            " * Acc@1 66.226 Acc@5 86.890\n",
            "Full quantization (W5A4) accuracy: 66.22599792480469\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 5 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.232 ( 7.232)\tAcc@1  67.19 ( 67.19)\tAcc@5  87.50 ( 87.50)\n",
            "Test: [100/782]\tTime  0.205 ( 0.178)\tAcc@1  59.38 ( 65.95)\tAcc@5  82.81 ( 86.66)\n",
            "Test: [200/782]\tTime  0.215 ( 0.143)\tAcc@1  60.94 ( 66.43)\tAcc@5  89.06 ( 87.06)\n",
            "Test: [300/782]\tTime  0.200 ( 0.131)\tAcc@1  64.06 ( 66.33)\tAcc@5  87.50 ( 86.90)\n",
            "Test: [400/782]\tTime  0.224 ( 0.125)\tAcc@1  71.88 ( 66.37)\tAcc@5  87.50 ( 86.88)\n",
            "Test: [500/782]\tTime  0.424 ( 0.122)\tAcc@1  65.62 ( 66.34)\tAcc@5  79.69 ( 86.94)\n",
            "Test: [600/782]\tTime  0.062 ( 0.119)\tAcc@1  62.50 ( 66.34)\tAcc@5  81.25 ( 87.00)\n",
            "Test: [700/782]\tTime  0.063 ( 0.118)\tAcc@1  53.12 ( 66.13)\tAcc@5  76.56 ( 86.87)\n",
            " * Acc@1 66.226 Acc@5 86.890\n",
            "Full quantization (W4A4) accuracy: 66.22599792480469\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.314 ( 7.314)\tAcc@1  65.62 ( 65.62)\tAcc@5  85.94 ( 85.94)\n",
            "Test: [100/782]\tTime  0.207 ( 0.184)\tAcc@1  54.69 ( 65.05)\tAcc@5  76.56 ( 86.19)\n",
            "Test: [200/782]\tTime  0.234 ( 0.146)\tAcc@1  65.62 ( 65.35)\tAcc@5  85.94 ( 86.71)\n",
            "Test: [300/782]\tTime  0.174 ( 0.133)\tAcc@1  62.50 ( 65.10)\tAcc@5  85.94 ( 86.45)\n",
            "Test: [400/782]\tTime  0.215 ( 0.126)\tAcc@1  68.75 ( 65.17)\tAcc@5  87.50 ( 86.33)\n",
            "Test: [500/782]\tTime  0.387 ( 0.123)\tAcc@1  62.50 ( 65.08)\tAcc@5  81.25 ( 86.40)\n",
            "Test: [600/782]\tTime  0.243 ( 0.121)\tAcc@1  59.38 ( 64.94)\tAcc@5  78.12 ( 86.46)\n",
            "Test: [700/782]\tTime  0.232 ( 0.119)\tAcc@1  56.25 ( 64.81)\tAcc@5  78.12 ( 86.38)\n",
            " * Acc@1 64.936 Acc@5 86.360\n",
            "Full quantization (W4A4) accuracy: 64.93599700927734\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 15"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AdaRound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Test: [  0/782]\tTime  7.325 ( 7.325)\tAcc@1  70.31 ( 70.31)\tAcc@5  93.75 ( 93.75)\n",
            "Test: [100/782]\tTime  0.136 ( 0.205)\tAcc@1  60.94 ( 67.37)\tAcc@5  82.81 ( 87.05)\n",
            "Test: [200/782]\tTime  0.173 ( 0.172)\tAcc@1  71.88 ( 67.26)\tAcc@5  89.06 ( 87.03)\n",
            "Test: [300/782]\tTime  0.257 ( 0.158)\tAcc@1  60.94 ( 66.72)\tAcc@5  87.50 ( 87.13)\n",
            "Test: [400/782]\tTime  0.067 ( 0.150)\tAcc@1  67.19 ( 66.72)\tAcc@5  82.81 ( 87.21)\n",
            "Test: [500/782]\tTime  0.053 ( 0.147)\tAcc@1  68.75 ( 66.38)\tAcc@5  92.19 ( 87.11)\n",
            "Test: [600/782]\tTime  0.061 ( 0.146)\tAcc@1  71.88 ( 66.32)\tAcc@5  90.62 ( 87.03)\n",
            "Test: [700/782]\tTime  0.060 ( 0.144)\tAcc@1  57.81 ( 66.50)\tAcc@5  81.25 ( 87.17)\n",
            " * Acc@1 66.678 Acc@5 87.222\n",
            "Full quantization (W4A4) accuracy: 66.6780014038086\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --iters_w 100"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qdrop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.009 (rec:0.009, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.010 (rec:0.010, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.008 (rec:0.008, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t84.182 (rec:0.009, round:84.173)\tb=20.00\tcount=2000\n",
            "Total loss:\t36.178 (rec:0.010, round:36.169)\tb=18.88\tcount=2500\n",
            "Total loss:\t32.894 (rec:0.010, round:32.884)\tb=17.75\tcount=3000\n",
            "Total loss:\t30.381 (rec:0.012, round:30.369)\tb=16.62\tcount=3500\n",
            "Total loss:\t27.864 (rec:0.011, round:27.853)\tb=15.50\tcount=4000\n",
            "Total loss:\t25.448 (rec:0.009, round:25.440)\tb=14.38\tcount=4500\n",
            "Total loss:\t23.322 (rec:0.010, round:23.312)\tb=13.25\tcount=5000\n",
            "Total loss:\t20.691 (rec:0.010, round:20.681)\tb=12.12\tcount=5500\n",
            "Total loss:\t17.719 (rec:0.010, round:17.709)\tb=11.00\tcount=6000\n",
            "Total loss:\t14.827 (rec:0.011, round:14.815)\tb=9.88\tcount=6500\n",
            "Total loss:\t11.837 (rec:0.010, round:11.827)\tb=8.75\tcount=7000\n",
            "Total loss:\t8.696 (rec:0.009, round:8.687)\tb=7.62\tcount=7500\n",
            "Total loss:\t5.274 (rec:0.017, round:5.257)\tb=6.50\tcount=8000\n",
            "Total loss:\t2.203 (rec:0.008, round:2.195)\tb=5.38\tcount=8500\n",
            "Total loss:\t0.564 (rec:0.013, round:0.551)\tb=4.25\tcount=9000\n",
            "Total loss:\t0.063 (rec:0.009, round:0.054)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.010 (rec:0.009, round:0.001)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.107 (rec:0.107, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.106 (rec:0.106, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.142 (rec:0.142, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t661.688 (rec:0.120, round:661.568)\tb=20.00\tcount=2000\n",
            "Total loss:\t362.514 (rec:0.101, round:362.413)\tb=18.88\tcount=2500\n",
            "Total loss:\t331.200 (rec:0.114, round:331.086)\tb=17.75\tcount=3000\n",
            "Total loss:\t308.621 (rec:0.111, round:308.510)\tb=16.62\tcount=3500\n",
            "Total loss:\t287.110 (rec:0.111, round:286.999)\tb=15.50\tcount=4000\n",
            "Total loss:\t266.562 (rec:0.107, round:266.455)\tb=14.38\tcount=4500\n",
            "Total loss:\t245.264 (rec:0.147, round:245.117)\tb=13.25\tcount=5000\n",
            "Total loss:\t223.150 (rec:0.115, round:223.036)\tb=12.12\tcount=5500\n",
            "Total loss:\t200.131 (rec:0.110, round:200.020)\tb=11.00\tcount=6000\n",
            "Total loss:\t174.541 (rec:0.101, round:174.440)\tb=9.88\tcount=6500\n",
            "Total loss:\t146.409 (rec:0.119, round:146.290)\tb=8.75\tcount=7000\n",
            "Total loss:\t116.900 (rec:0.132, round:116.768)\tb=7.62\tcount=7500\n",
            "Total loss:\t84.086 (rec:0.109, round:83.977)\tb=6.50\tcount=8000\n",
            "Total loss:\t51.195 (rec:0.130, round:51.065)\tb=5.38\tcount=8500\n",
            "Total loss:\t20.505 (rec:0.114, round:20.391)\tb=4.25\tcount=9000\n",
            "Total loss:\t2.945 (rec:0.138, round:2.808)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.232 (rec:0.139, round:0.093)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.296 (rec:0.296, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.315 (rec:0.315, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.331 (rec:0.331, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t686.644 (rec:0.293, round:686.351)\tb=20.00\tcount=2000\n",
            "Total loss:\t377.956 (rec:0.296, round:377.660)\tb=18.88\tcount=2500\n",
            "Total loss:\t344.463 (rec:0.290, round:344.173)\tb=17.75\tcount=3000\n",
            "Total loss:\t320.968 (rec:0.288, round:320.680)\tb=16.62\tcount=3500\n",
            "Total loss:\t299.307 (rec:0.281, round:299.026)\tb=15.50\tcount=4000\n",
            "Total loss:\t277.170 (rec:0.272, round:276.898)\tb=14.38\tcount=4500\n",
            "Total loss:\t254.484 (rec:0.304, round:254.180)\tb=13.25\tcount=5000\n",
            "Total loss:\t231.391 (rec:0.306, round:231.085)\tb=12.12\tcount=5500\n",
            "Total loss:\t206.417 (rec:0.294, round:206.123)\tb=11.00\tcount=6000\n",
            "Total loss:\t179.877 (rec:0.296, round:179.581)\tb=9.88\tcount=6500\n",
            "Total loss:\t150.888 (rec:0.270, round:150.619)\tb=8.75\tcount=7000\n",
            "Total loss:\t118.507 (rec:0.283, round:118.224)\tb=7.62\tcount=7500\n",
            "Total loss:\t85.097 (rec:0.306, round:84.791)\tb=6.50\tcount=8000\n",
            "Total loss:\t51.218 (rec:0.293, round:50.925)\tb=5.38\tcount=8500\n",
            "Total loss:\t21.052 (rec:0.310, round:20.742)\tb=4.25\tcount=9000\n",
            "Total loss:\t3.518 (rec:0.315, round:3.203)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.402 (rec:0.297, round:0.105)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.147 (rec:0.147, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.149 (rec:0.149, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.140 (rec:0.140, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2155.146 (rec:0.145, round:2155.000)\tb=20.00\tcount=2000\n",
            "Total loss:\t1098.437 (rec:0.155, round:1098.282)\tb=18.88\tcount=2500\n",
            "Total loss:\t995.773 (rec:0.164, round:995.609)\tb=17.75\tcount=3000\n",
            "Total loss:\t921.781 (rec:0.154, round:921.627)\tb=16.62\tcount=3500\n",
            "Total loss:\t854.780 (rec:0.151, round:854.629)\tb=15.50\tcount=4000\n",
            "Total loss:\t788.337 (rec:0.157, round:788.179)\tb=14.38\tcount=4500\n",
            "Total loss:\t718.829 (rec:0.150, round:718.680)\tb=13.25\tcount=5000\n",
            "Total loss:\t647.443 (rec:0.165, round:647.279)\tb=12.12\tcount=5500\n",
            "Total loss:\t571.325 (rec:0.155, round:571.169)\tb=11.00\tcount=6000\n",
            "Total loss:\t489.221 (rec:0.150, round:489.071)\tb=9.88\tcount=6500\n",
            "Total loss:\t399.643 (rec:0.163, round:399.481)\tb=8.75\tcount=7000\n",
            "Total loss:\t305.070 (rec:0.154, round:304.916)\tb=7.62\tcount=7500\n",
            "Total loss:\t205.689 (rec:0.166, round:205.523)\tb=6.50\tcount=8000\n",
            "Total loss:\t108.884 (rec:0.174, round:108.710)\tb=5.38\tcount=8500\n",
            "Total loss:\t32.409 (rec:0.171, round:32.238)\tb=4.25\tcount=9000\n",
            "Total loss:\t2.059 (rec:0.167, round:1.892)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.183 (rec:0.173, round:0.010)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.294 (rec:0.294, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.305 (rec:0.305, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.291 (rec:0.291, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2764.863 (rec:0.274, round:2764.589)\tb=20.00\tcount=2000\n",
            "Total loss:\t1409.739 (rec:0.286, round:1409.453)\tb=18.88\tcount=2500\n",
            "Total loss:\t1282.980 (rec:0.284, round:1282.696)\tb=17.75\tcount=3000\n",
            "Total loss:\t1191.402 (rec:0.284, round:1191.118)\tb=16.62\tcount=3500\n",
            "Total loss:\t1106.112 (rec:0.275, round:1105.837)\tb=15.50\tcount=4000\n",
            "Total loss:\t1022.336 (rec:0.271, round:1022.065)\tb=14.38\tcount=4500\n",
            "Total loss:\t934.889 (rec:0.280, round:934.609)\tb=13.25\tcount=5000\n",
            "Total loss:\t844.006 (rec:0.297, round:843.709)\tb=12.12\tcount=5500\n",
            "Total loss:\t749.004 (rec:0.284, round:748.720)\tb=11.00\tcount=6000\n",
            "Total loss:\t647.195 (rec:0.280, round:646.915)\tb=9.88\tcount=6500\n",
            "Total loss:\t537.857 (rec:0.283, round:537.575)\tb=8.75\tcount=7000\n",
            "Total loss:\t420.300 (rec:0.297, round:420.003)\tb=7.62\tcount=7500\n",
            "Total loss:\t294.368 (rec:0.310, round:294.058)\tb=6.50\tcount=8000\n",
            "Total loss:\t167.395 (rec:0.305, round:167.090)\tb=5.38\tcount=8500\n",
            "Total loss:\t53.848 (rec:0.344, round:53.503)\tb=4.25\tcount=9000\n",
            "Total loss:\t3.587 (rec:0.295, round:3.293)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.329 (rec:0.307, round:0.022)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.185 (rec:0.185, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.163 (rec:0.163, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.175 (rec:0.175, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t8657.739 (rec:0.178, round:8657.562)\tb=20.00\tcount=2000\n",
            "Total loss:\t4295.323 (rec:0.170, round:4295.153)\tb=18.88\tcount=2500\n",
            "Total loss:\t3912.806 (rec:0.166, round:3912.640)\tb=17.75\tcount=3000\n",
            "Total loss:\t3638.541 (rec:0.174, round:3638.367)\tb=16.62\tcount=3500\n",
            "Total loss:\t3383.721 (rec:0.179, round:3383.542)\tb=15.50\tcount=4000\n",
            "Total loss:\t3128.976 (rec:0.172, round:3128.803)\tb=14.38\tcount=4500\n",
            "Total loss:\t2867.506 (rec:0.178, round:2867.328)\tb=13.25\tcount=5000\n",
            "Total loss:\t2590.252 (rec:0.175, round:2590.077)\tb=12.12\tcount=5500\n",
            "Total loss:\t2297.773 (rec:0.179, round:2297.594)\tb=11.00\tcount=6000\n",
            "Total loss:\t1985.515 (rec:0.170, round:1985.344)\tb=9.88\tcount=6500\n",
            "Total loss:\t1650.368 (rec:0.190, round:1650.178)\tb=8.75\tcount=7000\n",
            "Total loss:\t1292.111 (rec:0.190, round:1291.921)\tb=7.62\tcount=7500\n",
            "Total loss:\t912.745 (rec:0.190, round:912.555)\tb=6.50\tcount=8000\n",
            "Total loss:\t524.242 (rec:0.187, round:524.055)\tb=5.38\tcount=8500\n",
            "Total loss:\t157.604 (rec:0.190, round:157.414)\tb=4.25\tcount=9000\n",
            "Total loss:\t7.606 (rec:0.186, round:7.420)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.238 (rec:0.192, round:0.046)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.287 (rec:0.287, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.279 (rec:0.279, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.257 (rec:0.257, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t11131.063 (rec:0.256, round:11130.807)\tb=20.00\tcount=2000\n",
            "Total loss:\t5529.685 (rec:0.283, round:5529.401)\tb=18.88\tcount=2500\n",
            "Total loss:\t5049.021 (rec:0.271, round:5048.750)\tb=17.75\tcount=3000\n",
            "Total loss:\t4701.907 (rec:0.254, round:4701.653)\tb=16.62\tcount=3500\n",
            "Total loss:\t4377.622 (rec:0.262, round:4377.360)\tb=15.50\tcount=4000\n",
            "Total loss:\t4052.947 (rec:0.269, round:4052.678)\tb=14.38\tcount=4500\n",
            "Total loss:\t3718.376 (rec:0.281, round:3718.095)\tb=13.25\tcount=5000\n",
            "Total loss:\t3367.244 (rec:0.274, round:3366.970)\tb=12.12\tcount=5500\n",
            "Total loss:\t2990.980 (rec:0.265, round:2990.715)\tb=11.00\tcount=6000\n",
            "Total loss:\t2585.344 (rec:0.282, round:2585.062)\tb=9.88\tcount=6500\n",
            "Total loss:\t2157.049 (rec:0.284, round:2156.765)\tb=8.75\tcount=7000\n",
            "Total loss:\t1702.706 (rec:0.281, round:1702.424)\tb=7.62\tcount=7500\n",
            "Total loss:\t1229.158 (rec:0.275, round:1228.883)\tb=6.50\tcount=8000\n",
            "Total loss:\t743.795 (rec:0.289, round:743.506)\tb=5.38\tcount=8500\n",
            "Total loss:\t266.961 (rec:0.289, round:266.673)\tb=4.25\tcount=9000\n",
            "Total loss:\t18.996 (rec:0.276, round:18.720)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.453 (rec:0.285, round:0.168)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.427 (rec:0.427, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.356 (rec:0.356, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.351 (rec:0.351, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t34656.805 (rec:0.311, round:34656.492)\tb=20.00\tcount=2000\n",
            "Total loss:\t17481.982 (rec:0.322, round:17481.660)\tb=18.88\tcount=2500\n",
            "Total loss:\t15980.201 (rec:0.367, round:15979.834)\tb=17.75\tcount=3000\n",
            "Total loss:\t14886.621 (rec:0.352, round:14886.270)\tb=16.62\tcount=3500\n",
            "Total loss:\t13858.496 (rec:0.336, round:13858.160)\tb=15.50\tcount=4000\n",
            "Total loss:\t12829.498 (rec:0.304, round:12829.193)\tb=14.38\tcount=4500\n",
            "Total loss:\t11762.680 (rec:0.330, round:11762.350)\tb=13.25\tcount=5000\n",
            "Total loss:\t10649.816 (rec:0.315, round:10649.501)\tb=12.12\tcount=5500\n",
            "Total loss:\t9471.115 (rec:0.307, round:9470.809)\tb=11.00\tcount=6000\n",
            "Total loss:\t8212.559 (rec:0.317, round:8212.242)\tb=9.88\tcount=6500\n",
            "Total loss:\t6876.937 (rec:0.318, round:6876.619)\tb=8.75\tcount=7000\n",
            "Total loss:\t5458.730 (rec:0.319, round:5458.411)\tb=7.62\tcount=7500\n",
            "Total loss:\t3988.639 (rec:0.290, round:3988.350)\tb=6.50\tcount=8000\n",
            "Total loss:\t2503.935 (rec:0.329, round:2503.606)\tb=5.38\tcount=8500\n",
            "Total loss:\t1060.829 (rec:0.333, round:1060.496)\tb=4.25\tcount=9000\n",
            "Total loss:\t117.830 (rec:0.369, round:117.461)\tb=3.12\tcount=9500\n",
            "Total loss:\t2.040 (rec:0.353, round:1.687)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t38.164 (rec:38.164, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t32.709 (rec:32.709, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t36.229 (rec:36.229, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t44662.930 (rec:32.832, round:44630.098)\tb=20.00\tcount=2000\n",
            "Total loss:\t29942.150 (rec:31.053, round:29911.098)\tb=18.88\tcount=2500\n",
            "Total loss:\t27928.930 (rec:29.811, round:27899.119)\tb=17.75\tcount=3000\n",
            "Total loss:\t26564.156 (rec:32.371, round:26531.785)\tb=16.62\tcount=3500\n",
            "Total loss:\t25348.695 (rec:28.121, round:25320.574)\tb=15.50\tcount=4000\n",
            "Total loss:\t24162.775 (rec:29.982, round:24132.793)\tb=14.38\tcount=4500\n",
            "Total loss:\t22943.744 (rec:31.910, round:22911.834)\tb=13.25\tcount=5000\n",
            "Total loss:\t21649.855 (rec:30.184, round:21619.672)\tb=12.12\tcount=5500\n",
            "Total loss:\t20250.854 (rec:27.822, round:20223.031)\tb=11.00\tcount=6000\n",
            "Total loss:\t18719.992 (rec:28.327, round:18691.664)\tb=9.88\tcount=6500\n",
            "Total loss:\t17011.367 (rec:27.669, round:16983.699)\tb=8.75\tcount=7000\n",
            "Total loss:\t15091.457 (rec:28.521, round:15062.936)\tb=7.62\tcount=7500\n",
            "Total loss:\t12908.168 (rec:31.456, round:12876.712)\tb=6.50\tcount=8000\n",
            "Total loss:\t10412.622 (rec:30.689, round:10381.934)\tb=5.38\tcount=8500\n",
            "Total loss:\t7538.291 (rec:30.130, round:7508.161)\tb=4.25\tcount=9000\n",
            "Total loss:\t4314.930 (rec:27.512, round:4287.418)\tb=3.12\tcount=9500\n",
            "Total loss:\t1255.495 (rec:29.579, round:1225.916)\tb=2.00\tcount=10000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t24.315 (rec:24.315, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t18.047 (rec:18.047, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t19.745 (rec:19.745, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t4849.059 (rec:18.048, round:4831.011)\tb=20.00\tcount=2000\n",
            "Total loss:\t3026.808 (rec:20.713, round:3006.094)\tb=18.88\tcount=2500\n",
            "Total loss:\t2808.812 (rec:21.533, round:2787.278)\tb=17.75\tcount=3000\n",
            "Total loss:\t2655.878 (rec:22.683, round:2633.195)\tb=16.62\tcount=3500\n",
            "Total loss:\t2511.508 (rec:19.687, round:2491.821)\tb=15.50\tcount=4000\n",
            "Total loss:\t2375.070 (rec:21.976, round:2353.094)\tb=14.38\tcount=4500\n",
            "Total loss:\t2227.804 (rec:19.002, round:2208.802)\tb=13.25\tcount=5000\n",
            "Total loss:\t2071.926 (rec:18.093, round:2053.832)\tb=12.12\tcount=5500\n",
            "Total loss:\t1908.488 (rec:19.186, round:1889.302)\tb=11.00\tcount=6000\n",
            "Total loss:\t1731.615 (rec:20.020, round:1711.595)\tb=9.88\tcount=6500\n",
            "Total loss:\t1544.158 (rec:24.450, round:1519.708)\tb=8.75\tcount=7000\n",
            "Total loss:\t1329.625 (rec:19.811, round:1309.814)\tb=7.62\tcount=7500\n",
            "Total loss:\t1096.814 (rec:21.904, round:1074.910)\tb=6.50\tcount=8000\n",
            "Total loss:\t838.594 (rec:20.012, round:818.582)\tb=5.38\tcount=8500\n",
            "Total loss:\t559.833 (rec:18.915, round:540.917)\tb=4.25\tcount=9000\n",
            "Total loss:\t279.280 (rec:20.634, round:258.646)\tb=3.12\tcount=9500\n",
            "Total loss:\t70.146 (rec:21.504, round:48.643)\tb=2.00\tcount=10000\n",
            "Test: [  0/782]\tTime  7.574 ( 7.574)\tAcc@1  62.50 ( 62.50)\tAcc@5  84.38 ( 84.38)\n",
            "Test: [100/782]\tTime  0.064 ( 0.214)\tAcc@1  65.62 ( 68.73)\tAcc@5  87.50 ( 88.23)\n",
            "Test: [200/782]\tTime  0.064 ( 0.179)\tAcc@1  70.31 ( 69.01)\tAcc@5  90.62 ( 88.43)\n",
            "Test: [300/782]\tTime  0.063 ( 0.165)\tAcc@1  70.31 ( 69.19)\tAcc@5  89.06 ( 88.76)\n",
            "Test: [400/782]\tTime  0.060 ( 0.158)\tAcc@1  73.44 ( 69.22)\tAcc@5  87.50 ( 88.77)\n",
            "Test: [500/782]\tTime  0.155 ( 0.151)\tAcc@1  64.06 ( 69.30)\tAcc@5  84.38 ( 88.85)\n",
            "Test: [600/782]\tTime  0.066 ( 0.149)\tAcc@1  60.94 ( 69.23)\tAcc@5  84.38 ( 88.83)\n",
            "Test: [700/782]\tTime  0.062 ( 0.148)\tAcc@1  67.19 ( 69.21)\tAcc@5  87.50 ( 88.90)\n",
            " * Acc@1 69.222 Acc@5 88.930\n",
            "Full quantization (W4A4) accuracy: 69.22200012207031\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=5, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=5, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\main_imagenet.py:258\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39m# Start calibration\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39muse_basic_quantization:\n\u001b[1;32m--> 258\u001b[0m     recon_model(qnn)\n\u001b[0;32m    260\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mact_quant \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39morder \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mafter\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mwaq \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Case 1'''\u001b[39;00m\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\main_imagenet.py:255\u001b[0m, in \u001b[0;36mrecon_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    253\u001b[0m     set_weight_act_quantize_params(module)\n\u001b[0;32m    254\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 255\u001b[0m     recon_model(module)\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\main_imagenet.py:250\u001b[0m, in \u001b[0;36mrecon_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, QuantModule):\n\u001b[0;32m    249\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mReconstruction for layer \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m--> 250\u001b[0m     set_weight_act_quantize_params(module)\n\u001b[0;32m    251\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, BaseQuantBlock):\n\u001b[0;32m    252\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mReconstruction for block \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name))\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\main_imagenet.py:237\u001b[0m, in \u001b[0;36mset_weight_act_quantize_params\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_weight_act_quantize_params\u001b[39m(module):\n\u001b[0;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, QuantModule):\n\u001b[1;32m--> 237\u001b[0m         layer_reconstruction(qnn, module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    238\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, BaseQuantBlock):\n\u001b[0;32m    239\u001b[0m         block_reconstruction(qnn, module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\layer_recon.py:38\u001b[0m, in \u001b[0;36mlayer_reconstruction\u001b[1;34m(model, layer, cali_data, batch_size, iters, weight, opt_mode, act_quant, b_range, warmup, p, lr, wwq, waq, order, input_prob, keep_gpu)\u001b[0m\n\u001b[0;32m     35\u001b[0m cached_inps, cached_outs \u001b[39m=\u001b[39m get_init(model, layer, cali_data, wq\u001b[39m=\u001b[39mwwq, aq\u001b[39m=\u001b[39mwaq, batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m     36\u001b[0m                                     input_prob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, keep_gpu\u001b[39m=\u001b[39mkeep_gpu)\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m act_quant \u001b[39mand\u001b[39;00m order \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtogether\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 38\u001b[0m     set_act_quantize_params(layer, cali_data\u001b[39m=\u001b[39;49mcached_inps[\u001b[39m0\u001b[39;49m][:\u001b[39mmin\u001b[39;49m(\u001b[39m256\u001b[39;49m, cached_inps[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m))], awq\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, order\u001b[39m=\u001b[39;49morder)\n\u001b[0;32m     40\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''set state'''\u001b[39;00m\n\u001b[0;32m     41\u001b[0m cur_weight, cur_act \u001b[39m=\u001b[39m weight_get_quant_state(order, act_quant)\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\set_act_quantize_params.py:21\u001b[0m, in \u001b[0;36mset_act_quantize_params\u001b[1;34m(module, cali_data, awq, order, batch_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(cali_data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m batch_size)):\n\u001b[1;32m---> 21\u001b[0m         module(cali_data[i \u001b[39m*\u001b[39;49m batch_size:(i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m batch_size]\u001b[39m.\u001b[39;49mcuda())\n\u001b[0;32m     22\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mmodules():\n",
            "File \u001b[1;32mc:\\Users\\sedov\\.conda\\envs\\quantization\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:290\u001b[0m, in \u001b[0;36mQuantModule.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_act_quant:\n\u001b[1;32m--> 290\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact_quantizer(out)\n\u001b[0;32m    291\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\sedov\\.conda\\envs\\quantization\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:99\u001b[0m, in \u001b[0;36mUniformAffineQuantizer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minited \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleaf_param:\n\u001b[1;32m---> 99\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelta, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzero_point \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_quantization_scale(x\u001b[39m.\u001b[39;49mclone()\u001b[39m.\u001b[39;49mdetach(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchannel_wise)\n\u001b[0;32m    100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelta, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzero_point \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_quantization_scale(x\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel_wise)\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:222\u001b[0m, in \u001b[0;36mUniformAffineQuantizer.init_quantization_scale\u001b[1;34m(self, x_clone, channel_wise)\u001b[0m\n\u001b[0;32m    220\u001b[0m     zero_point \u001b[39m=\u001b[39m zero_point\u001b[39m.\u001b[39mreshape(new_shape)\n\u001b[0;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     delta, zero_point \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_quantization_scale_channel(x_clone)\n\u001b[0;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m delta, zero_point\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:210\u001b[0m, in \u001b[0;36mUniformAffineQuantizer.init_quantization_scale_channel\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_quantization_scale_channel\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 210\u001b[0m     x_min, x_max \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_x_min_x_max(x)\n\u001b[0;32m    211\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_qparams(x_min, x_max)\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:202\u001b[0m, in \u001b[0;36mUniformAffineQuantizer.get_x_min_x_max\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_side_dist \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mmin() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mno\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_side_dist \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mno\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msym:  \u001b[39m# one-side distribution or symmetric value for 1-d search\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     best_min, best_max \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_1D_search(x)\n\u001b[0;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# 2-d search\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     best_min, best_max \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperform_2D_search(x)\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:189\u001b[0m, in \u001b[0;36mUniformAffineQuantizer.perform_1D_search\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    187\u001b[0m new_min \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(x_min) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_side_dist \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mthres\n\u001b[0;32m    188\u001b[0m new_max \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(x_max) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_side_dist \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m thres\n\u001b[1;32m--> 189\u001b[0m x_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantize(x, new_max, new_min)\n\u001b[0;32m    190\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlp_loss(x, x_q, \u001b[39m2.4\u001b[39m)\n\u001b[0;32m    191\u001b[0m best_min \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(score \u001b[39m<\u001b[39m best_score, new_min, best_min)\n",
            "File \u001b[1;32m~\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:141\u001b[0m, in \u001b[0;36mUniformAffineQuantizer.quantize\u001b[1;34m(self, x, x_max, x_min)\u001b[0m\n\u001b[0;32m    139\u001b[0m     zero_point \u001b[39m=\u001b[39m zero_point\u001b[39m.\u001b[39mreshape(new_shape)\n\u001b[0;32m    140\u001b[0m x_int \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mround(x \u001b[39m/\u001b[39m delta)\n\u001b[1;32m--> 141\u001b[0m x_quant \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mclamp(x_int \u001b[39m+\u001b[39;49m zero_point, \u001b[39m0\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_levels \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m x_float_q \u001b[39m=\u001b[39m (x_quant \u001b[39m-\u001b[39m zero_point) \u001b[39m*\u001b[39m delta\n\u001b[0;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m x_float_q\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 5  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Дополнительная квантизация"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для того, чтобы смоделировать дополнительную квантизацию, изменим количество уровней в обычной квантизации. Для 4х бит уровней по умлочанию 16, с дополнительной квантизацией добавляется максимум 15 уровней. Будем варьировать количество уровней от 16 до 31, и посмотрим, как при этом меняется accuracy модели."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обычный способ квантизации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sedov\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:148: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorCompare.cpp:652.)\n",
            "  x_min, x_max = torch._aminmax(y, 1)\n",
            "C:\\Users\\sedov\\dev\\m1p\\2023-Project-138\\code\\QDrop\\quant\\quant_layer.py:179: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceAllOps.cpp:66.)\n",
            "  x_min, x_max = torch._aminmax(x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test: [  0/782]\tTime  8.098 ( 8.098)\tAcc@1  60.94 ( 60.94)\tAcc@5  85.94 ( 85.94)\n",
            "Test: [100/782]\tTime  0.283 ( 0.199)\tAcc@1  57.81 ( 63.38)\tAcc@5  78.12 ( 84.28)\n",
            "Test: [200/782]\tTime  0.373 ( 0.167)\tAcc@1  65.62 ( 63.80)\tAcc@5  89.06 ( 85.29)\n",
            "Test: [300/782]\tTime  0.312 ( 0.157)\tAcc@1  64.06 ( 63.70)\tAcc@5  84.38 ( 85.26)\n",
            "Test: [400/782]\tTime  0.331 ( 0.153)\tAcc@1  65.62 ( 63.83)\tAcc@5  87.50 ( 85.17)\n",
            "Test: [500/782]\tTime  0.570 ( 0.151)\tAcc@1  65.62 ( 63.78)\tAcc@5  76.56 ( 85.07)\n",
            "Test: [600/782]\tTime  0.144 ( 0.149)\tAcc@1  60.94 ( 63.72)\tAcc@5  78.12 ( 85.20)\n",
            "Test: [700/782]\tTime  0.356 ( 0.148)\tAcc@1  54.69 ( 63.61)\tAcc@5  76.56 ( 85.13)\n",
            " * Acc@1 63.738 Acc@5 85.192\n",
            "Full quantization (W4A4) accuracy: 63.737998962402344\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 15 --disable_8bit_head_stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.225 ( 7.225)\tAcc@1  65.62 ( 65.62)\tAcc@5  82.81 ( 82.81)\n",
            "Test: [100/782]\tTime  0.243 ( 0.188)\tAcc@1  62.50 ( 63.61)\tAcc@5  87.50 ( 84.70)\n",
            "Test: [200/782]\tTime  0.170 ( 0.147)\tAcc@1  60.94 ( 63.67)\tAcc@5  89.06 ( 85.40)\n",
            "Test: [300/782]\tTime  0.066 ( 0.133)\tAcc@1  73.44 ( 63.60)\tAcc@5  90.62 ( 85.29)\n",
            "Test: [400/782]\tTime  0.065 ( 0.125)\tAcc@1  59.38 ( 63.58)\tAcc@5  85.94 ( 85.36)\n",
            "Test: [500/782]\tTime  0.103 ( 0.122)\tAcc@1  62.50 ( 63.57)\tAcc@5  78.12 ( 85.38)\n",
            "Test: [600/782]\tTime  0.054 ( 0.120)\tAcc@1  62.50 ( 63.56)\tAcc@5  81.25 ( 85.28)\n",
            "Test: [700/782]\tTime  0.064 ( 0.119)\tAcc@1  56.25 ( 63.42)\tAcc@5  76.56 ( 85.20)\n",
            " * Acc@1 63.482 Acc@5 85.190\n",
            "Full quantization (W4A4) accuracy: 63.481998443603516\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 12 --disable_8bit_head_stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.253 ( 7.253)\tAcc@1  70.31 ( 70.31)\tAcc@5  84.38 ( 84.38)\n",
            "Test: [100/782]\tTime  0.309 ( 0.203)\tAcc@1  65.62 ( 63.99)\tAcc@5  85.94 ( 85.43)\n",
            "Test: [200/782]\tTime  0.176 ( 0.155)\tAcc@1  65.62 ( 64.30)\tAcc@5  89.06 ( 85.63)\n",
            "Test: [300/782]\tTime  0.190 ( 0.138)\tAcc@1  71.88 ( 64.09)\tAcc@5  87.50 ( 85.40)\n",
            "Test: [400/782]\tTime  0.256 ( 0.130)\tAcc@1  64.06 ( 64.28)\tAcc@5  87.50 ( 85.57)\n",
            "Test: [500/782]\tTime  0.372 ( 0.126)\tAcc@1  65.62 ( 64.17)\tAcc@5  73.44 ( 85.53)\n",
            "Test: [600/782]\tTime  0.087 ( 0.123)\tAcc@1  68.75 ( 64.13)\tAcc@5  81.25 ( 85.52)\n",
            "Test: [700/782]\tTime  0.129 ( 0.121)\tAcc@1  57.81 ( 63.97)\tAcc@5  76.56 ( 85.50)\n",
            " * Acc@1 64.082 Acc@5 85.514\n",
            "Full quantization (W4A4) accuracy: 64.08200073242188\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 10 --disable_8bit_head_stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.353 ( 7.353)\tAcc@1  65.62 ( 65.62)\tAcc@5  84.38 ( 84.38)\n",
            "Test: [100/782]\tTime  0.214 ( 0.204)\tAcc@1  65.62 ( 63.23)\tAcc@5  82.81 ( 84.81)\n",
            "Test: [200/782]\tTime  0.225 ( 0.157)\tAcc@1  65.62 ( 63.25)\tAcc@5  89.06 ( 85.22)\n",
            "Test: [300/782]\tTime  0.164 ( 0.140)\tAcc@1  65.62 ( 62.89)\tAcc@5  87.50 ( 84.91)\n",
            "Test: [400/782]\tTime  0.275 ( 0.131)\tAcc@1  65.62 ( 63.03)\tAcc@5  90.62 ( 84.94)\n",
            "Test: [500/782]\tTime  0.412 ( 0.127)\tAcc@1  59.38 ( 62.98)\tAcc@5  70.31 ( 84.96)\n",
            "Test: [600/782]\tTime  0.257 ( 0.124)\tAcc@1  68.75 ( 62.95)\tAcc@5  84.38 ( 85.00)\n",
            "Test: [700/782]\tTime  0.196 ( 0.122)\tAcc@1  56.25 ( 62.82)\tAcc@5  78.12 ( 84.97)\n",
            " * Acc@1 62.958 Acc@5 84.988\n",
            "Full quantization (W4A4) accuracy: 62.95800018310547\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 7 --disable_8bit_head_stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.324 ( 7.324)\tAcc@1  57.81 ( 57.81)\tAcc@5  79.69 ( 79.69)\n",
            "Test: [100/782]\tTime  0.231 ( 0.194)\tAcc@1  62.50 ( 59.17)\tAcc@5  81.25 ( 81.81)\n",
            "Test: [200/782]\tTime  0.180 ( 0.149)\tAcc@1  68.75 ( 58.90)\tAcc@5  85.94 ( 82.33)\n",
            "Test: [300/782]\tTime  0.207 ( 0.133)\tAcc@1  65.62 ( 58.83)\tAcc@5  79.69 ( 82.08)\n",
            "Test: [400/782]\tTime  0.230 ( 0.126)\tAcc@1  59.38 ( 59.00)\tAcc@5  87.50 ( 82.13)\n",
            "Test: [500/782]\tTime  0.428 ( 0.122)\tAcc@1  64.06 ( 59.00)\tAcc@5  71.88 ( 82.07)\n",
            "Test: [600/782]\tTime  0.187 ( 0.119)\tAcc@1  60.94 ( 58.91)\tAcc@5  82.81 ( 81.94)\n",
            "Test: [700/782]\tTime  0.236 ( 0.118)\tAcc@1  45.31 ( 58.79)\tAcc@5  75.00 ( 81.85)\n",
            " * Acc@1 58.906 Acc@5 81.864\n",
            "Full quantization (W4A4) accuracy: 58.90599822998047\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 3 --disable_8bit_head_stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=4, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.913 ( 7.913)\tAcc@1  51.56 ( 51.56)\tAcc@5  76.56 ( 76.56)\n",
            "Test: [100/782]\tTime  0.237 ( 0.190)\tAcc@1  53.12 ( 52.82)\tAcc@5  76.56 ( 76.70)\n",
            "Test: [200/782]\tTime  0.258 ( 0.153)\tAcc@1  48.44 ( 52.97)\tAcc@5  82.81 ( 77.71)\n",
            "Test: [300/782]\tTime  0.272 ( 0.142)\tAcc@1  60.94 ( 53.00)\tAcc@5  79.69 ( 77.45)\n",
            "Test: [400/782]\tTime  0.285 ( 0.136)\tAcc@1  51.56 ( 53.10)\tAcc@5  81.25 ( 77.68)\n",
            "Test: [500/782]\tTime  0.466 ( 0.133)\tAcc@1  50.00 ( 52.99)\tAcc@5  65.62 ( 77.64)\n",
            "Test: [600/782]\tTime  0.251 ( 0.132)\tAcc@1  59.38 ( 52.89)\tAcc@5  78.12 ( 77.61)\n",
            "Test: [700/782]\tTime  0.280 ( 0.131)\tAcc@1  42.19 ( 52.80)\tAcc@5  68.75 ( 77.57)\n",
            " * Acc@1 52.962 Acc@5 77.586\n",
            "Full quantization (W4A4) accuracy: 52.961997985839844\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 0 --disable_8bit_head_stem"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AdaRound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Test: [  0/782]\tTime  8.379 ( 8.379)\tAcc@1  70.31 ( 70.31)\tAcc@5  93.75 ( 93.75)\n",
            "Test: [100/782]\tTime  0.304 ( 0.203)\tAcc@1  64.06 ( 67.57)\tAcc@5  85.94 ( 87.72)\n",
            "Test: [200/782]\tTime  0.259 ( 0.162)\tAcc@1  71.88 ( 67.63)\tAcc@5  85.94 ( 87.67)\n",
            "Test: [300/782]\tTime  0.322 ( 0.149)\tAcc@1  62.50 ( 67.75)\tAcc@5  85.94 ( 87.65)\n",
            "Test: [400/782]\tTime  0.079 ( 0.143)\tAcc@1  62.50 ( 67.60)\tAcc@5  82.81 ( 87.61)\n",
            "Test: [500/782]\tTime  0.069 ( 0.140)\tAcc@1  65.62 ( 67.44)\tAcc@5  93.75 ( 87.70)\n",
            "Test: [600/782]\tTime  0.065 ( 0.138)\tAcc@1  68.75 ( 67.52)\tAcc@5  90.62 ( 87.67)\n",
            "Test: [700/782]\tTime  0.061 ( 0.136)\tAcc@1  57.81 ( 67.59)\tAcc@5  84.38 ( 87.81)\n",
            " * Acc@1 67.712 Acc@5 87.896\n",
            "Full quantization (W4A4) accuracy: 67.71199798583984\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --iters_w 100 --quantize_clipped --additional_levels_num 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Test: [  0/782]\tTime  8.283 ( 8.283)\tAcc@1  75.00 ( 75.00)\tAcc@5  90.62 ( 90.62)\n",
            "Test: [100/782]\tTime  0.315 ( 0.201)\tAcc@1  62.50 ( 67.22)\tAcc@5  84.38 ( 87.38)\n",
            "Test: [200/782]\tTime  0.268 ( 0.161)\tAcc@1  73.44 ( 67.64)\tAcc@5  87.50 ( 87.42)\n",
            "Test: [300/782]\tTime  0.330 ( 0.148)\tAcc@1  59.38 ( 67.62)\tAcc@5  87.50 ( 87.54)\n",
            "Test: [400/782]\tTime  0.360 ( 0.142)\tAcc@1  57.81 ( 67.36)\tAcc@5  81.25 ( 87.59)\n",
            "Test: [500/782]\tTime  0.170 ( 0.138)\tAcc@1  64.06 ( 67.25)\tAcc@5  89.06 ( 87.63)\n",
            "Test: [600/782]\tTime  0.060 ( 0.136)\tAcc@1  70.31 ( 67.39)\tAcc@5  90.62 ( 87.59)\n",
            "Test: [700/782]\tTime  0.050 ( 0.134)\tAcc@1  59.38 ( 67.45)\tAcc@5  84.38 ( 87.70)\n",
            " * Acc@1 67.576 Acc@5 87.786\n",
            "Full quantization (W4A4) accuracy: 67.57599639892578\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --iters_w 100 --quantize_clipped --additional_levels_num 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Test: [  0/782]\tTime  8.440 ( 8.440)\tAcc@1  73.44 ( 73.44)\tAcc@5  93.75 ( 93.75)\n",
            "Test: [100/782]\tTime  0.300 ( 0.205)\tAcc@1  67.19 ( 67.39)\tAcc@5  84.38 ( 86.96)\n",
            "Test: [200/782]\tTime  0.297 ( 0.164)\tAcc@1  79.69 ( 67.26)\tAcc@5  89.06 ( 87.00)\n",
            "Test: [300/782]\tTime  0.354 ( 0.150)\tAcc@1  65.62 ( 67.34)\tAcc@5  87.50 ( 87.25)\n",
            "Test: [400/782]\tTime  0.050 ( 0.144)\tAcc@1  64.06 ( 67.11)\tAcc@5  81.25 ( 87.29)\n",
            "Test: [500/782]\tTime  0.058 ( 0.140)\tAcc@1  68.75 ( 67.04)\tAcc@5  92.19 ( 87.33)\n",
            "Test: [600/782]\tTime  0.070 ( 0.138)\tAcc@1  70.31 ( 67.11)\tAcc@5  89.06 ( 87.35)\n",
            "Test: [700/782]\tTime  0.065 ( 0.136)\tAcc@1  57.81 ( 67.19)\tAcc@5  84.38 ( 87.50)\n",
            " * Acc@1 67.298 Acc@5 87.562\n",
            "Full quantization (W4A4) accuracy: 67.2979965209961\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --iters_w 100 --quantize_clipped --additional_levels_num 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QDrop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.031 (rec:0.031, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.037 (rec:0.037, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.029 (rec:0.029, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t80.525 (rec:0.029, round:80.497)\tb=20.00\tcount=2000\n",
            "Total loss:\t37.427 (rec:0.033, round:37.394)\tb=18.88\tcount=2500\n",
            "Total loss:\t34.099 (rec:0.037, round:34.063)\tb=17.75\tcount=3000\n",
            "Total loss:\t31.647 (rec:0.038, round:31.610)\tb=16.62\tcount=3500\n",
            "Total loss:\t29.438 (rec:0.035, round:29.403)\tb=15.50\tcount=4000\n",
            "Total loss:\t27.154 (rec:0.030, round:27.124)\tb=14.38\tcount=4500\n",
            "Total loss:\t24.975 (rec:0.038, round:24.937)\tb=13.25\tcount=5000\n",
            "Total loss:\t22.287 (rec:0.033, round:22.254)\tb=12.12\tcount=5500\n",
            "Total loss:\t19.568 (rec:0.037, round:19.531)\tb=11.00\tcount=6000\n",
            "Total loss:\t16.936 (rec:0.040, round:16.896)\tb=9.88\tcount=6500\n",
            "Total loss:\t14.052 (rec:0.034, round:14.017)\tb=8.75\tcount=7000\n",
            "Total loss:\t10.608 (rec:0.032, round:10.575)\tb=7.62\tcount=7500\n",
            "Total loss:\t7.125 (rec:0.052, round:7.073)\tb=6.50\tcount=8000\n",
            "Total loss:\t3.575 (rec:0.030, round:3.545)\tb=5.38\tcount=8500\n",
            "Total loss:\t0.770 (rec:0.043, round:0.727)\tb=4.25\tcount=9000\n",
            "Total loss:\t0.075 (rec:0.031, round:0.044)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.034 (rec:0.034, round:0.000)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.203 (rec:0.203, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.207 (rec:0.207, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.299 (rec:0.299, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t647.852 (rec:0.249, round:647.604)\tb=20.00\tcount=2000\n",
            "Total loss:\t348.395 (rec:0.189, round:348.206)\tb=18.88\tcount=2500\n",
            "Total loss:\t318.355 (rec:0.221, round:318.134)\tb=17.75\tcount=3000\n",
            "Total loss:\t295.641 (rec:0.208, round:295.433)\tb=16.62\tcount=3500\n",
            "Total loss:\t275.623 (rec:0.214, round:275.409)\tb=15.50\tcount=4000\n",
            "Total loss:\t254.850 (rec:0.201, round:254.649)\tb=14.38\tcount=4500\n",
            "Total loss:\t233.627 (rec:0.283, round:233.345)\tb=13.25\tcount=5000\n",
            "Total loss:\t212.076 (rec:0.226, round:211.850)\tb=12.12\tcount=5500\n",
            "Total loss:\t189.301 (rec:0.203, round:189.098)\tb=11.00\tcount=6000\n",
            "Total loss:\t164.685 (rec:0.185, round:164.500)\tb=9.88\tcount=6500\n",
            "Total loss:\t138.853 (rec:0.224, round:138.629)\tb=8.75\tcount=7000\n",
            "Total loss:\t111.478 (rec:0.238, round:111.240)\tb=7.62\tcount=7500\n",
            "Total loss:\t82.094 (rec:0.187, round:81.907)\tb=6.50\tcount=8000\n",
            "Total loss:\t50.868 (rec:0.216, round:50.652)\tb=5.38\tcount=8500\n",
            "Total loss:\t21.644 (rec:0.191, round:21.453)\tb=4.25\tcount=9000\n",
            "Total loss:\t3.127 (rec:0.255, round:2.872)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.267 (rec:0.207, round:0.060)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.625 (rec:0.625, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.667 (rec:0.667, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.705 (rec:0.705, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t681.557 (rec:0.618, round:680.939)\tb=20.00\tcount=2000\n",
            "Total loss:\t373.780 (rec:0.633, round:373.147)\tb=18.88\tcount=2500\n",
            "Total loss:\t338.967 (rec:0.605, round:338.361)\tb=17.75\tcount=3000\n",
            "Total loss:\t312.798 (rec:0.604, round:312.193)\tb=16.62\tcount=3500\n",
            "Total loss:\t288.693 (rec:0.587, round:288.106)\tb=15.50\tcount=4000\n",
            "Total loss:\t265.312 (rec:0.553, round:264.759)\tb=14.38\tcount=4500\n",
            "Total loss:\t242.192 (rec:0.625, round:241.567)\tb=13.25\tcount=5000\n",
            "Total loss:\t217.887 (rec:0.642, round:217.245)\tb=12.12\tcount=5500\n",
            "Total loss:\t192.367 (rec:0.608, round:191.758)\tb=11.00\tcount=6000\n",
            "Total loss:\t166.790 (rec:0.604, round:166.186)\tb=9.88\tcount=6500\n",
            "Total loss:\t139.169 (rec:0.541, round:138.628)\tb=8.75\tcount=7000\n",
            "Total loss:\t110.932 (rec:0.569, round:110.363)\tb=7.62\tcount=7500\n",
            "Total loss:\t81.064 (rec:0.620, round:80.445)\tb=6.50\tcount=8000\n",
            "Total loss:\t50.430 (rec:0.586, round:49.844)\tb=5.38\tcount=8500\n",
            "Total loss:\t22.070 (rec:0.616, round:21.453)\tb=4.25\tcount=9000\n",
            "Total loss:\t4.104 (rec:0.632, round:3.471)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.710 (rec:0.584, round:0.126)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.364 (rec:0.364, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.363 (rec:0.363, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.354 (rec:0.354, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2151.681 (rec:0.372, round:2151.309)\tb=20.00\tcount=2000\n",
            "Total loss:\t1102.274 (rec:0.379, round:1101.895)\tb=18.88\tcount=2500\n",
            "Total loss:\t998.980 (rec:0.401, round:998.579)\tb=17.75\tcount=3000\n",
            "Total loss:\t925.410 (rec:0.388, round:925.022)\tb=16.62\tcount=3500\n",
            "Total loss:\t857.746 (rec:0.367, round:857.379)\tb=15.50\tcount=4000\n",
            "Total loss:\t791.289 (rec:0.378, round:790.911)\tb=14.38\tcount=4500\n",
            "Total loss:\t723.151 (rec:0.352, round:722.798)\tb=13.25\tcount=5000\n",
            "Total loss:\t651.642 (rec:0.391, round:651.251)\tb=12.12\tcount=5500\n",
            "Total loss:\t578.003 (rec:0.380, round:577.623)\tb=11.00\tcount=6000\n",
            "Total loss:\t498.588 (rec:0.351, round:498.237)\tb=9.88\tcount=6500\n",
            "Total loss:\t413.048 (rec:0.398, round:412.650)\tb=8.75\tcount=7000\n",
            "Total loss:\t322.470 (rec:0.356, round:322.113)\tb=7.62\tcount=7500\n",
            "Total loss:\t226.042 (rec:0.382, round:225.659)\tb=6.50\tcount=8000\n",
            "Total loss:\t129.579 (rec:0.398, round:129.180)\tb=5.38\tcount=8500\n",
            "Total loss:\t45.047 (rec:0.388, round:44.658)\tb=4.25\tcount=9000\n",
            "Total loss:\t4.192 (rec:0.366, round:3.826)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.433 (rec:0.368, round:0.064)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.612 (rec:0.612, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.623 (rec:0.623, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.617 (rec:0.617, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2758.022 (rec:0.578, round:2757.444)\tb=20.00\tcount=2000\n",
            "Total loss:\t1402.061 (rec:0.603, round:1401.458)\tb=18.88\tcount=2500\n",
            "Total loss:\t1276.373 (rec:0.568, round:1275.804)\tb=17.75\tcount=3000\n",
            "Total loss:\t1184.711 (rec:0.600, round:1184.111)\tb=16.62\tcount=3500\n",
            "Total loss:\t1100.206 (rec:0.565, round:1099.641)\tb=15.50\tcount=4000\n",
            "Total loss:\t1013.611 (rec:0.551, round:1013.060)\tb=14.38\tcount=4500\n",
            "Total loss:\t926.123 (rec:0.568, round:925.556)\tb=13.25\tcount=5000\n",
            "Total loss:\t836.124 (rec:0.601, round:835.522)\tb=12.12\tcount=5500\n",
            "Total loss:\t740.250 (rec:0.582, round:739.667)\tb=11.00\tcount=6000\n",
            "Total loss:\t639.746 (rec:0.575, round:639.171)\tb=9.88\tcount=6500\n",
            "Total loss:\t532.938 (rec:0.557, round:532.381)\tb=8.75\tcount=7000\n",
            "Total loss:\t419.554 (rec:0.592, round:418.962)\tb=7.62\tcount=7500\n",
            "Total loss:\t301.002 (rec:0.620, round:300.381)\tb=6.50\tcount=8000\n",
            "Total loss:\t180.244 (rec:0.601, round:179.643)\tb=5.38\tcount=8500\n",
            "Total loss:\t65.465 (rec:0.681, round:64.784)\tb=4.25\tcount=9000\n",
            "Total loss:\t5.155 (rec:0.574, round:4.581)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.638 (rec:0.602, round:0.036)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.422 (rec:0.422, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.385 (rec:0.385, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.407 (rec:0.407, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t8653.519 (rec:0.375, round:8653.144)\tb=20.00\tcount=2000\n",
            "Total loss:\t4334.658 (rec:0.384, round:4334.274)\tb=18.88\tcount=2500\n",
            "Total loss:\t3954.900 (rec:0.378, round:3954.522)\tb=17.75\tcount=3000\n",
            "Total loss:\t3682.390 (rec:0.388, round:3682.003)\tb=16.62\tcount=3500\n",
            "Total loss:\t3428.519 (rec:0.399, round:3428.121)\tb=15.50\tcount=4000\n",
            "Total loss:\t3174.382 (rec:0.404, round:3173.978)\tb=14.38\tcount=4500\n",
            "Total loss:\t2910.988 (rec:0.397, round:2910.590)\tb=13.25\tcount=5000\n",
            "Total loss:\t2635.343 (rec:0.395, round:2634.948)\tb=12.12\tcount=5500\n",
            "Total loss:\t2344.286 (rec:0.384, round:2343.901)\tb=11.00\tcount=6000\n",
            "Total loss:\t2032.297 (rec:0.383, round:2031.915)\tb=9.88\tcount=6500\n",
            "Total loss:\t1700.134 (rec:0.382, round:1699.752)\tb=8.75\tcount=7000\n",
            "Total loss:\t1346.394 (rec:0.396, round:1345.999)\tb=7.62\tcount=7500\n",
            "Total loss:\t977.391 (rec:0.418, round:976.973)\tb=6.50\tcount=8000\n",
            "Total loss:\t595.210 (rec:0.398, round:594.812)\tb=5.38\tcount=8500\n",
            "Total loss:\t216.276 (rec:0.397, round:215.879)\tb=4.25\tcount=9000\n",
            "Total loss:\t15.506 (rec:0.385, round:15.121)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.517 (rec:0.399, round:0.119)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.593 (rec:0.593, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.602 (rec:0.602, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.512 (rec:0.512, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t11092.100 (rec:0.525, round:11091.574)\tb=20.00\tcount=2000\n",
            "Total loss:\t5478.636 (rec:0.587, round:5478.050)\tb=18.88\tcount=2500\n",
            "Total loss:\t4991.883 (rec:0.560, round:4991.323)\tb=17.75\tcount=3000\n",
            "Total loss:\t4636.804 (rec:0.555, round:4636.250)\tb=16.62\tcount=3500\n",
            "Total loss:\t4301.853 (rec:0.530, round:4301.323)\tb=15.50\tcount=4000\n",
            "Total loss:\t3965.762 (rec:0.553, round:3965.209)\tb=14.38\tcount=4500\n",
            "Total loss:\t3622.842 (rec:0.576, round:3622.266)\tb=13.25\tcount=5000\n",
            "Total loss:\t3266.827 (rec:0.553, round:3266.274)\tb=12.12\tcount=5500\n",
            "Total loss:\t2890.909 (rec:0.562, round:2890.347)\tb=11.00\tcount=6000\n",
            "Total loss:\t2496.341 (rec:0.550, round:2495.791)\tb=9.88\tcount=6500\n",
            "Total loss:\t2076.005 (rec:0.589, round:2075.417)\tb=8.75\tcount=7000\n",
            "Total loss:\t1638.317 (rec:0.559, round:1637.758)\tb=7.62\tcount=7500\n",
            "Total loss:\t1184.476 (rec:0.549, round:1183.927)\tb=6.50\tcount=8000\n",
            "Total loss:\t728.097 (rec:0.580, round:727.517)\tb=5.38\tcount=8500\n",
            "Total loss:\t282.077 (rec:0.541, round:281.536)\tb=4.25\tcount=9000\n",
            "Total loss:\t25.151 (rec:0.557, round:24.594)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.824 (rec:0.555, round:0.268)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.966 (rec:0.966, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.860 (rec:0.860, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.893 (rec:0.893, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t34638.047 (rec:0.797, round:34637.250)\tb=20.00\tcount=2000\n",
            "Total loss:\t17720.938 (rec:0.865, round:17720.072)\tb=18.88\tcount=2500\n",
            "Total loss:\t16203.948 (rec:0.852, round:16203.097)\tb=17.75\tcount=3000\n",
            "Total loss:\t15095.700 (rec:0.815, round:15094.885)\tb=16.62\tcount=3500\n",
            "Total loss:\t14055.562 (rec:0.905, round:14054.656)\tb=15.50\tcount=4000\n",
            "Total loss:\t13006.830 (rec:0.829, round:13006.001)\tb=14.38\tcount=4500\n",
            "Total loss:\t11926.376 (rec:0.892, round:11925.484)\tb=13.25\tcount=5000\n",
            "Total loss:\t10789.514 (rec:0.770, round:10788.743)\tb=12.12\tcount=5500\n",
            "Total loss:\t9586.611 (rec:0.744, round:9585.867)\tb=11.00\tcount=6000\n",
            "Total loss:\t8322.394 (rec:0.779, round:8321.614)\tb=9.88\tcount=6500\n",
            "Total loss:\t6984.506 (rec:0.777, round:6983.729)\tb=8.75\tcount=7000\n",
            "Total loss:\t5569.918 (rec:0.811, round:5569.107)\tb=7.62\tcount=7500\n",
            "Total loss:\t4101.582 (rec:0.739, round:4100.843)\tb=6.50\tcount=8000\n",
            "Total loss:\t2621.052 (rec:0.845, round:2620.207)\tb=5.38\tcount=8500\n",
            "Total loss:\t1216.138 (rec:0.814, round:1215.324)\tb=4.25\tcount=9000\n",
            "Total loss:\t185.066 (rec:0.854, round:184.212)\tb=3.12\tcount=9500\n",
            "Total loss:\t4.663 (rec:0.890, round:3.773)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t35.278 (rec:35.278, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t31.540 (rec:31.540, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t35.715 (rec:35.715, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t44652.129 (rec:34.807, round:44617.320)\tb=20.00\tcount=2000\n",
            "Total loss:\t29018.424 (rec:32.897, round:28985.527)\tb=18.88\tcount=2500\n",
            "Total loss:\t27003.248 (rec:32.787, round:26970.461)\tb=17.75\tcount=3000\n",
            "Total loss:\t25610.990 (rec:28.875, round:25582.115)\tb=16.62\tcount=3500\n",
            "Total loss:\t24371.410 (rec:30.188, round:24341.223)\tb=15.50\tcount=4000\n",
            "Total loss:\t23145.250 (rec:32.773, round:23112.477)\tb=14.38\tcount=4500\n",
            "Total loss:\t21876.541 (rec:37.628, round:21838.914)\tb=13.25\tcount=5000\n",
            "Total loss:\t20528.303 (rec:30.111, round:20498.191)\tb=12.12\tcount=5500\n",
            "Total loss:\t19080.369 (rec:30.448, round:19049.922)\tb=11.00\tcount=6000\n",
            "Total loss:\t17506.098 (rec:28.668, round:17477.430)\tb=9.88\tcount=6500\n",
            "Total loss:\t15763.267 (rec:27.785, round:15735.482)\tb=8.75\tcount=7000\n",
            "Total loss:\t13823.227 (rec:29.991, round:13793.236)\tb=7.62\tcount=7500\n",
            "Total loss:\t11639.257 (rec:30.217, round:11609.039)\tb=6.50\tcount=8000\n",
            "Total loss:\t9174.390 (rec:29.108, round:9145.281)\tb=5.38\tcount=8500\n",
            "Total loss:\t6403.013 (rec:27.403, round:6375.610)\tb=4.25\tcount=9000\n",
            "Total loss:\t3405.280 (rec:28.193, round:3377.087)\tb=3.12\tcount=9500\n",
            "Total loss:\t821.324 (rec:29.320, round:792.004)\tb=2.00\tcount=10000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t48.008 (rec:48.008, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t39.699 (rec:39.699, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t45.173 (rec:45.173, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t4860.545 (rec:39.126, round:4821.419)\tb=20.00\tcount=2000\n",
            "Total loss:\t3144.667 (rec:43.871, round:3100.797)\tb=18.88\tcount=2500\n",
            "Total loss:\t2920.549 (rec:42.884, round:2877.664)\tb=17.75\tcount=3000\n",
            "Total loss:\t2765.082 (rec:42.718, round:2722.364)\tb=16.62\tcount=3500\n",
            "Total loss:\t2624.741 (rec:46.137, round:2578.604)\tb=15.50\tcount=4000\n",
            "Total loss:\t2483.013 (rec:47.128, round:2435.885)\tb=14.38\tcount=4500\n",
            "Total loss:\t2334.996 (rec:44.443, round:2290.554)\tb=13.25\tcount=5000\n",
            "Total loss:\t2178.126 (rec:41.250, round:2136.876)\tb=12.12\tcount=5500\n",
            "Total loss:\t2014.219 (rec:43.766, round:1970.453)\tb=11.00\tcount=6000\n",
            "Total loss:\t1835.028 (rec:39.092, round:1795.937)\tb=9.88\tcount=6500\n",
            "Total loss:\t1644.243 (rec:38.544, round:1605.699)\tb=8.75\tcount=7000\n",
            "Total loss:\t1435.609 (rec:40.316, round:1395.294)\tb=7.62\tcount=7500\n",
            "Total loss:\t1205.017 (rec:43.980, round:1161.038)\tb=6.50\tcount=8000\n",
            "Total loss:\t946.724 (rec:43.088, round:903.636)\tb=5.38\tcount=8500\n",
            "Total loss:\t671.522 (rec:48.893, round:622.628)\tb=4.25\tcount=9000\n",
            "Total loss:\t367.333 (rec:42.899, round:324.434)\tb=3.12\tcount=9500\n",
            "Total loss:\t118.737 (rec:44.981, round:73.756)\tb=2.00\tcount=10000\n",
            "Test: [  0/782]\tTime  8.335 ( 8.335)\tAcc@1  75.00 ( 75.00)\tAcc@5  89.06 ( 89.06)\n",
            "Test: [100/782]\tTime  0.065 ( 0.205)\tAcc@1  67.19 ( 66.97)\tAcc@5  85.94 ( 87.52)\n",
            "Test: [200/782]\tTime  0.065 ( 0.166)\tAcc@1  64.06 ( 67.74)\tAcc@5  89.06 ( 87.70)\n",
            "Test: [300/782]\tTime  0.068 ( 0.153)\tAcc@1  67.19 ( 67.64)\tAcc@5  87.50 ( 87.86)\n",
            "Test: [400/782]\tTime  0.055 ( 0.147)\tAcc@1  68.75 ( 67.87)\tAcc@5  84.38 ( 88.02)\n",
            "Test: [500/782]\tTime  0.068 ( 0.143)\tAcc@1  65.62 ( 67.97)\tAcc@5  90.62 ( 88.19)\n",
            "Test: [600/782]\tTime  0.065 ( 0.139)\tAcc@1  65.62 ( 68.08)\tAcc@5  81.25 ( 88.23)\n",
            "Test: [700/782]\tTime  0.060 ( 0.138)\tAcc@1  67.19 ( 68.20)\tAcc@5  93.75 ( 88.24)\n",
            " * Acc@1 68.206 Acc@5 88.300\n",
            "Full quantization (W4A4) accuracy: 68.20600128173828\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000 --quantize_clipped --additional_levels_num 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.031 (rec:0.031, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.037 (rec:0.037, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.029 (rec:0.029, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t80.525 (rec:0.029, round:80.497)\tb=20.00\tcount=2000\n",
            "Total loss:\t37.427 (rec:0.033, round:37.394)\tb=18.88\tcount=2500\n",
            "Total loss:\t34.099 (rec:0.037, round:34.063)\tb=17.75\tcount=3000\n",
            "Total loss:\t31.647 (rec:0.038, round:31.610)\tb=16.62\tcount=3500\n",
            "Total loss:\t29.438 (rec:0.035, round:29.403)\tb=15.50\tcount=4000\n",
            "Total loss:\t27.154 (rec:0.030, round:27.124)\tb=14.38\tcount=4500\n",
            "Total loss:\t24.975 (rec:0.038, round:24.937)\tb=13.25\tcount=5000\n",
            "Total loss:\t22.287 (rec:0.033, round:22.254)\tb=12.12\tcount=5500\n",
            "Total loss:\t19.568 (rec:0.037, round:19.531)\tb=11.00\tcount=6000\n",
            "Total loss:\t16.936 (rec:0.040, round:16.896)\tb=9.88\tcount=6500\n",
            "Total loss:\t14.052 (rec:0.034, round:14.017)\tb=8.75\tcount=7000\n",
            "Total loss:\t10.608 (rec:0.032, round:10.575)\tb=7.62\tcount=7500\n",
            "Total loss:\t7.125 (rec:0.052, round:7.073)\tb=6.50\tcount=8000\n",
            "Total loss:\t3.575 (rec:0.030, round:3.545)\tb=5.38\tcount=8500\n",
            "Total loss:\t0.770 (rec:0.043, round:0.727)\tb=4.25\tcount=9000\n",
            "Total loss:\t0.075 (rec:0.031, round:0.044)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.034 (rec:0.034, round:0.000)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.203 (rec:0.203, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.206 (rec:0.206, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.299 (rec:0.299, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t648.405 (rec:0.248, round:648.157)\tb=20.00\tcount=2000\n",
            "Total loss:\t353.260 (rec:0.189, round:353.071)\tb=18.88\tcount=2500\n",
            "Total loss:\t323.931 (rec:0.221, round:323.710)\tb=17.75\tcount=3000\n",
            "Total loss:\t301.282 (rec:0.207, round:301.075)\tb=16.62\tcount=3500\n",
            "Total loss:\t280.316 (rec:0.213, round:280.102)\tb=15.50\tcount=4000\n",
            "Total loss:\t259.914 (rec:0.201, round:259.713)\tb=14.38\tcount=4500\n",
            "Total loss:\t238.059 (rec:0.282, round:237.777)\tb=13.25\tcount=5000\n",
            "Total loss:\t215.490 (rec:0.226, round:215.264)\tb=12.12\tcount=5500\n",
            "Total loss:\t192.648 (rec:0.203, round:192.444)\tb=11.00\tcount=6000\n",
            "Total loss:\t168.564 (rec:0.185, round:168.379)\tb=9.88\tcount=6500\n",
            "Total loss:\t143.659 (rec:0.223, round:143.436)\tb=8.75\tcount=7000\n",
            "Total loss:\t115.692 (rec:0.237, round:115.455)\tb=7.62\tcount=7500\n",
            "Total loss:\t85.662 (rec:0.187, round:85.476)\tb=6.50\tcount=8000\n",
            "Total loss:\t53.889 (rec:0.217, round:53.672)\tb=5.38\tcount=8500\n",
            "Total loss:\t22.726 (rec:0.192, round:22.533)\tb=4.25\tcount=9000\n",
            "Total loss:\t3.477 (rec:0.257, round:3.221)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.274 (rec:0.210, round:0.064)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.622 (rec:0.622, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.661 (rec:0.661, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.703 (rec:0.703, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t682.838 (rec:0.615, round:682.222)\tb=20.00\tcount=2000\n",
            "Total loss:\t380.953 (rec:0.633, round:380.320)\tb=18.88\tcount=2500\n",
            "Total loss:\t345.781 (rec:0.603, round:345.177)\tb=17.75\tcount=3000\n",
            "Total loss:\t318.872 (rec:0.604, round:318.268)\tb=16.62\tcount=3500\n",
            "Total loss:\t295.422 (rec:0.586, round:294.836)\tb=15.50\tcount=4000\n",
            "Total loss:\t272.683 (rec:0.555, round:272.128)\tb=14.38\tcount=4500\n",
            "Total loss:\t249.603 (rec:0.623, round:248.980)\tb=13.25\tcount=5000\n",
            "Total loss:\t225.851 (rec:0.643, round:225.208)\tb=12.12\tcount=5500\n",
            "Total loss:\t200.646 (rec:0.606, round:200.040)\tb=11.00\tcount=6000\n",
            "Total loss:\t174.776 (rec:0.605, round:174.171)\tb=9.88\tcount=6500\n",
            "Total loss:\t146.808 (rec:0.543, round:146.265)\tb=8.75\tcount=7000\n",
            "Total loss:\t117.228 (rec:0.569, round:116.659)\tb=7.62\tcount=7500\n",
            "Total loss:\t86.224 (rec:0.623, round:85.601)\tb=6.50\tcount=8000\n",
            "Total loss:\t54.973 (rec:0.588, round:54.384)\tb=5.38\tcount=8500\n",
            "Total loss:\t24.807 (rec:0.619, round:24.187)\tb=4.25\tcount=9000\n",
            "Total loss:\t4.518 (rec:0.637, round:3.881)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.706 (rec:0.591, round:0.115)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.363 (rec:0.363, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.361 (rec:0.361, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.354 (rec:0.354, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2154.172 (rec:0.370, round:2153.802)\tb=20.00\tcount=2000\n",
            "Total loss:\t1118.056 (rec:0.378, round:1117.678)\tb=18.88\tcount=2500\n",
            "Total loss:\t1016.156 (rec:0.401, round:1015.755)\tb=17.75\tcount=3000\n",
            "Total loss:\t942.962 (rec:0.388, round:942.574)\tb=16.62\tcount=3500\n",
            "Total loss:\t875.494 (rec:0.367, round:875.127)\tb=15.50\tcount=4000\n",
            "Total loss:\t808.630 (rec:0.377, round:808.252)\tb=14.38\tcount=4500\n",
            "Total loss:\t740.425 (rec:0.355, round:740.070)\tb=13.25\tcount=5000\n",
            "Total loss:\t669.601 (rec:0.393, round:669.209)\tb=12.12\tcount=5500\n",
            "Total loss:\t593.532 (rec:0.381, round:593.151)\tb=11.00\tcount=6000\n",
            "Total loss:\t511.683 (rec:0.353, round:511.330)\tb=9.88\tcount=6500\n",
            "Total loss:\t423.921 (rec:0.400, round:423.521)\tb=8.75\tcount=7000\n",
            "Total loss:\t331.929 (rec:0.359, round:331.570)\tb=7.62\tcount=7500\n",
            "Total loss:\t234.578 (rec:0.385, round:234.193)\tb=6.50\tcount=8000\n",
            "Total loss:\t135.884 (rec:0.404, round:135.480)\tb=5.38\tcount=8500\n",
            "Total loss:\t48.682 (rec:0.392, round:48.290)\tb=4.25\tcount=9000\n",
            "Total loss:\t5.189 (rec:0.370, round:4.819)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.451 (rec:0.375, round:0.076)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.610 (rec:0.610, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.626 (rec:0.626, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.617 (rec:0.617, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2758.379 (rec:0.580, round:2757.799)\tb=20.00\tcount=2000\n",
            "Total loss:\t1412.542 (rec:0.603, round:1411.938)\tb=18.88\tcount=2500\n",
            "Total loss:\t1286.411 (rec:0.573, round:1285.839)\tb=17.75\tcount=3000\n",
            "Total loss:\t1193.741 (rec:0.603, round:1193.137)\tb=16.62\tcount=3500\n",
            "Total loss:\t1107.994 (rec:0.569, round:1107.425)\tb=15.50\tcount=4000\n",
            "Total loss:\t1024.210 (rec:0.554, round:1023.656)\tb=14.38\tcount=4500\n",
            "Total loss:\t938.650 (rec:0.575, round:938.075)\tb=13.25\tcount=5000\n",
            "Total loss:\t850.569 (rec:0.605, round:849.964)\tb=12.12\tcount=5500\n",
            "Total loss:\t755.655 (rec:0.582, round:755.073)\tb=11.00\tcount=6000\n",
            "Total loss:\t656.211 (rec:0.578, round:655.633)\tb=9.88\tcount=6500\n",
            "Total loss:\t547.159 (rec:0.563, round:546.596)\tb=8.75\tcount=7000\n",
            "Total loss:\t431.845 (rec:0.598, round:431.247)\tb=7.62\tcount=7500\n",
            "Total loss:\t309.960 (rec:0.625, round:309.335)\tb=6.50\tcount=8000\n",
            "Total loss:\t185.395 (rec:0.604, round:184.791)\tb=5.38\tcount=8500\n",
            "Total loss:\t66.410 (rec:0.689, round:65.721)\tb=4.25\tcount=9000\n",
            "Total loss:\t5.560 (rec:0.581, round:4.979)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.642 (rec:0.610, round:0.033)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.424 (rec:0.424, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.384 (rec:0.384, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.409 (rec:0.409, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t8651.776 (rec:0.380, round:8651.396)\tb=20.00\tcount=2000\n",
            "Total loss:\t4374.358 (rec:0.387, round:4373.971)\tb=18.88\tcount=2500\n",
            "Total loss:\t3995.851 (rec:0.382, round:3995.470)\tb=17.75\tcount=3000\n",
            "Total loss:\t3721.352 (rec:0.393, round:3720.960)\tb=16.62\tcount=3500\n",
            "Total loss:\t3468.221 (rec:0.402, round:3467.819)\tb=15.50\tcount=4000\n",
            "Total loss:\t3215.749 (rec:0.409, round:3215.340)\tb=14.38\tcount=4500\n",
            "Total loss:\t2953.326 (rec:0.399, round:2952.927)\tb=13.25\tcount=5000\n",
            "Total loss:\t2675.944 (rec:0.400, round:2675.544)\tb=12.12\tcount=5500\n",
            "Total loss:\t2383.251 (rec:0.387, round:2382.863)\tb=11.00\tcount=6000\n",
            "Total loss:\t2068.206 (rec:0.387, round:2067.818)\tb=9.88\tcount=6500\n",
            "Total loss:\t1734.801 (rec:0.386, round:1734.415)\tb=8.75\tcount=7000\n",
            "Total loss:\t1377.621 (rec:0.400, round:1377.221)\tb=7.62\tcount=7500\n",
            "Total loss:\t999.594 (rec:0.421, round:999.172)\tb=6.50\tcount=8000\n",
            "Total loss:\t611.892 (rec:0.405, round:611.487)\tb=5.38\tcount=8500\n",
            "Total loss:\t225.349 (rec:0.402, round:224.947)\tb=4.25\tcount=9000\n",
            "Total loss:\t16.266 (rec:0.393, round:15.874)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.512 (rec:0.405, round:0.107)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.594 (rec:0.594, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.603 (rec:0.603, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.511 (rec:0.511, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t11098.395 (rec:0.531, round:11097.863)\tb=20.00\tcount=2000\n",
            "Total loss:\t5526.319 (rec:0.588, round:5525.731)\tb=18.88\tcount=2500\n",
            "Total loss:\t5040.789 (rec:0.564, round:5040.225)\tb=17.75\tcount=3000\n",
            "Total loss:\t4688.074 (rec:0.557, round:4687.517)\tb=16.62\tcount=3500\n",
            "Total loss:\t4359.314 (rec:0.528, round:4358.786)\tb=15.50\tcount=4000\n",
            "Total loss:\t4028.010 (rec:0.554, round:4027.457)\tb=14.38\tcount=4500\n",
            "Total loss:\t3687.022 (rec:0.579, round:3686.443)\tb=13.25\tcount=5000\n",
            "Total loss:\t3330.095 (rec:0.557, round:3329.538)\tb=12.12\tcount=5500\n",
            "Total loss:\t2951.980 (rec:0.560, round:2951.419)\tb=11.00\tcount=6000\n",
            "Total loss:\t2554.285 (rec:0.556, round:2553.729)\tb=9.88\tcount=6500\n",
            "Total loss:\t2131.757 (rec:0.591, round:2131.166)\tb=8.75\tcount=7000\n",
            "Total loss:\t1685.569 (rec:0.560, round:1685.009)\tb=7.62\tcount=7500\n",
            "Total loss:\t1222.888 (rec:0.553, round:1222.335)\tb=6.50\tcount=8000\n",
            "Total loss:\t754.657 (rec:0.574, round:754.083)\tb=5.38\tcount=8500\n",
            "Total loss:\t297.064 (rec:0.546, round:296.518)\tb=4.25\tcount=9000\n",
            "Total loss:\t27.806 (rec:0.563, round:27.244)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.901 (rec:0.551, round:0.350)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.973 (rec:0.973, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.856 (rec:0.856, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.878 (rec:0.878, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t34640.031 (rec:0.785, round:34639.246)\tb=20.00\tcount=2000\n",
            "Total loss:\t17868.111 (rec:0.859, round:17867.252)\tb=18.88\tcount=2500\n",
            "Total loss:\t16355.962 (rec:0.847, round:16355.114)\tb=17.75\tcount=3000\n",
            "Total loss:\t15250.634 (rec:0.808, round:15249.825)\tb=16.62\tcount=3500\n",
            "Total loss:\t14216.402 (rec:0.904, round:14215.498)\tb=15.50\tcount=4000\n",
            "Total loss:\t13166.917 (rec:0.828, round:13166.089)\tb=14.38\tcount=4500\n",
            "Total loss:\t12087.553 (rec:0.885, round:12086.668)\tb=13.25\tcount=5000\n",
            "Total loss:\t10954.664 (rec:0.782, round:10953.882)\tb=12.12\tcount=5500\n",
            "Total loss:\t9764.735 (rec:0.744, round:9763.991)\tb=11.00\tcount=6000\n",
            "Total loss:\t8491.627 (rec:0.784, round:8490.843)\tb=9.88\tcount=6500\n",
            "Total loss:\t7145.922 (rec:0.775, round:7145.148)\tb=8.75\tcount=7000\n",
            "Total loss:\t5719.118 (rec:0.811, round:5718.307)\tb=7.62\tcount=7500\n",
            "Total loss:\t4237.908 (rec:0.738, round:4237.169)\tb=6.50\tcount=8000\n",
            "Total loss:\t2731.246 (rec:0.841, round:2730.404)\tb=5.38\tcount=8500\n",
            "Total loss:\t1278.456 (rec:0.824, round:1277.633)\tb=4.25\tcount=9000\n",
            "Total loss:\t199.149 (rec:0.857, round:198.292)\tb=3.12\tcount=9500\n",
            "Total loss:\t5.227 (rec:0.887, round:4.339)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t35.500 (rec:35.500, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t31.969 (rec:31.969, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t35.174 (rec:35.174, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t44651.965 (rec:32.655, round:44619.309)\tb=20.00\tcount=2000\n",
            "Total loss:\t29255.766 (rec:33.078, round:29222.688)\tb=18.88\tcount=2500\n",
            "Total loss:\t27247.994 (rec:32.267, round:27215.727)\tb=17.75\tcount=3000\n",
            "Total loss:\t25869.670 (rec:28.037, round:25841.633)\tb=16.62\tcount=3500\n",
            "Total loss:\t24640.156 (rec:29.785, round:24610.371)\tb=15.50\tcount=4000\n",
            "Total loss:\t23422.117 (rec:31.412, round:23390.705)\tb=14.38\tcount=4500\n",
            "Total loss:\t22170.926 (rec:36.503, round:22134.422)\tb=13.25\tcount=5000\n",
            "Total loss:\t20832.832 (rec:29.806, round:20803.025)\tb=12.12\tcount=5500\n",
            "Total loss:\t19405.576 (rec:30.937, round:19374.639)\tb=11.00\tcount=6000\n",
            "Total loss:\t17837.295 (rec:28.530, round:17808.766)\tb=9.88\tcount=6500\n",
            "Total loss:\t16100.137 (rec:27.587, round:16072.549)\tb=8.75\tcount=7000\n",
            "Total loss:\t14159.229 (rec:28.768, round:14130.460)\tb=7.62\tcount=7500\n",
            "Total loss:\t11966.575 (rec:29.462, round:11937.113)\tb=6.50\tcount=8000\n",
            "Total loss:\t9484.696 (rec:28.869, round:9455.827)\tb=5.38\tcount=8500\n",
            "Total loss:\t6677.970 (rec:27.166, round:6650.804)\tb=4.25\tcount=9000\n",
            "Total loss:\t3616.727 (rec:28.014, round:3588.713)\tb=3.12\tcount=9500\n",
            "Total loss:\t905.827 (rec:29.392, round:876.435)\tb=2.00\tcount=10000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t47.144 (rec:47.144, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t38.509 (rec:38.509, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t45.218 (rec:45.218, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t4858.594 (rec:39.539, round:4819.055)\tb=20.00\tcount=2000\n",
            "Total loss:\t3133.749 (rec:41.467, round:3092.282)\tb=18.88\tcount=2500\n",
            "Total loss:\t2911.866 (rec:42.420, round:2869.446)\tb=17.75\tcount=3000\n",
            "Total loss:\t2754.370 (rec:40.903, round:2713.467)\tb=16.62\tcount=3500\n",
            "Total loss:\t2614.815 (rec:44.237, round:2570.578)\tb=15.50\tcount=4000\n",
            "Total loss:\t2472.814 (rec:43.401, round:2429.412)\tb=14.38\tcount=4500\n",
            "Total loss:\t2323.969 (rec:42.767, round:2281.201)\tb=13.25\tcount=5000\n",
            "Total loss:\t2168.027 (rec:38.492, round:2129.536)\tb=12.12\tcount=5500\n",
            "Total loss:\t2007.357 (rec:40.722, round:1966.636)\tb=11.00\tcount=6000\n",
            "Total loss:\t1825.735 (rec:36.832, round:1788.903)\tb=9.88\tcount=6500\n",
            "Total loss:\t1636.410 (rec:39.464, round:1596.946)\tb=8.75\tcount=7000\n",
            "Total loss:\t1423.156 (rec:35.723, round:1387.433)\tb=7.62\tcount=7500\n",
            "Total loss:\t1198.921 (rec:43.092, round:1155.829)\tb=6.50\tcount=8000\n",
            "Total loss:\t939.040 (rec:41.898, round:897.142)\tb=5.38\tcount=8500\n",
            "Total loss:\t654.336 (rec:39.055, round:615.281)\tb=4.25\tcount=9000\n",
            "Total loss:\t359.254 (rec:41.002, round:318.252)\tb=3.12\tcount=9500\n",
            "Total loss:\t112.526 (rec:41.105, round:71.421)\tb=2.00\tcount=10000\n",
            "Test: [  0/782]\tTime  8.432 ( 8.432)\tAcc@1  71.88 ( 71.88)\tAcc@5  89.06 ( 89.06)\n",
            "Test: [100/782]\tTime  0.064 ( 0.204)\tAcc@1  65.62 ( 66.97)\tAcc@5  84.38 ( 87.59)\n",
            "Test: [200/782]\tTime  0.048 ( 0.163)\tAcc@1  65.62 ( 67.81)\tAcc@5  92.19 ( 87.80)\n",
            "Test: [300/782]\tTime  0.062 ( 0.150)\tAcc@1  65.62 ( 67.69)\tAcc@5  85.94 ( 87.89)\n",
            "Test: [400/782]\tTime  0.064 ( 0.143)\tAcc@1  70.31 ( 67.87)\tAcc@5  85.94 ( 87.98)\n",
            "Test: [500/782]\tTime  0.112 ( 0.140)\tAcc@1  67.19 ( 68.04)\tAcc@5  90.62 ( 88.11)\n",
            "Test: [600/782]\tTime  0.082 ( 0.137)\tAcc@1  65.62 ( 68.15)\tAcc@5  81.25 ( 88.11)\n",
            "Test: [700/782]\tTime  0.062 ( 0.136)\tAcc@1  64.06 ( 68.17)\tAcc@5  93.75 ( 88.18)\n",
            " * Acc@1 68.224 Acc@5 88.248\n",
            "Full quantization (W4A4) accuracy: 68.2239990234375\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000 --quantize_clipped --additional_levels_num 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.031 (rec:0.031, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.037 (rec:0.037, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.029 (rec:0.029, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t80.525 (rec:0.029, round:80.497)\tb=20.00\tcount=2000\n",
            "Total loss:\t37.427 (rec:0.033, round:37.394)\tb=18.88\tcount=2500\n",
            "Total loss:\t34.099 (rec:0.037, round:34.063)\tb=17.75\tcount=3000\n",
            "Total loss:\t31.647 (rec:0.038, round:31.610)\tb=16.62\tcount=3500\n",
            "Total loss:\t29.438 (rec:0.035, round:29.403)\tb=15.50\tcount=4000\n",
            "Total loss:\t27.154 (rec:0.030, round:27.124)\tb=14.38\tcount=4500\n",
            "Total loss:\t24.975 (rec:0.038, round:24.937)\tb=13.25\tcount=5000\n",
            "Total loss:\t22.287 (rec:0.033, round:22.254)\tb=12.12\tcount=5500\n",
            "Total loss:\t19.568 (rec:0.037, round:19.531)\tb=11.00\tcount=6000\n",
            "Total loss:\t16.936 (rec:0.040, round:16.896)\tb=9.88\tcount=6500\n",
            "Total loss:\t14.052 (rec:0.034, round:14.017)\tb=8.75\tcount=7000\n",
            "Total loss:\t10.608 (rec:0.032, round:10.575)\tb=7.62\tcount=7500\n",
            "Total loss:\t7.125 (rec:0.052, round:7.073)\tb=6.50\tcount=8000\n",
            "Total loss:\t3.575 (rec:0.030, round:3.545)\tb=5.38\tcount=8500\n",
            "Total loss:\t0.770 (rec:0.043, round:0.727)\tb=4.25\tcount=9000\n",
            "Total loss:\t0.075 (rec:0.031, round:0.044)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.034 (rec:0.034, round:0.000)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.203 (rec:0.203, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.206 (rec:0.206, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.298 (rec:0.298, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t649.505 (rec:0.247, round:649.258)\tb=20.00\tcount=2000\n",
            "Total loss:\t357.320 (rec:0.188, round:357.132)\tb=18.88\tcount=2500\n",
            "Total loss:\t325.764 (rec:0.221, round:325.544)\tb=17.75\tcount=3000\n",
            "Total loss:\t304.050 (rec:0.207, round:303.843)\tb=16.62\tcount=3500\n",
            "Total loss:\t283.878 (rec:0.213, round:283.664)\tb=15.50\tcount=4000\n",
            "Total loss:\t263.252 (rec:0.201, round:263.051)\tb=14.38\tcount=4500\n",
            "Total loss:\t242.346 (rec:0.281, round:242.065)\tb=13.25\tcount=5000\n",
            "Total loss:\t220.793 (rec:0.226, round:220.567)\tb=12.12\tcount=5500\n",
            "Total loss:\t198.464 (rec:0.203, round:198.261)\tb=11.00\tcount=6000\n",
            "Total loss:\t173.409 (rec:0.186, round:173.223)\tb=9.88\tcount=6500\n",
            "Total loss:\t146.940 (rec:0.223, round:146.717)\tb=8.75\tcount=7000\n",
            "Total loss:\t119.108 (rec:0.237, round:118.871)\tb=7.62\tcount=7500\n",
            "Total loss:\t88.855 (rec:0.187, round:88.667)\tb=6.50\tcount=8000\n",
            "Total loss:\t56.616 (rec:0.219, round:56.397)\tb=5.38\tcount=8500\n",
            "Total loss:\t24.512 (rec:0.194, round:24.318)\tb=4.25\tcount=9000\n",
            "Total loss:\t4.140 (rec:0.261, round:3.880)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.313 (rec:0.214, round:0.100)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.620 (rec:0.620, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.662 (rec:0.662, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.703 (rec:0.703, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t684.080 (rec:0.613, round:683.467)\tb=20.00\tcount=2000\n",
            "Total loss:\t380.631 (rec:0.635, round:379.996)\tb=18.88\tcount=2500\n",
            "Total loss:\t347.475 (rec:0.605, round:346.870)\tb=17.75\tcount=3000\n",
            "Total loss:\t322.630 (rec:0.606, round:322.025)\tb=16.62\tcount=3500\n",
            "Total loss:\t299.641 (rec:0.587, round:299.053)\tb=15.50\tcount=4000\n",
            "Total loss:\t276.838 (rec:0.556, round:276.281)\tb=14.38\tcount=4500\n",
            "Total loss:\t253.102 (rec:0.624, round:252.478)\tb=13.25\tcount=5000\n",
            "Total loss:\t229.631 (rec:0.642, round:228.990)\tb=12.12\tcount=5500\n",
            "Total loss:\t206.098 (rec:0.606, round:205.492)\tb=11.00\tcount=6000\n",
            "Total loss:\t180.293 (rec:0.606, round:179.687)\tb=9.88\tcount=6500\n",
            "Total loss:\t153.136 (rec:0.546, round:152.590)\tb=8.75\tcount=7000\n",
            "Total loss:\t122.429 (rec:0.573, round:121.856)\tb=7.62\tcount=7500\n",
            "Total loss:\t90.092 (rec:0.628, round:89.465)\tb=6.50\tcount=8000\n",
            "Total loss:\t56.716 (rec:0.595, round:56.121)\tb=5.38\tcount=8500\n",
            "Total loss:\t26.106 (rec:0.625, round:25.481)\tb=4.25\tcount=9000\n",
            "Total loss:\t5.671 (rec:0.649, round:5.022)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.846 (rec:0.604, round:0.243)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.366 (rec:0.366, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.363 (rec:0.363, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.357 (rec:0.357, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2155.530 (rec:0.371, round:2155.158)\tb=20.00\tcount=2000\n",
            "Total loss:\t1134.282 (rec:0.383, round:1133.899)\tb=18.88\tcount=2500\n",
            "Total loss:\t1032.904 (rec:0.405, round:1032.498)\tb=17.75\tcount=3000\n",
            "Total loss:\t959.563 (rec:0.391, round:959.172)\tb=16.62\tcount=3500\n",
            "Total loss:\t892.006 (rec:0.370, round:891.635)\tb=15.50\tcount=4000\n",
            "Total loss:\t826.467 (rec:0.382, round:826.085)\tb=14.38\tcount=4500\n",
            "Total loss:\t758.449 (rec:0.359, round:758.090)\tb=13.25\tcount=5000\n",
            "Total loss:\t686.745 (rec:0.397, round:686.348)\tb=12.12\tcount=5500\n",
            "Total loss:\t609.874 (rec:0.387, round:609.487)\tb=11.00\tcount=6000\n",
            "Total loss:\t526.742 (rec:0.359, round:526.383)\tb=9.88\tcount=6500\n",
            "Total loss:\t439.794 (rec:0.406, round:439.388)\tb=8.75\tcount=7000\n",
            "Total loss:\t344.383 (rec:0.367, round:344.015)\tb=7.62\tcount=7500\n",
            "Total loss:\t243.349 (rec:0.394, round:242.955)\tb=6.50\tcount=8000\n",
            "Total loss:\t142.403 (rec:0.415, round:141.988)\tb=5.38\tcount=8500\n",
            "Total loss:\t53.608 (rec:0.406, round:53.202)\tb=4.25\tcount=9000\n",
            "Total loss:\t6.353 (rec:0.387, round:5.966)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.501 (rec:0.392, round:0.109)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.623 (rec:0.623, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.637 (rec:0.637, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.626 (rec:0.626, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2761.681 (rec:0.589, round:2761.093)\tb=20.00\tcount=2000\n",
            "Total loss:\t1430.573 (rec:0.613, round:1429.960)\tb=18.88\tcount=2500\n",
            "Total loss:\t1304.074 (rec:0.584, round:1303.490)\tb=17.75\tcount=3000\n",
            "Total loss:\t1213.059 (rec:0.612, round:1212.447)\tb=16.62\tcount=3500\n",
            "Total loss:\t1128.848 (rec:0.577, round:1128.271)\tb=15.50\tcount=4000\n",
            "Total loss:\t1044.205 (rec:0.566, round:1043.639)\tb=14.38\tcount=4500\n",
            "Total loss:\t959.172 (rec:0.587, round:958.585)\tb=13.25\tcount=5000\n",
            "Total loss:\t870.558 (rec:0.613, round:869.946)\tb=12.12\tcount=5500\n",
            "Total loss:\t776.221 (rec:0.590, round:775.632)\tb=11.00\tcount=6000\n",
            "Total loss:\t673.980 (rec:0.589, round:673.391)\tb=9.88\tcount=6500\n",
            "Total loss:\t564.387 (rec:0.574, round:563.812)\tb=8.75\tcount=7000\n",
            "Total loss:\t447.682 (rec:0.610, round:447.072)\tb=7.62\tcount=7500\n",
            "Total loss:\t324.699 (rec:0.633, round:324.065)\tb=6.50\tcount=8000\n",
            "Total loss:\t194.922 (rec:0.619, round:194.303)\tb=5.38\tcount=8500\n",
            "Total loss:\t73.559 (rec:0.697, round:72.861)\tb=4.25\tcount=9000\n",
            "Total loss:\t6.772 (rec:0.596, round:6.176)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.690 (rec:0.624, round:0.065)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.430 (rec:0.430, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.392 (rec:0.392, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.409 (rec:0.409, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t8652.970 (rec:0.384, round:8652.586)\tb=20.00\tcount=2000\n",
            "Total loss:\t4422.645 (rec:0.394, round:4422.251)\tb=18.88\tcount=2500\n",
            "Total loss:\t4039.848 (rec:0.385, round:4039.463)\tb=17.75\tcount=3000\n",
            "Total loss:\t3767.928 (rec:0.396, round:3767.532)\tb=16.62\tcount=3500\n",
            "Total loss:\t3517.850 (rec:0.409, round:3517.440)\tb=15.50\tcount=4000\n",
            "Total loss:\t3264.009 (rec:0.415, round:3263.594)\tb=14.38\tcount=4500\n",
            "Total loss:\t3002.262 (rec:0.407, round:3001.855)\tb=13.25\tcount=5000\n",
            "Total loss:\t2727.209 (rec:0.406, round:2726.803)\tb=12.12\tcount=5500\n",
            "Total loss:\t2432.546 (rec:0.396, round:2432.150)\tb=11.00\tcount=6000\n",
            "Total loss:\t2117.422 (rec:0.396, round:2117.026)\tb=9.88\tcount=6500\n",
            "Total loss:\t1779.291 (rec:0.396, round:1778.895)\tb=8.75\tcount=7000\n",
            "Total loss:\t1417.031 (rec:0.410, round:1416.622)\tb=7.62\tcount=7500\n",
            "Total loss:\t1030.617 (rec:0.428, round:1030.189)\tb=6.50\tcount=8000\n",
            "Total loss:\t636.015 (rec:0.419, round:635.596)\tb=5.38\tcount=8500\n",
            "Total loss:\t243.254 (rec:0.414, round:242.840)\tb=4.25\tcount=9000\n",
            "Total loss:\t19.984 (rec:0.407, round:19.577)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.593 (rec:0.422, round:0.172)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.598 (rec:0.598, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.610 (rec:0.610, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.523 (rec:0.523, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t11099.499 (rec:0.535, round:11098.965)\tb=20.00\tcount=2000\n",
            "Total loss:\t5586.831 (rec:0.596, round:5586.234)\tb=18.88\tcount=2500\n",
            "Total loss:\t5101.730 (rec:0.572, round:5101.158)\tb=17.75\tcount=3000\n",
            "Total loss:\t4750.055 (rec:0.573, round:4749.482)\tb=16.62\tcount=3500\n",
            "Total loss:\t4420.429 (rec:0.535, round:4419.894)\tb=15.50\tcount=4000\n",
            "Total loss:\t4093.709 (rec:0.565, round:4093.144)\tb=14.38\tcount=4500\n",
            "Total loss:\t3753.309 (rec:0.585, round:3752.724)\tb=13.25\tcount=5000\n",
            "Total loss:\t3402.121 (rec:0.566, round:3401.555)\tb=12.12\tcount=5500\n",
            "Total loss:\t3023.952 (rec:0.575, round:3023.377)\tb=11.00\tcount=6000\n",
            "Total loss:\t2625.185 (rec:0.566, round:2624.618)\tb=9.88\tcount=6500\n",
            "Total loss:\t2198.669 (rec:0.598, round:2198.071)\tb=8.75\tcount=7000\n",
            "Total loss:\t1746.042 (rec:0.566, round:1745.476)\tb=7.62\tcount=7500\n",
            "Total loss:\t1274.322 (rec:0.564, round:1273.758)\tb=6.50\tcount=8000\n",
            "Total loss:\t791.963 (rec:0.592, round:791.371)\tb=5.38\tcount=8500\n",
            "Total loss:\t319.930 (rec:0.559, round:319.371)\tb=4.25\tcount=9000\n",
            "Total loss:\t30.685 (rec:0.573, round:30.112)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.966 (rec:0.571, round:0.395)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.981 (rec:0.981, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.863 (rec:0.863, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.890 (rec:0.890, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t34645.988 (rec:0.806, round:34645.184)\tb=20.00\tcount=2000\n",
            "Total loss:\t18024.596 (rec:0.860, round:18023.734)\tb=18.88\tcount=2500\n",
            "Total loss:\t16503.254 (rec:0.859, round:16502.395)\tb=17.75\tcount=3000\n",
            "Total loss:\t15401.729 (rec:0.823, round:15400.906)\tb=16.62\tcount=3500\n",
            "Total loss:\t14366.205 (rec:0.903, round:14365.302)\tb=15.50\tcount=4000\n",
            "Total loss:\t13327.593 (rec:0.824, round:13326.770)\tb=14.38\tcount=4500\n",
            "Total loss:\t12258.400 (rec:0.878, round:12257.522)\tb=13.25\tcount=5000\n",
            "Total loss:\t11131.666 (rec:0.792, round:11130.874)\tb=12.12\tcount=5500\n",
            "Total loss:\t9939.454 (rec:0.751, round:9938.703)\tb=11.00\tcount=6000\n",
            "Total loss:\t8672.565 (rec:0.792, round:8671.773)\tb=9.88\tcount=6500\n",
            "Total loss:\t7320.001 (rec:0.781, round:7319.220)\tb=8.75\tcount=7000\n",
            "Total loss:\t5888.529 (rec:0.828, round:5887.701)\tb=7.62\tcount=7500\n",
            "Total loss:\t4378.930 (rec:0.748, round:4378.182)\tb=6.50\tcount=8000\n",
            "Total loss:\t2850.809 (rec:0.856, round:2849.953)\tb=5.38\tcount=8500\n",
            "Total loss:\t1365.099 (rec:0.843, round:1364.255)\tb=4.25\tcount=9000\n",
            "Total loss:\t219.701 (rec:0.881, round:218.820)\tb=3.12\tcount=9500\n",
            "Total loss:\t5.644 (rec:0.907, round:4.737)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t35.589 (rec:35.589, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t33.497 (rec:33.497, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t34.262 (rec:34.262, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t44664.375 (rec:33.298, round:44631.078)\tb=20.00\tcount=2000\n",
            "Total loss:\t29555.529 (rec:32.305, round:29523.225)\tb=18.88\tcount=2500\n",
            "Total loss:\t27540.129 (rec:32.141, round:27507.988)\tb=17.75\tcount=3000\n",
            "Total loss:\t26171.441 (rec:28.806, round:26142.637)\tb=16.62\tcount=3500\n",
            "Total loss:\t24943.027 (rec:29.970, round:24913.059)\tb=15.50\tcount=4000\n",
            "Total loss:\t23739.004 (rec:31.719, round:23707.285)\tb=14.38\tcount=4500\n",
            "Total loss:\t22500.531 (rec:36.515, round:22464.016)\tb=13.25\tcount=5000\n",
            "Total loss:\t21180.836 (rec:29.265, round:21151.570)\tb=12.12\tcount=5500\n",
            "Total loss:\t19763.164 (rec:29.806, round:19733.359)\tb=11.00\tcount=6000\n",
            "Total loss:\t18213.098 (rec:28.348, round:18184.750)\tb=9.88\tcount=6500\n",
            "Total loss:\t16496.535 (rec:26.739, round:16469.795)\tb=8.75\tcount=7000\n",
            "Total loss:\t14563.219 (rec:29.556, round:14533.662)\tb=7.62\tcount=7500\n",
            "Total loss:\t12374.016 (rec:28.886, round:12345.129)\tb=6.50\tcount=8000\n",
            "Total loss:\t9871.962 (rec:29.017, round:9842.944)\tb=5.38\tcount=8500\n",
            "Total loss:\t7032.726 (rec:27.864, round:7004.862)\tb=4.25\tcount=9000\n",
            "Total loss:\t3892.098 (rec:28.370, round:3863.728)\tb=3.12\tcount=9500\n",
            "Total loss:\t1026.984 (rec:31.059, round:995.925)\tb=2.00\tcount=10000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t45.663 (rec:45.663, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t36.127 (rec:36.127, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t42.773 (rec:42.773, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t4852.015 (rec:39.300, round:4812.715)\tb=20.00\tcount=2000\n",
            "Total loss:\t3128.631 (rec:40.396, round:3088.235)\tb=18.88\tcount=2500\n",
            "Total loss:\t2902.510 (rec:40.081, round:2862.429)\tb=17.75\tcount=3000\n",
            "Total loss:\t2746.992 (rec:40.607, round:2706.385)\tb=16.62\tcount=3500\n",
            "Total loss:\t2606.471 (rec:45.021, round:2561.450)\tb=15.50\tcount=4000\n",
            "Total loss:\t2458.991 (rec:43.024, round:2415.967)\tb=14.38\tcount=4500\n",
            "Total loss:\t2311.064 (rec:43.202, round:2267.862)\tb=13.25\tcount=5000\n",
            "Total loss:\t2147.317 (rec:38.041, round:2109.276)\tb=12.12\tcount=5500\n",
            "Total loss:\t1982.460 (rec:40.634, round:1941.825)\tb=11.00\tcount=6000\n",
            "Total loss:\t1801.670 (rec:37.030, round:1764.640)\tb=9.88\tcount=6500\n",
            "Total loss:\t1607.279 (rec:37.382, round:1569.897)\tb=8.75\tcount=7000\n",
            "Total loss:\t1394.358 (rec:35.902, round:1358.457)\tb=7.62\tcount=7500\n",
            "Total loss:\t1167.104 (rec:40.361, round:1126.743)\tb=6.50\tcount=8000\n",
            "Total loss:\t914.050 (rec:39.785, round:874.265)\tb=5.38\tcount=8500\n",
            "Total loss:\t636.585 (rec:39.775, round:596.810)\tb=4.25\tcount=9000\n",
            "Total loss:\t347.377 (rec:39.280, round:308.097)\tb=3.12\tcount=9500\n",
            "Total loss:\t113.175 (rec:42.946, round:70.229)\tb=2.00\tcount=10000\n",
            "Test: [  0/782]\tTime  8.711 ( 8.711)\tAcc@1  73.44 ( 73.44)\tAcc@5  89.06 ( 89.06)\n",
            "Test: [100/782]\tTime  0.065 ( 0.207)\tAcc@1  68.75 ( 67.05)\tAcc@5  85.94 ( 87.61)\n",
            "Test: [200/782]\tTime  0.053 ( 0.168)\tAcc@1  65.62 ( 67.74)\tAcc@5  87.50 ( 87.67)\n",
            "Test: [300/782]\tTime  0.069 ( 0.160)\tAcc@1  68.75 ( 67.80)\tAcc@5  89.06 ( 87.83)\n",
            "Test: [400/782]\tTime  0.054 ( 0.153)\tAcc@1  67.19 ( 67.94)\tAcc@5  84.38 ( 87.95)\n",
            "Test: [500/782]\tTime  0.065 ( 0.150)\tAcc@1  64.06 ( 68.03)\tAcc@5  87.50 ( 88.09)\n",
            "Test: [600/782]\tTime  0.049 ( 0.149)\tAcc@1  64.06 ( 68.07)\tAcc@5  81.25 ( 88.10)\n",
            "Test: [700/782]\tTime  0.065 ( 0.147)\tAcc@1  62.50 ( 68.13)\tAcc@5  93.75 ( 88.15)\n",
            " * Acc@1 68.178 Acc@5 88.178\n",
            "Full quantization (W4A4) accuracy: 68.1780014038086\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000 --quantize_clipped --additional_levels_num 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QDrop 3 bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=3, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=3, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.089 (rec:0.089, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.105 (rec:0.105, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.083 (rec:0.083, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t80.167 (rec:0.080, round:80.087)\tb=20.00\tcount=2000\n",
            "Total loss:\t38.348 (rec:0.092, round:38.255)\tb=18.88\tcount=2500\n",
            "Total loss:\t34.791 (rec:0.101, round:34.690)\tb=17.75\tcount=3000\n",
            "Total loss:\t32.427 (rec:0.115, round:32.312)\tb=16.62\tcount=3500\n",
            "Total loss:\t30.169 (rec:0.106, round:30.063)\tb=15.50\tcount=4000\n",
            "Total loss:\t27.912 (rec:0.086, round:27.826)\tb=14.38\tcount=4500\n",
            "Total loss:\t25.723 (rec:0.101, round:25.622)\tb=13.25\tcount=5000\n",
            "Total loss:\t23.493 (rec:0.097, round:23.397)\tb=12.12\tcount=5500\n",
            "Total loss:\t21.077 (rec:0.104, round:20.973)\tb=11.00\tcount=6000\n",
            "Total loss:\t18.280 (rec:0.109, round:18.171)\tb=9.88\tcount=6500\n",
            "Total loss:\t15.354 (rec:0.098, round:15.256)\tb=8.75\tcount=7000\n",
            "Total loss:\t12.058 (rec:0.090, round:11.968)\tb=7.62\tcount=7500\n",
            "Total loss:\t8.563 (rec:0.138, round:8.425)\tb=6.50\tcount=8000\n",
            "Total loss:\t5.157 (rec:0.083, round:5.074)\tb=5.38\tcount=8500\n",
            "Total loss:\t1.799 (rec:0.106, round:1.693)\tb=4.25\tcount=9000\n",
            "Total loss:\t0.221 (rec:0.086, round:0.134)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.099 (rec:0.098, round:0.001)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.606 (rec:0.606, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.593 (rec:0.593, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.763 (rec:0.763, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t641.829 (rec:0.675, round:641.154)\tb=20.00\tcount=2000\n",
            "Total loss:\t363.564 (rec:0.519, round:363.045)\tb=18.88\tcount=2500\n",
            "Total loss:\t330.844 (rec:0.622, round:330.222)\tb=17.75\tcount=3000\n",
            "Total loss:\t305.391 (rec:0.587, round:304.805)\tb=16.62\tcount=3500\n",
            "Total loss:\t282.516 (rec:0.587, round:281.929)\tb=15.50\tcount=4000\n",
            "Total loss:\t258.618 (rec:0.551, round:258.067)\tb=14.38\tcount=4500\n",
            "Total loss:\t235.500 (rec:0.715, round:234.786)\tb=13.25\tcount=5000\n",
            "Total loss:\t212.273 (rec:0.637, round:211.636)\tb=12.12\tcount=5500\n",
            "Total loss:\t188.477 (rec:0.552, round:187.925)\tb=11.00\tcount=6000\n",
            "Total loss:\t165.592 (rec:0.523, round:165.069)\tb=9.88\tcount=6500\n",
            "Total loss:\t140.689 (rec:0.602, round:140.088)\tb=8.75\tcount=7000\n",
            "Total loss:\t114.241 (rec:0.593, round:113.648)\tb=7.62\tcount=7500\n",
            "Total loss:\t87.789 (rec:0.508, round:87.281)\tb=6.50\tcount=8000\n",
            "Total loss:\t59.744 (rec:0.608, round:59.136)\tb=5.38\tcount=8500\n",
            "Total loss:\t29.462 (rec:0.523, round:28.939)\tb=4.25\tcount=9000\n",
            "Total loss:\t6.528 (rec:0.627, round:5.901)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.822 (rec:0.563, round:0.259)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.505 (rec:1.505, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.600 (rec:1.600, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t1.690 (rec:1.690, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t672.257 (rec:1.476, round:670.781)\tb=20.00\tcount=2000\n",
            "Total loss:\t385.229 (rec:1.553, round:383.676)\tb=18.88\tcount=2500\n",
            "Total loss:\t348.514 (rec:1.458, round:347.056)\tb=17.75\tcount=3000\n",
            "Total loss:\t319.779 (rec:1.473, round:318.307)\tb=16.62\tcount=3500\n",
            "Total loss:\t292.004 (rec:1.416, round:290.588)\tb=15.50\tcount=4000\n",
            "Total loss:\t265.447 (rec:1.374, round:264.073)\tb=14.38\tcount=4500\n",
            "Total loss:\t240.256 (rec:1.514, round:238.742)\tb=13.25\tcount=5000\n",
            "Total loss:\t214.922 (rec:1.522, round:213.400)\tb=12.12\tcount=5500\n",
            "Total loss:\t190.145 (rec:1.413, round:188.732)\tb=11.00\tcount=6000\n",
            "Total loss:\t166.168 (rec:1.484, round:164.684)\tb=9.88\tcount=6500\n",
            "Total loss:\t139.978 (rec:1.348, round:138.630)\tb=8.75\tcount=7000\n",
            "Total loss:\t113.374 (rec:1.383, round:111.991)\tb=7.62\tcount=7500\n",
            "Total loss:\t85.464 (rec:1.504, round:83.960)\tb=6.50\tcount=8000\n",
            "Total loss:\t57.607 (rec:1.392, round:56.215)\tb=5.38\tcount=8500\n",
            "Total loss:\t30.320 (rec:1.485, round:28.835)\tb=4.25\tcount=9000\n",
            "Total loss:\t8.479 (rec:1.511, round:6.968)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.813 (rec:1.396, round:0.417)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.859 (rec:0.859, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.836 (rec:0.836, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.830 (rec:0.830, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2136.243 (rec:0.838, round:2135.405)\tb=20.00\tcount=2000\n",
            "Total loss:\t1146.570 (rec:0.843, round:1145.727)\tb=18.88\tcount=2500\n",
            "Total loss:\t1045.346 (rec:0.892, round:1044.453)\tb=17.75\tcount=3000\n",
            "Total loss:\t969.080 (rec:0.853, round:968.227)\tb=16.62\tcount=3500\n",
            "Total loss:\t899.715 (rec:0.829, round:898.886)\tb=15.50\tcount=4000\n",
            "Total loss:\t830.706 (rec:0.852, round:829.853)\tb=14.38\tcount=4500\n",
            "Total loss:\t759.747 (rec:0.828, round:758.919)\tb=13.25\tcount=5000\n",
            "Total loss:\t687.339 (rec:0.871, round:686.468)\tb=12.12\tcount=5500\n",
            "Total loss:\t610.795 (rec:0.837, round:609.958)\tb=11.00\tcount=6000\n",
            "Total loss:\t529.872 (rec:0.794, round:529.078)\tb=9.88\tcount=6500\n",
            "Total loss:\t445.555 (rec:0.858, round:444.697)\tb=8.75\tcount=7000\n",
            "Total loss:\t356.930 (rec:0.801, round:356.129)\tb=7.62\tcount=7500\n",
            "Total loss:\t262.338 (rec:0.847, round:261.492)\tb=6.50\tcount=8000\n",
            "Total loss:\t163.460 (rec:0.886, round:162.574)\tb=5.38\tcount=8500\n",
            "Total loss:\t70.719 (rec:0.844, round:69.875)\tb=4.25\tcount=9000\n",
            "Total loss:\t11.280 (rec:0.799, round:10.482)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.116 (rec:0.839, round:0.278)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.334 (rec:1.334, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.378 (rec:1.378, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t1.325 (rec:1.325, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2736.354 (rec:1.287, round:2735.066)\tb=20.00\tcount=2000\n",
            "Total loss:\t1443.842 (rec:1.320, round:1442.522)\tb=18.88\tcount=2500\n",
            "Total loss:\t1314.901 (rec:1.310, round:1313.590)\tb=17.75\tcount=3000\n",
            "Total loss:\t1218.016 (rec:1.329, round:1216.687)\tb=16.62\tcount=3500\n",
            "Total loss:\t1127.197 (rec:1.274, round:1125.923)\tb=15.50\tcount=4000\n",
            "Total loss:\t1038.906 (rec:1.244, round:1037.662)\tb=14.38\tcount=4500\n",
            "Total loss:\t947.216 (rec:1.304, round:945.912)\tb=13.25\tcount=5000\n",
            "Total loss:\t854.186 (rec:1.327, round:852.860)\tb=12.12\tcount=5500\n",
            "Total loss:\t757.375 (rec:1.244, round:756.131)\tb=11.00\tcount=6000\n",
            "Total loss:\t655.699 (rec:1.255, round:654.444)\tb=9.88\tcount=6500\n",
            "Total loss:\t549.436 (rec:1.283, round:548.152)\tb=8.75\tcount=7000\n",
            "Total loss:\t437.941 (rec:1.323, round:436.618)\tb=7.62\tcount=7500\n",
            "Total loss:\t323.943 (rec:1.339, round:322.603)\tb=6.50\tcount=8000\n",
            "Total loss:\t205.102 (rec:1.307, round:203.795)\tb=5.38\tcount=8500\n",
            "Total loss:\t89.178 (rec:1.460, round:87.718)\tb=4.25\tcount=9000\n",
            "Total loss:\t11.719 (rec:1.254, round:10.465)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.470 (rec:1.324, round:0.146)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.956 (rec:0.956, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.852 (rec:0.852, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.898 (rec:0.898, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t8623.581 (rec:0.882, round:8622.699)\tb=20.00\tcount=2000\n",
            "Total loss:\t4494.654 (rec:0.887, round:4493.767)\tb=18.88\tcount=2500\n",
            "Total loss:\t4110.048 (rec:0.854, round:4109.194)\tb=17.75\tcount=3000\n",
            "Total loss:\t3831.551 (rec:0.880, round:3830.671)\tb=16.62\tcount=3500\n",
            "Total loss:\t3572.924 (rec:0.920, round:3572.004)\tb=15.50\tcount=4000\n",
            "Total loss:\t3314.054 (rec:0.919, round:3313.135)\tb=14.38\tcount=4500\n",
            "Total loss:\t3046.570 (rec:0.902, round:3045.668)\tb=13.25\tcount=5000\n",
            "Total loss:\t2768.542 (rec:0.865, round:2767.677)\tb=12.12\tcount=5500\n",
            "Total loss:\t2472.248 (rec:0.876, round:2471.372)\tb=11.00\tcount=6000\n",
            "Total loss:\t2157.201 (rec:0.873, round:2156.328)\tb=9.88\tcount=6500\n",
            "Total loss:\t1819.285 (rec:0.861, round:1818.424)\tb=8.75\tcount=7000\n",
            "Total loss:\t1462.632 (rec:0.891, round:1461.741)\tb=7.62\tcount=7500\n",
            "Total loss:\t1087.514 (rec:0.901, round:1086.613)\tb=6.50\tcount=8000\n",
            "Total loss:\t695.557 (rec:0.890, round:694.667)\tb=5.38\tcount=8500\n",
            "Total loss:\t303.762 (rec:0.870, round:302.892)\tb=4.25\tcount=9000\n",
            "Total loss:\t35.411 (rec:0.855, round:34.556)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.303 (rec:0.860, round:0.442)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.315 (rec:1.315, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.331 (rec:1.331, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t1.190 (rec:1.190, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t11056.707 (rec:1.192, round:11055.515)\tb=20.00\tcount=2000\n",
            "Total loss:\t5656.575 (rec:1.287, round:5655.289)\tb=18.88\tcount=2500\n",
            "Total loss:\t5161.850 (rec:1.266, round:5160.584)\tb=17.75\tcount=3000\n",
            "Total loss:\t4797.040 (rec:1.214, round:4795.825)\tb=16.62\tcount=3500\n",
            "Total loss:\t4456.896 (rec:1.180, round:4455.716)\tb=15.50\tcount=4000\n",
            "Total loss:\t4116.261 (rec:1.252, round:4115.008)\tb=14.38\tcount=4500\n",
            "Total loss:\t3765.800 (rec:1.273, round:3764.527)\tb=13.25\tcount=5000\n",
            "Total loss:\t3401.926 (rec:1.255, round:3400.670)\tb=12.12\tcount=5500\n",
            "Total loss:\t3019.448 (rec:1.236, round:3018.212)\tb=11.00\tcount=6000\n",
            "Total loss:\t2616.658 (rec:1.238, round:2615.420)\tb=9.88\tcount=6500\n",
            "Total loss:\t2190.600 (rec:1.290, round:2189.310)\tb=8.75\tcount=7000\n",
            "Total loss:\t1745.701 (rec:1.225, round:1744.476)\tb=7.62\tcount=7500\n",
            "Total loss:\t1286.352 (rec:1.226, round:1285.126)\tb=6.50\tcount=8000\n",
            "Total loss:\t823.413 (rec:1.246, round:822.168)\tb=5.38\tcount=8500\n",
            "Total loss:\t374.632 (rec:1.204, round:373.429)\tb=4.25\tcount=9000\n",
            "Total loss:\t53.289 (rec:1.233, round:52.056)\tb=3.12\tcount=9500\n",
            "Total loss:\t2.187 (rec:1.239, round:0.948)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t2.249 (rec:2.249, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.975 (rec:1.975, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t2.035 (rec:2.035, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t34595.254 (rec:1.866, round:34593.387)\tb=20.00\tcount=2000\n",
            "Total loss:\t18337.557 (rec:1.923, round:18335.635)\tb=18.88\tcount=2500\n",
            "Total loss:\t16804.311 (rec:1.949, round:16802.361)\tb=17.75\tcount=3000\n",
            "Total loss:\t15688.062 (rec:1.934, round:15686.128)\tb=16.62\tcount=3500\n",
            "Total loss:\t14639.192 (rec:2.020, round:14637.172)\tb=15.50\tcount=4000\n",
            "Total loss:\t13585.847 (rec:1.893, round:13583.954)\tb=14.38\tcount=4500\n",
            "Total loss:\t12493.873 (rec:2.051, round:12491.822)\tb=13.25\tcount=5000\n",
            "Total loss:\t11352.534 (rec:1.835, round:11350.699)\tb=12.12\tcount=5500\n",
            "Total loss:\t10151.218 (rec:1.738, round:10149.479)\tb=11.00\tcount=6000\n",
            "Total loss:\t8868.187 (rec:1.878, round:8866.309)\tb=9.88\tcount=6500\n",
            "Total loss:\t7511.487 (rec:1.818, round:7509.669)\tb=8.75\tcount=7000\n",
            "Total loss:\t6065.714 (rec:1.812, round:6063.902)\tb=7.62\tcount=7500\n",
            "Total loss:\t4556.219 (rec:1.719, round:4554.500)\tb=6.50\tcount=8000\n",
            "Total loss:\t3016.730 (rec:1.858, round:3014.872)\tb=5.38\tcount=8500\n",
            "Total loss:\t1538.545 (rec:1.854, round:1536.690)\tb=4.25\tcount=9000\n",
            "Total loss:\t341.154 (rec:1.964, round:339.190)\tb=3.12\tcount=9500\n",
            "Total loss:\t15.583 (rec:1.937, round:13.646)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t83.794 (rec:83.794, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t77.486 (rec:77.486, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t84.060 (rec:84.060, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t44661.180 (rec:80.798, round:44580.383)\tb=20.00\tcount=2000\n",
            "Total loss:\t30183.449 (rec:78.741, round:30104.709)\tb=18.88\tcount=2500\n",
            "Total loss:\t28148.045 (rec:76.985, round:28071.061)\tb=17.75\tcount=3000\n",
            "Total loss:\t26756.770 (rec:71.028, round:26685.742)\tb=16.62\tcount=3500\n",
            "Total loss:\t25523.375 (rec:75.989, round:25447.387)\tb=15.50\tcount=4000\n",
            "Total loss:\t24298.541 (rec:75.293, round:24223.248)\tb=14.38\tcount=4500\n",
            "Total loss:\t23054.326 (rec:88.553, round:22965.773)\tb=13.25\tcount=5000\n",
            "Total loss:\t21717.473 (rec:72.364, round:21645.109)\tb=12.12\tcount=5500\n",
            "Total loss:\t20301.590 (rec:76.480, round:20225.109)\tb=11.00\tcount=6000\n",
            "Total loss:\t18746.074 (rec:71.463, round:18674.611)\tb=9.88\tcount=6500\n",
            "Total loss:\t17033.484 (rec:70.856, round:16962.629)\tb=8.75\tcount=7000\n",
            "Total loss:\t15128.808 (rec:70.599, round:15058.209)\tb=7.62\tcount=7500\n",
            "Total loss:\t12970.560 (rec:69.720, round:12900.840)\tb=6.50\tcount=8000\n",
            "Total loss:\t10502.931 (rec:70.253, round:10432.678)\tb=5.38\tcount=8500\n",
            "Total loss:\t7678.341 (rec:63.495, round:7614.846)\tb=4.25\tcount=9000\n",
            "Total loss:\t4506.826 (rec:68.717, round:4438.109)\tb=3.12\tcount=9500\n",
            "Total loss:\t1400.727 (rec:68.983, round:1331.744)\tb=2.00\tcount=10000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t120.006 (rec:120.006, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t106.731 (rec:106.731, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t125.173 (rec:125.173, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t4923.866 (rec:125.571, round:4798.295)\tb=20.00\tcount=2000\n",
            "Total loss:\t3299.157 (rec:118.325, round:3180.833)\tb=18.88\tcount=2500\n",
            "Total loss:\t3077.937 (rec:119.024, round:2958.914)\tb=17.75\tcount=3000\n",
            "Total loss:\t2933.036 (rec:130.491, round:2802.546)\tb=16.62\tcount=3500\n",
            "Total loss:\t2779.782 (rec:119.415, round:2660.367)\tb=15.50\tcount=4000\n",
            "Total loss:\t2635.974 (rec:117.194, round:2518.780)\tb=14.38\tcount=4500\n",
            "Total loss:\t2477.697 (rec:104.181, round:2373.517)\tb=13.25\tcount=5000\n",
            "Total loss:\t2329.809 (rec:112.733, round:2217.076)\tb=12.12\tcount=5500\n",
            "Total loss:\t2176.708 (rec:123.239, round:2053.469)\tb=11.00\tcount=6000\n",
            "Total loss:\t1981.177 (rec:107.192, round:1873.985)\tb=9.88\tcount=6500\n",
            "Total loss:\t1793.941 (rec:112.475, round:1681.466)\tb=8.75\tcount=7000\n",
            "Total loss:\t1573.401 (rec:103.401, round:1470.000)\tb=7.62\tcount=7500\n",
            "Total loss:\t1359.891 (rec:124.381, round:1235.510)\tb=6.50\tcount=8000\n",
            "Total loss:\t1091.618 (rec:114.386, round:977.231)\tb=5.38\tcount=8500\n",
            "Total loss:\t801.642 (rec:109.723, round:691.919)\tb=4.25\tcount=9000\n",
            "Total loss:\t494.628 (rec:110.450, round:384.178)\tb=3.12\tcount=9500\n",
            "Total loss:\t220.832 (rec:116.297, round:104.535)\tb=2.00\tcount=10000\n",
            "Test: [  0/782]\tTime  8.945 ( 8.945)\tAcc@1  70.31 ( 70.31)\tAcc@5  85.94 ( 85.94)\n",
            "Test: [100/782]\tTime  0.067 ( 0.209)\tAcc@1  62.50 ( 64.67)\tAcc@5  84.38 ( 86.08)\n",
            "Test: [200/782]\tTime  0.061 ( 0.168)\tAcc@1  64.06 ( 65.54)\tAcc@5  89.06 ( 86.28)\n",
            "Test: [300/782]\tTime  0.060 ( 0.156)\tAcc@1  67.19 ( 65.50)\tAcc@5  89.06 ( 86.51)\n",
            "Test: [400/782]\tTime  0.065 ( 0.149)\tAcc@1  65.62 ( 65.59)\tAcc@5  84.38 ( 86.62)\n",
            "Test: [500/782]\tTime  0.062 ( 0.145)\tAcc@1  59.38 ( 65.77)\tAcc@5  82.81 ( 86.78)\n",
            "Test: [600/782]\tTime  0.061 ( 0.142)\tAcc@1  60.94 ( 65.81)\tAcc@5  79.69 ( 86.81)\n",
            "Test: [700/782]\tTime  0.067 ( 0.141)\tAcc@1  60.94 ( 65.90)\tAcc@5  90.62 ( 86.90)\n",
            " * Acc@1 65.926 Acc@5 86.926\n",
            "Full quantization (W3A3) accuracy: 65.92599487304688\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 3  --channel_wise --n_bits_a 3  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000 --quantize_clipped --additional_levels_num 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=3, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=3, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.089 (rec:0.089, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.105 (rec:0.105, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.083 (rec:0.083, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t80.167 (rec:0.080, round:80.087)\tb=20.00\tcount=2000\n",
            "Total loss:\t38.348 (rec:0.092, round:38.255)\tb=18.88\tcount=2500\n",
            "Total loss:\t34.791 (rec:0.101, round:34.690)\tb=17.75\tcount=3000\n",
            "Total loss:\t32.427 (rec:0.115, round:32.312)\tb=16.62\tcount=3500\n",
            "Total loss:\t30.169 (rec:0.106, round:30.063)\tb=15.50\tcount=4000\n",
            "Total loss:\t27.912 (rec:0.086, round:27.826)\tb=14.38\tcount=4500\n",
            "Total loss:\t25.723 (rec:0.101, round:25.622)\tb=13.25\tcount=5000\n",
            "Total loss:\t23.493 (rec:0.097, round:23.397)\tb=12.12\tcount=5500\n",
            "Total loss:\t21.077 (rec:0.104, round:20.973)\tb=11.00\tcount=6000\n",
            "Total loss:\t18.280 (rec:0.109, round:18.171)\tb=9.88\tcount=6500\n",
            "Total loss:\t15.354 (rec:0.098, round:15.256)\tb=8.75\tcount=7000\n",
            "Total loss:\t12.058 (rec:0.090, round:11.968)\tb=7.62\tcount=7500\n",
            "Total loss:\t8.563 (rec:0.138, round:8.425)\tb=6.50\tcount=8000\n",
            "Total loss:\t5.157 (rec:0.083, round:5.074)\tb=5.38\tcount=8500\n",
            "Total loss:\t1.799 (rec:0.106, round:1.693)\tb=4.25\tcount=9000\n",
            "Total loss:\t0.221 (rec:0.086, round:0.134)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.099 (rec:0.098, round:0.001)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.604 (rec:0.604, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.590 (rec:0.590, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.759 (rec:0.759, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t643.432 (rec:0.670, round:642.762)\tb=20.00\tcount=2000\n",
            "Total loss:\t367.993 (rec:0.516, round:367.477)\tb=18.88\tcount=2500\n",
            "Total loss:\t336.059 (rec:0.619, round:335.440)\tb=17.75\tcount=3000\n",
            "Total loss:\t312.474 (rec:0.582, round:311.892)\tb=16.62\tcount=3500\n",
            "Total loss:\t290.000 (rec:0.583, round:289.417)\tb=15.50\tcount=4000\n",
            "Total loss:\t267.726 (rec:0.548, round:267.177)\tb=14.38\tcount=4500\n",
            "Total loss:\t245.498 (rec:0.709, round:244.789)\tb=13.25\tcount=5000\n",
            "Total loss:\t222.747 (rec:0.634, round:222.113)\tb=12.12\tcount=5500\n",
            "Total loss:\t199.524 (rec:0.548, round:198.976)\tb=11.00\tcount=6000\n",
            "Total loss:\t174.399 (rec:0.519, round:173.880)\tb=9.88\tcount=6500\n",
            "Total loss:\t148.762 (rec:0.597, round:148.164)\tb=8.75\tcount=7000\n",
            "Total loss:\t121.776 (rec:0.588, round:121.188)\tb=7.62\tcount=7500\n",
            "Total loss:\t93.325 (rec:0.505, round:92.820)\tb=6.50\tcount=8000\n",
            "Total loss:\t63.559 (rec:0.607, round:62.952)\tb=5.38\tcount=8500\n",
            "Total loss:\t33.544 (rec:0.522, round:33.023)\tb=4.25\tcount=9000\n",
            "Total loss:\t8.529 (rec:0.625, round:7.904)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.975 (rec:0.565, round:0.410)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.484 (rec:1.484, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.580 (rec:1.580, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t1.663 (rec:1.663, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t676.373 (rec:1.455, round:674.918)\tb=20.00\tcount=2000\n",
            "Total loss:\t392.591 (rec:1.537, round:391.053)\tb=18.88\tcount=2500\n",
            "Total loss:\t355.845 (rec:1.430, round:354.415)\tb=17.75\tcount=3000\n",
            "Total loss:\t329.116 (rec:1.450, round:327.665)\tb=16.62\tcount=3500\n",
            "Total loss:\t303.513 (rec:1.402, round:302.111)\tb=15.50\tcount=4000\n",
            "Total loss:\t278.379 (rec:1.364, round:277.015)\tb=14.38\tcount=4500\n",
            "Total loss:\t253.566 (rec:1.496, round:252.070)\tb=13.25\tcount=5000\n",
            "Total loss:\t228.825 (rec:1.511, round:227.314)\tb=12.12\tcount=5500\n",
            "Total loss:\t203.686 (rec:1.397, round:202.288)\tb=11.00\tcount=6000\n",
            "Total loss:\t178.611 (rec:1.463, round:177.148)\tb=9.88\tcount=6500\n",
            "Total loss:\t151.821 (rec:1.339, round:150.482)\tb=8.75\tcount=7000\n",
            "Total loss:\t123.731 (rec:1.367, round:122.364)\tb=7.62\tcount=7500\n",
            "Total loss:\t95.038 (rec:1.493, round:93.544)\tb=6.50\tcount=8000\n",
            "Total loss:\t64.957 (rec:1.385, round:63.572)\tb=5.38\tcount=8500\n",
            "Total loss:\t35.245 (rec:1.479, round:33.767)\tb=4.25\tcount=9000\n",
            "Total loss:\t10.815 (rec:1.512, round:9.303)\tb=3.12\tcount=9500\n",
            "Total loss:\t2.152 (rec:1.394, round:0.758)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.849 (rec:0.849, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.839 (rec:0.839, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.820 (rec:0.820, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2141.655 (rec:0.828, round:2140.827)\tb=20.00\tcount=2000\n",
            "Total loss:\t1168.749 (rec:0.839, round:1167.910)\tb=18.88\tcount=2500\n",
            "Total loss:\t1066.189 (rec:0.887, round:1065.303)\tb=17.75\tcount=3000\n",
            "Total loss:\t992.732 (rec:0.849, round:991.883)\tb=16.62\tcount=3500\n",
            "Total loss:\t923.344 (rec:0.824, round:922.521)\tb=15.50\tcount=4000\n",
            "Total loss:\t855.682 (rec:0.853, round:854.830)\tb=14.38\tcount=4500\n",
            "Total loss:\t785.547 (rec:0.828, round:784.719)\tb=13.25\tcount=5000\n",
            "Total loss:\t712.731 (rec:0.871, round:711.860)\tb=12.12\tcount=5500\n",
            "Total loss:\t635.111 (rec:0.836, round:634.275)\tb=11.00\tcount=6000\n",
            "Total loss:\t553.983 (rec:0.792, round:553.191)\tb=9.88\tcount=6500\n",
            "Total loss:\t467.790 (rec:0.857, round:466.933)\tb=8.75\tcount=7000\n",
            "Total loss:\t375.544 (rec:0.802, round:374.741)\tb=7.62\tcount=7500\n",
            "Total loss:\t278.122 (rec:0.851, round:277.271)\tb=6.50\tcount=8000\n",
            "Total loss:\t175.928 (rec:0.887, round:175.040)\tb=5.38\tcount=8500\n",
            "Total loss:\t76.480 (rec:0.851, round:75.630)\tb=4.25\tcount=9000\n",
            "Total loss:\t12.622 (rec:0.808, round:11.814)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.208 (rec:0.854, round:0.354)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.326 (rec:1.326, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.377 (rec:1.377, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t1.319 (rec:1.319, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2745.645 (rec:1.286, round:2744.359)\tb=20.00\tcount=2000\n",
            "Total loss:\t1467.151 (rec:1.312, round:1465.838)\tb=18.88\tcount=2500\n",
            "Total loss:\t1337.959 (rec:1.298, round:1336.661)\tb=17.75\tcount=3000\n",
            "Total loss:\t1243.116 (rec:1.321, round:1241.795)\tb=16.62\tcount=3500\n",
            "Total loss:\t1154.503 (rec:1.269, round:1153.233)\tb=15.50\tcount=4000\n",
            "Total loss:\t1067.212 (rec:1.247, round:1065.965)\tb=14.38\tcount=4500\n",
            "Total loss:\t976.616 (rec:1.303, round:975.313)\tb=13.25\tcount=5000\n",
            "Total loss:\t884.980 (rec:1.328, round:883.652)\tb=12.12\tcount=5500\n",
            "Total loss:\t787.640 (rec:1.245, round:786.395)\tb=11.00\tcount=6000\n",
            "Total loss:\t686.674 (rec:1.259, round:685.415)\tb=9.88\tcount=6500\n",
            "Total loss:\t579.014 (rec:1.276, round:577.738)\tb=8.75\tcount=7000\n",
            "Total loss:\t463.423 (rec:1.324, round:462.099)\tb=7.62\tcount=7500\n",
            "Total loss:\t343.966 (rec:1.347, round:342.619)\tb=6.50\tcount=8000\n",
            "Total loss:\t219.890 (rec:1.311, round:218.579)\tb=5.38\tcount=8500\n",
            "Total loss:\t98.633 (rec:1.464, round:97.169)\tb=4.25\tcount=9000\n",
            "Total loss:\t14.190 (rec:1.259, round:12.931)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.565 (rec:1.327, round:0.238)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.962 (rec:0.962, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.861 (rec:0.861, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.898 (rec:0.898, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t8629.894 (rec:0.882, round:8629.012)\tb=20.00\tcount=2000\n",
            "Total loss:\t4543.720 (rec:0.888, round:4542.833)\tb=18.88\tcount=2500\n",
            "Total loss:\t4159.553 (rec:0.851, round:4158.703)\tb=17.75\tcount=3000\n",
            "Total loss:\t3882.173 (rec:0.885, round:3881.288)\tb=16.62\tcount=3500\n",
            "Total loss:\t3625.216 (rec:0.920, round:3624.296)\tb=15.50\tcount=4000\n",
            "Total loss:\t3370.576 (rec:0.917, round:3369.659)\tb=14.38\tcount=4500\n",
            "Total loss:\t3106.095 (rec:0.907, round:3105.188)\tb=13.25\tcount=5000\n",
            "Total loss:\t2830.346 (rec:0.871, round:2829.475)\tb=12.12\tcount=5500\n",
            "Total loss:\t2537.047 (rec:0.875, round:2536.172)\tb=11.00\tcount=6000\n",
            "Total loss:\t2218.044 (rec:0.880, round:2217.164)\tb=9.88\tcount=6500\n",
            "Total loss:\t1880.658 (rec:0.869, round:1879.789)\tb=8.75\tcount=7000\n",
            "Total loss:\t1518.627 (rec:0.896, round:1517.731)\tb=7.62\tcount=7500\n",
            "Total loss:\t1132.803 (rec:0.901, round:1131.901)\tb=6.50\tcount=8000\n",
            "Total loss:\t734.523 (rec:0.901, round:733.622)\tb=5.38\tcount=8500\n",
            "Total loss:\t332.383 (rec:0.880, round:331.503)\tb=4.25\tcount=9000\n",
            "Total loss:\t43.564 (rec:0.873, round:42.692)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.495 (rec:0.874, round:0.621)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.322 (rec:1.322, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.312 (rec:1.312, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t1.187 (rec:1.187, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t11064.153 (rec:1.179, round:11062.975)\tb=20.00\tcount=2000\n",
            "Total loss:\t5724.518 (rec:1.288, round:5723.230)\tb=18.88\tcount=2500\n",
            "Total loss:\t5229.634 (rec:1.260, round:5228.374)\tb=17.75\tcount=3000\n",
            "Total loss:\t4872.134 (rec:1.204, round:4870.930)\tb=16.62\tcount=3500\n",
            "Total loss:\t4534.047 (rec:1.180, round:4532.867)\tb=15.50\tcount=4000\n",
            "Total loss:\t4196.233 (rec:1.243, round:4194.990)\tb=14.38\tcount=4500\n",
            "Total loss:\t3850.908 (rec:1.269, round:3849.639)\tb=13.25\tcount=5000\n",
            "Total loss:\t3493.685 (rec:1.247, round:3492.438)\tb=12.12\tcount=5500\n",
            "Total loss:\t3113.609 (rec:1.237, round:3112.373)\tb=11.00\tcount=6000\n",
            "Total loss:\t2710.356 (rec:1.227, round:2709.129)\tb=9.88\tcount=6500\n",
            "Total loss:\t2285.066 (rec:1.282, round:2283.784)\tb=8.75\tcount=7000\n",
            "Total loss:\t1831.128 (rec:1.227, round:1829.901)\tb=7.62\tcount=7500\n",
            "Total loss:\t1359.540 (rec:1.226, round:1358.314)\tb=6.50\tcount=8000\n",
            "Total loss:\t876.527 (rec:1.251, round:875.276)\tb=5.38\tcount=8500\n",
            "Total loss:\t405.718 (rec:1.196, round:404.523)\tb=4.25\tcount=9000\n",
            "Total loss:\t60.481 (rec:1.242, round:59.239)\tb=3.12\tcount=9500\n",
            "Total loss:\t2.417 (rec:1.242, round:1.175)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t2.230 (rec:2.230, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.974 (rec:1.974, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t2.049 (rec:2.049, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t34616.566 (rec:1.856, round:34614.711)\tb=20.00\tcount=2000\n",
            "Total loss:\t18546.902 (rec:1.902, round:18545.000)\tb=18.88\tcount=2500\n",
            "Total loss:\t17015.611 (rec:1.941, round:17013.670)\tb=17.75\tcount=3000\n",
            "Total loss:\t15901.896 (rec:1.924, round:15899.972)\tb=16.62\tcount=3500\n",
            "Total loss:\t14854.486 (rec:2.003, round:14852.483)\tb=15.50\tcount=4000\n",
            "Total loss:\t13807.964 (rec:1.876, round:13806.088)\tb=14.38\tcount=4500\n",
            "Total loss:\t12733.877 (rec:2.015, round:12731.862)\tb=13.25\tcount=5000\n",
            "Total loss:\t11599.045 (rec:1.819, round:11597.226)\tb=12.12\tcount=5500\n",
            "Total loss:\t10403.951 (rec:1.710, round:10402.241)\tb=11.00\tcount=6000\n",
            "Total loss:\t9125.008 (rec:1.866, round:9123.143)\tb=9.88\tcount=6500\n",
            "Total loss:\t7757.070 (rec:1.800, round:7755.270)\tb=8.75\tcount=7000\n",
            "Total loss:\t6301.231 (rec:1.814, round:6299.417)\tb=7.62\tcount=7500\n",
            "Total loss:\t4762.190 (rec:1.724, round:4760.466)\tb=6.50\tcount=8000\n",
            "Total loss:\t3187.089 (rec:1.861, round:3185.229)\tb=5.38\tcount=8500\n",
            "Total loss:\t1657.384 (rec:1.840, round:1655.544)\tb=4.25\tcount=9000\n",
            "Total loss:\t380.739 (rec:1.982, round:378.756)\tb=3.12\tcount=9500\n",
            "Total loss:\t17.914 (rec:1.982, round:15.932)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t84.018 (rec:84.018, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t75.068 (rec:75.068, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t84.690 (rec:84.690, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t44669.141 (rec:79.960, round:44589.180)\tb=20.00\tcount=2000\n",
            "Total loss:\t30528.881 (rec:76.324, round:30452.557)\tb=18.88\tcount=2500\n",
            "Total loss:\t28506.078 (rec:76.816, round:28429.262)\tb=17.75\tcount=3000\n",
            "Total loss:\t27123.172 (rec:70.280, round:27052.891)\tb=16.62\tcount=3500\n",
            "Total loss:\t25901.594 (rec:75.121, round:25826.473)\tb=15.50\tcount=4000\n",
            "Total loss:\t24700.678 (rec:71.721, round:24628.957)\tb=14.38\tcount=4500\n",
            "Total loss:\t23474.082 (rec:84.894, round:23389.188)\tb=13.25\tcount=5000\n",
            "Total loss:\t22155.137 (rec:71.013, round:22084.123)\tb=12.12\tcount=5500\n",
            "Total loss:\t20748.502 (rec:73.853, round:20674.648)\tb=11.00\tcount=6000\n",
            "Total loss:\t19211.102 (rec:69.076, round:19142.025)\tb=9.88\tcount=6500\n",
            "Total loss:\t17521.705 (rec:68.369, round:17453.336)\tb=8.75\tcount=7000\n",
            "Total loss:\t15621.410 (rec:67.742, round:15553.668)\tb=7.62\tcount=7500\n",
            "Total loss:\t13462.719 (rec:68.154, round:13394.564)\tb=6.50\tcount=8000\n",
            "Total loss:\t10990.284 (rec:66.079, round:10924.205)\tb=5.38\tcount=8500\n",
            "Total loss:\t8136.407 (rec:62.479, round:8073.928)\tb=4.25\tcount=9000\n",
            "Total loss:\t4883.938 (rec:68.099, round:4815.840)\tb=3.12\tcount=9500\n",
            "Total loss:\t1593.802 (rec:67.554, round:1526.248)\tb=2.00\tcount=10000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t122.174 (rec:122.174, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t93.895 (rec:93.895, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t122.052 (rec:122.052, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t4916.171 (rec:118.056, round:4798.116)\tb=20.00\tcount=2000\n",
            "Total loss:\t3286.274 (rec:108.699, round:3177.574)\tb=18.88\tcount=2500\n",
            "Total loss:\t3063.881 (rec:110.309, round:2953.572)\tb=17.75\tcount=3000\n",
            "Total loss:\t2908.388 (rec:112.816, round:2795.572)\tb=16.62\tcount=3500\n",
            "Total loss:\t2766.618 (rec:114.444, round:2652.175)\tb=15.50\tcount=4000\n",
            "Total loss:\t2623.420 (rec:114.455, round:2508.966)\tb=14.38\tcount=4500\n",
            "Total loss:\t2474.989 (rec:112.733, round:2362.256)\tb=13.25\tcount=5000\n",
            "Total loss:\t2306.193 (rec:98.624, round:2207.570)\tb=12.12\tcount=5500\n",
            "Total loss:\t2160.401 (rec:115.327, round:2045.074)\tb=11.00\tcount=6000\n",
            "Total loss:\t1970.316 (rec:104.431, round:1865.885)\tb=9.88\tcount=6500\n",
            "Total loss:\t1784.632 (rec:110.278, round:1674.354)\tb=8.75\tcount=7000\n",
            "Total loss:\t1562.863 (rec:101.032, round:1461.831)\tb=7.62\tcount=7500\n",
            "Total loss:\t1349.219 (rec:122.339, round:1226.880)\tb=6.50\tcount=8000\n",
            "Total loss:\t1072.856 (rec:104.422, round:968.434)\tb=5.38\tcount=8500\n",
            "Total loss:\t794.773 (rec:111.174, round:683.599)\tb=4.25\tcount=9000\n",
            "Total loss:\t486.219 (rec:106.935, round:379.284)\tb=3.12\tcount=9500\n",
            "Total loss:\t219.864 (rec:116.966, round:102.897)\tb=2.00\tcount=10000\n",
            "Test: [  0/782]\tTime  8.806 ( 8.806)\tAcc@1  71.88 ( 71.88)\tAcc@5  85.94 ( 85.94)\n",
            "Test: [100/782]\tTime  0.069 ( 0.206)\tAcc@1  62.50 ( 64.67)\tAcc@5  82.81 ( 85.88)\n",
            "Test: [200/782]\tTime  0.062 ( 0.166)\tAcc@1  65.62 ( 65.43)\tAcc@5  90.62 ( 86.26)\n",
            "Test: [300/782]\tTime  0.062 ( 0.152)\tAcc@1  62.50 ( 65.26)\tAcc@5  87.50 ( 86.42)\n",
            "Test: [400/782]\tTime  0.061 ( 0.146)\tAcc@1  62.50 ( 65.38)\tAcc@5  82.81 ( 86.50)\n",
            "Test: [500/782]\tTime  0.233 ( 0.141)\tAcc@1  62.50 ( 65.55)\tAcc@5  84.38 ( 86.70)\n",
            "Test: [600/782]\tTime  0.058 ( 0.139)\tAcc@1  60.94 ( 65.65)\tAcc@5  76.56 ( 86.70)\n",
            "Test: [700/782]\tTime  0.062 ( 0.137)\tAcc@1  65.62 ( 65.77)\tAcc@5  89.06 ( 86.80)\n",
            " * Acc@1 65.796 Acc@5 86.826\n",
            "Full quantization (W3A3) accuracy: 65.7959976196289\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 3  --channel_wise --n_bits_a 3  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000 --quantize_clipped --additional_levels_num 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=3, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=3, abit=3, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=3, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=3, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.089 (rec:0.089, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.105 (rec:0.105, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.083 (rec:0.083, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t80.167 (rec:0.080, round:80.087)\tb=20.00\tcount=2000\n",
            "Total loss:\t38.348 (rec:0.092, round:38.255)\tb=18.88\tcount=2500\n",
            "Total loss:\t34.791 (rec:0.101, round:34.690)\tb=17.75\tcount=3000\n",
            "Total loss:\t32.427 (rec:0.115, round:32.312)\tb=16.62\tcount=3500\n",
            "Total loss:\t30.169 (rec:0.106, round:30.063)\tb=15.50\tcount=4000\n",
            "Total loss:\t27.912 (rec:0.086, round:27.826)\tb=14.38\tcount=4500\n",
            "Total loss:\t25.723 (rec:0.101, round:25.622)\tb=13.25\tcount=5000\n",
            "Total loss:\t23.493 (rec:0.097, round:23.397)\tb=12.12\tcount=5500\n",
            "Total loss:\t21.077 (rec:0.104, round:20.973)\tb=11.00\tcount=6000\n",
            "Total loss:\t18.280 (rec:0.109, round:18.171)\tb=9.88\tcount=6500\n",
            "Total loss:\t15.354 (rec:0.098, round:15.256)\tb=8.75\tcount=7000\n",
            "Total loss:\t12.058 (rec:0.090, round:11.968)\tb=7.62\tcount=7500\n",
            "Total loss:\t8.563 (rec:0.138, round:8.425)\tb=6.50\tcount=8000\n",
            "Total loss:\t5.157 (rec:0.083, round:5.074)\tb=5.38\tcount=8500\n",
            "Total loss:\t1.799 (rec:0.106, round:1.693)\tb=4.25\tcount=9000\n",
            "Total loss:\t0.221 (rec:0.086, round:0.134)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.099 (rec:0.098, round:0.001)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.605 (rec:0.605, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.589 (rec:0.589, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.755 (rec:0.755, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t645.180 (rec:0.666, round:644.514)\tb=20.00\tcount=2000\n",
            "Total loss:\t376.180 (rec:0.514, round:375.666)\tb=18.88\tcount=2500\n",
            "Total loss:\t343.857 (rec:0.617, round:343.240)\tb=17.75\tcount=3000\n",
            "Total loss:\t320.707 (rec:0.578, round:320.128)\tb=16.62\tcount=3500\n",
            "Total loss:\t297.745 (rec:0.579, round:297.166)\tb=15.50\tcount=4000\n",
            "Total loss:\t275.915 (rec:0.546, round:275.368)\tb=14.38\tcount=4500\n",
            "Total loss:\t254.319 (rec:0.704, round:253.614)\tb=13.25\tcount=5000\n",
            "Total loss:\t231.566 (rec:0.631, round:230.935)\tb=12.12\tcount=5500\n",
            "Total loss:\t208.019 (rec:0.546, round:207.473)\tb=11.00\tcount=6000\n",
            "Total loss:\t184.156 (rec:0.518, round:183.638)\tb=9.88\tcount=6500\n",
            "Total loss:\t158.145 (rec:0.594, round:157.551)\tb=8.75\tcount=7000\n",
            "Total loss:\t130.334 (rec:0.585, round:129.749)\tb=7.62\tcount=7500\n",
            "Total loss:\t100.887 (rec:0.504, round:100.382)\tb=6.50\tcount=8000\n",
            "Total loss:\t69.820 (rec:0.609, round:69.211)\tb=5.38\tcount=8500\n",
            "Total loss:\t36.861 (rec:0.525, round:36.337)\tb=4.25\tcount=9000\n",
            "Total loss:\t9.752 (rec:0.628, round:9.124)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.168 (rec:0.570, round:0.598)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.463 (rec:1.463, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.553 (rec:1.553, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t1.636 (rec:1.636, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t680.507 (rec:1.431, round:679.076)\tb=20.00\tcount=2000\n",
            "Total loss:\t402.535 (rec:1.524, round:401.011)\tb=18.88\tcount=2500\n",
            "Total loss:\t366.643 (rec:1.417, round:365.226)\tb=17.75\tcount=3000\n",
            "Total loss:\t340.050 (rec:1.428, round:338.622)\tb=16.62\tcount=3500\n",
            "Total loss:\t315.601 (rec:1.385, round:314.217)\tb=15.50\tcount=4000\n",
            "Total loss:\t291.746 (rec:1.355, round:290.391)\tb=14.38\tcount=4500\n",
            "Total loss:\t268.176 (rec:1.469, round:266.707)\tb=13.25\tcount=5000\n",
            "Total loss:\t244.245 (rec:1.488, round:242.757)\tb=12.12\tcount=5500\n",
            "Total loss:\t219.992 (rec:1.379, round:218.612)\tb=11.00\tcount=6000\n",
            "Total loss:\t193.808 (rec:1.448, round:192.361)\tb=9.88\tcount=6500\n",
            "Total loss:\t165.274 (rec:1.334, round:163.940)\tb=8.75\tcount=7000\n",
            "Total loss:\t135.957 (rec:1.359, round:134.598)\tb=7.62\tcount=7500\n",
            "Total loss:\t104.787 (rec:1.484, round:103.302)\tb=6.50\tcount=8000\n",
            "Total loss:\t72.294 (rec:1.382, round:70.911)\tb=5.38\tcount=8500\n",
            "Total loss:\t38.862 (rec:1.476, round:37.386)\tb=4.25\tcount=9000\n",
            "Total loss:\t12.021 (rec:1.527, round:10.494)\tb=3.12\tcount=9500\n",
            "Total loss:\t2.303 (rec:1.412, round:0.890)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.852 (rec:0.852, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.836 (rec:0.836, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.819 (rec:0.819, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2144.970 (rec:0.823, round:2144.148)\tb=20.00\tcount=2000\n",
            "Total loss:\t1183.657 (rec:0.831, round:1182.826)\tb=18.88\tcount=2500\n",
            "Total loss:\t1081.534 (rec:0.886, round:1080.648)\tb=17.75\tcount=3000\n",
            "Total loss:\t1007.718 (rec:0.846, round:1006.872)\tb=16.62\tcount=3500\n",
            "Total loss:\t939.159 (rec:0.822, round:938.337)\tb=15.50\tcount=4000\n",
            "Total loss:\t871.682 (rec:0.848, round:870.834)\tb=14.38\tcount=4500\n",
            "Total loss:\t802.718 (rec:0.829, round:801.890)\tb=13.25\tcount=5000\n",
            "Total loss:\t731.430 (rec:0.872, round:730.558)\tb=12.12\tcount=5500\n",
            "Total loss:\t653.886 (rec:0.834, round:653.052)\tb=11.00\tcount=6000\n",
            "Total loss:\t572.910 (rec:0.798, round:572.112)\tb=9.88\tcount=6500\n",
            "Total loss:\t486.369 (rec:0.859, round:485.510)\tb=8.75\tcount=7000\n",
            "Total loss:\t391.635 (rec:0.809, round:390.826)\tb=7.62\tcount=7500\n",
            "Total loss:\t291.306 (rec:0.863, round:290.443)\tb=6.50\tcount=8000\n",
            "Total loss:\t185.302 (rec:0.903, round:184.399)\tb=5.38\tcount=8500\n",
            "Total loss:\t85.330 (rec:0.869, round:84.461)\tb=4.25\tcount=9000\n",
            "Total loss:\t15.394 (rec:0.834, round:14.560)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.383 (rec:0.885, round:0.499)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.334 (rec:1.334, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.383 (rec:1.383, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t1.326 (rec:1.326, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2748.180 (rec:1.293, round:2746.887)\tb=20.00\tcount=2000\n",
            "Total loss:\t1490.172 (rec:1.324, round:1488.848)\tb=18.88\tcount=2500\n",
            "Total loss:\t1359.945 (rec:1.303, round:1358.642)\tb=17.75\tcount=3000\n",
            "Total loss:\t1265.864 (rec:1.333, round:1264.531)\tb=16.62\tcount=3500\n",
            "Total loss:\t1178.705 (rec:1.279, round:1177.425)\tb=15.50\tcount=4000\n",
            "Total loss:\t1092.778 (rec:1.256, round:1091.521)\tb=14.38\tcount=4500\n",
            "Total loss:\t1007.081 (rec:1.315, round:1005.766)\tb=13.25\tcount=5000\n",
            "Total loss:\t916.445 (rec:1.330, round:915.115)\tb=12.12\tcount=5500\n",
            "Total loss:\t821.351 (rec:1.259, round:820.092)\tb=11.00\tcount=6000\n",
            "Total loss:\t721.713 (rec:1.266, round:720.447)\tb=9.88\tcount=6500\n",
            "Total loss:\t612.575 (rec:1.297, round:611.278)\tb=8.75\tcount=7000\n",
            "Total loss:\t494.213 (rec:1.350, round:492.863)\tb=7.62\tcount=7500\n",
            "Total loss:\t369.268 (rec:1.361, round:367.907)\tb=6.50\tcount=8000\n",
            "Total loss:\t239.109 (rec:1.330, round:237.780)\tb=5.38\tcount=8500\n",
            "Total loss:\t110.203 (rec:1.489, round:108.713)\tb=4.25\tcount=9000\n",
            "Total loss:\t17.739 (rec:1.285, round:16.454)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.713 (rec:1.352, round:0.361)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.981 (rec:0.981, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.865 (rec:0.865, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.901 (rec:0.901, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t8633.960 (rec:0.893, round:8633.067)\tb=20.00\tcount=2000\n",
            "Total loss:\t4615.353 (rec:0.904, round:4614.449)\tb=18.88\tcount=2500\n",
            "Total loss:\t4228.728 (rec:0.863, round:4227.864)\tb=17.75\tcount=3000\n",
            "Total loss:\t3954.917 (rec:0.896, round:3954.021)\tb=16.62\tcount=3500\n",
            "Total loss:\t3698.343 (rec:0.933, round:3697.410)\tb=15.50\tcount=4000\n",
            "Total loss:\t3444.391 (rec:0.935, round:3443.456)\tb=14.38\tcount=4500\n",
            "Total loss:\t3180.841 (rec:0.920, round:3179.921)\tb=13.25\tcount=5000\n",
            "Total loss:\t2903.134 (rec:0.881, round:2902.254)\tb=12.12\tcount=5500\n",
            "Total loss:\t2610.633 (rec:0.889, round:2609.744)\tb=11.00\tcount=6000\n",
            "Total loss:\t2294.982 (rec:0.896, round:2294.086)\tb=9.88\tcount=6500\n",
            "Total loss:\t1956.679 (rec:0.880, round:1955.799)\tb=8.75\tcount=7000\n",
            "Total loss:\t1584.815 (rec:0.909, round:1583.906)\tb=7.62\tcount=7500\n",
            "Total loss:\t1188.618 (rec:0.911, round:1187.708)\tb=6.50\tcount=8000\n",
            "Total loss:\t774.991 (rec:0.923, round:774.068)\tb=5.38\tcount=8500\n",
            "Total loss:\t360.299 (rec:0.899, round:359.400)\tb=4.25\tcount=9000\n",
            "Total loss:\t52.828 (rec:0.902, round:51.926)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.840 (rec:0.912, round:0.928)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.329 (rec:1.329, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t1.329 (rec:1.329, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t1.207 (rec:1.207, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t11070.849 (rec:1.208, round:11069.641)\tb=20.00\tcount=2000\n",
            "Total loss:\t5812.604 (rec:1.299, round:5811.305)\tb=18.88\tcount=2500\n",
            "Total loss:\t5322.524 (rec:1.267, round:5321.257)\tb=17.75\tcount=3000\n",
            "Total loss:\t4963.323 (rec:1.225, round:4962.099)\tb=16.62\tcount=3500\n",
            "Total loss:\t4630.613 (rec:1.191, round:4629.422)\tb=15.50\tcount=4000\n",
            "Total loss:\t4297.780 (rec:1.260, round:4296.521)\tb=14.38\tcount=4500\n",
            "Total loss:\t3953.404 (rec:1.276, round:3952.128)\tb=13.25\tcount=5000\n",
            "Total loss:\t3594.422 (rec:1.262, round:3593.160)\tb=12.12\tcount=5500\n",
            "Total loss:\t3216.782 (rec:1.247, round:3215.535)\tb=11.00\tcount=6000\n",
            "Total loss:\t2815.850 (rec:1.243, round:2814.607)\tb=9.88\tcount=6500\n",
            "Total loss:\t2386.675 (rec:1.283, round:2385.392)\tb=8.75\tcount=7000\n",
            "Total loss:\t1927.239 (rec:1.245, round:1925.994)\tb=7.62\tcount=7500\n",
            "Total loss:\t1445.483 (rec:1.242, round:1444.241)\tb=6.50\tcount=8000\n",
            "Total loss:\t945.843 (rec:1.265, round:944.578)\tb=5.38\tcount=8500\n",
            "Total loss:\t449.026 (rec:1.227, round:447.799)\tb=4.25\tcount=9000\n",
            "Total loss:\t70.090 (rec:1.264, round:68.826)\tb=3.12\tcount=9500\n",
            "Total loss:\t2.696 (rec:1.263, round:1.433)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t2.249 (rec:2.249, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t2.052 (rec:2.052, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t2.049 (rec:2.049, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t34615.805 (rec:1.891, round:34613.914)\tb=20.00\tcount=2000\n",
            "Total loss:\t18782.766 (rec:1.887, round:18780.879)\tb=18.88\tcount=2500\n",
            "Total loss:\t17256.705 (rec:1.951, round:17254.754)\tb=17.75\tcount=3000\n",
            "Total loss:\t16152.085 (rec:1.929, round:16150.156)\tb=16.62\tcount=3500\n",
            "Total loss:\t15119.874 (rec:1.986, round:15117.889)\tb=15.50\tcount=4000\n",
            "Total loss:\t14085.892 (rec:1.875, round:14084.017)\tb=14.38\tcount=4500\n",
            "Total loss:\t13014.442 (rec:2.016, round:13012.427)\tb=13.25\tcount=5000\n",
            "Total loss:\t11883.774 (rec:1.810, round:11881.965)\tb=12.12\tcount=5500\n",
            "Total loss:\t10687.545 (rec:1.733, round:10685.812)\tb=11.00\tcount=6000\n",
            "Total loss:\t9407.581 (rec:1.874, round:9405.707)\tb=9.88\tcount=6500\n",
            "Total loss:\t8039.266 (rec:1.798, round:8037.468)\tb=8.75\tcount=7000\n",
            "Total loss:\t6572.804 (rec:1.799, round:6571.005)\tb=7.62\tcount=7500\n",
            "Total loss:\t5012.562 (rec:1.740, round:5010.821)\tb=6.50\tcount=8000\n",
            "Total loss:\t3392.852 (rec:1.857, round:3390.994)\tb=5.38\tcount=8500\n",
            "Total loss:\t1797.329 (rec:1.872, round:1795.458)\tb=4.25\tcount=9000\n",
            "Total loss:\t430.609 (rec:2.023, round:428.586)\tb=3.12\tcount=9500\n",
            "Total loss:\t20.603 (rec:2.033, round:18.570)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t84.307 (rec:84.307, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t77.085 (rec:77.085, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t82.854 (rec:82.854, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t44669.953 (rec:79.322, round:44590.633)\tb=20.00\tcount=2000\n",
            "Total loss:\t30911.172 (rec:78.562, round:30832.609)\tb=18.88\tcount=2500\n",
            "Total loss:\t28878.992 (rec:74.746, round:28804.246)\tb=17.75\tcount=3000\n",
            "Total loss:\t27509.113 (rec:69.703, round:27439.410)\tb=16.62\tcount=3500\n",
            "Total loss:\t26305.068 (rec:73.599, round:26231.469)\tb=15.50\tcount=4000\n",
            "Total loss:\t25117.377 (rec:70.382, round:25046.996)\tb=14.38\tcount=4500\n",
            "Total loss:\t23922.350 (rec:84.056, round:23838.293)\tb=13.25\tcount=5000\n",
            "Total loss:\t22627.307 (rec:68.815, round:22558.492)\tb=12.12\tcount=5500\n",
            "Total loss:\t21249.311 (rec:72.723, round:21176.588)\tb=11.00\tcount=6000\n",
            "Total loss:\t19735.076 (rec:66.971, round:19668.105)\tb=9.88\tcount=6500\n",
            "Total loss:\t18058.217 (rec:67.104, round:17991.113)\tb=8.75\tcount=7000\n",
            "Total loss:\t16177.754 (rec:65.176, round:16112.578)\tb=7.62\tcount=7500\n",
            "Total loss:\t14037.192 (rec:68.026, round:13969.166)\tb=6.50\tcount=8000\n",
            "Total loss:\t11558.159 (rec:64.667, round:11493.492)\tb=5.38\tcount=8500\n",
            "Total loss:\t8683.781 (rec:62.623, round:8621.158)\tb=4.25\tcount=9000\n",
            "Total loss:\t5361.378 (rec:66.594, round:5294.784)\tb=3.12\tcount=9500\n",
            "Total loss:\t1865.820 (rec:68.582, round:1797.238)\tb=2.00\tcount=10000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t111.678 (rec:111.678, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t87.379 (rec:87.379, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t110.545 (rec:110.545, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t4896.592 (rec:113.890, round:4782.702)\tb=20.00\tcount=2000\n",
            "Total loss:\t3266.656 (rec:103.472, round:3163.185)\tb=18.88\tcount=2500\n",
            "Total loss:\t3042.986 (rec:101.734, round:2941.252)\tb=17.75\tcount=3000\n",
            "Total loss:\t2895.843 (rec:114.264, round:2781.580)\tb=16.62\tcount=3500\n",
            "Total loss:\t2741.836 (rec:104.603, round:2637.233)\tb=15.50\tcount=4000\n",
            "Total loss:\t2607.418 (rec:113.555, round:2493.863)\tb=14.38\tcount=4500\n",
            "Total loss:\t2444.358 (rec:100.289, round:2344.068)\tb=13.25\tcount=5000\n",
            "Total loss:\t2282.831 (rec:96.158, round:2186.673)\tb=12.12\tcount=5500\n",
            "Total loss:\t2132.901 (rec:114.044, round:2018.857)\tb=11.00\tcount=6000\n",
            "Total loss:\t1938.593 (rec:98.237, round:1840.356)\tb=9.88\tcount=6500\n",
            "Total loss:\t1756.983 (rec:110.305, round:1646.678)\tb=8.75\tcount=7000\n",
            "Total loss:\t1534.474 (rec:100.075, round:1434.399)\tb=7.62\tcount=7500\n",
            "Total loss:\t1311.589 (rec:110.509, round:1201.080)\tb=6.50\tcount=8000\n",
            "Total loss:\t1046.421 (rec:101.419, round:945.002)\tb=5.38\tcount=8500\n",
            "Total loss:\t769.325 (rec:103.615, round:665.710)\tb=4.25\tcount=9000\n",
            "Total loss:\t461.874 (rec:96.644, round:365.230)\tb=3.12\tcount=9500\n",
            "Total loss:\t211.109 (rec:114.788, round:96.320)\tb=2.00\tcount=10000\n",
            "Test: [  0/782]\tTime  8.734 ( 8.734)\tAcc@1  68.75 ( 68.75)\tAcc@5  89.06 ( 89.06)\n",
            "Test: [100/782]\tTime  0.067 ( 0.206)\tAcc@1  60.94 ( 64.73)\tAcc@5  84.38 ( 85.80)\n",
            "Test: [200/782]\tTime  0.062 ( 0.166)\tAcc@1  59.38 ( 65.38)\tAcc@5  89.06 ( 86.04)\n",
            "Test: [300/782]\tTime  0.063 ( 0.153)\tAcc@1  68.75 ( 65.59)\tAcc@5  85.94 ( 86.33)\n",
            "Test: [400/782]\tTime  0.063 ( 0.146)\tAcc@1  65.62 ( 65.57)\tAcc@5  84.38 ( 86.41)\n",
            "Test: [500/782]\tTime  0.135 ( 0.142)\tAcc@1  62.50 ( 65.71)\tAcc@5  85.94 ( 86.58)\n",
            "Test: [600/782]\tTime  0.054 ( 0.139)\tAcc@1  64.06 ( 65.72)\tAcc@5  78.12 ( 86.55)\n",
            "Test: [700/782]\tTime  0.062 ( 0.138)\tAcc@1  67.19 ( 65.81)\tAcc@5  89.06 ( 86.68)\n",
            " * Acc@1 65.796 Acc@5 86.686\n",
            "Full quantization (W3A3) accuracy: 65.7959976196289\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 3  --channel_wise --n_bits_a 3  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000 --quantize_clipped --additional_levels_num 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml-course-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "b2d9c01f2b82f2d13cd87b7a6e7c575ae2d1fb70678e14201d3043ea189d77ff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
