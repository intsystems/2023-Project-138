{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Обычная квантизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.865 ( 7.865)\tAcc@1  59.38 ( 59.38)\tAcc@5  79.69 ( 79.69)\n",
            "Test: [100/782]\tTime  0.285 ( 0.209)\tAcc@1  53.12 ( 55.46)\tAcc@5  76.56 ( 79.87)\n",
            "Test: [200/782]\tTime  0.302 ( 0.167)\tAcc@1  57.81 ( 55.88)\tAcc@5  85.94 ( 80.07)\n",
            "Test: [300/782]\tTime  0.308 ( 0.153)\tAcc@1  59.38 ( 56.01)\tAcc@5  82.81 ( 79.68)\n",
            "Test: [400/782]\tTime  0.325 ( 0.147)\tAcc@1  51.56 ( 56.02)\tAcc@5  81.25 ( 79.73)\n",
            "Test: [500/782]\tTime  0.497 ( 0.144)\tAcc@1  54.69 ( 55.90)\tAcc@5  68.75 ( 79.67)\n",
            "Test: [600/782]\tTime  0.295 ( 0.142)\tAcc@1  64.06 ( 55.81)\tAcc@5  79.69 ( 79.64)\n",
            "Test: [700/782]\tTime  0.156 ( 0.140)\tAcc@1  40.62 ( 55.67)\tAcc@5  65.62 ( 79.58)\n",
            " * Acc@1 55.766 Acc@5 79.600\n",
            "Full quantization (W4A4) accuracy: 55.76599884033203\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AdaRound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Test: [  0/782]\tTime  8.874 ( 8.874)\tAcc@1  75.00 ( 75.00)\tAcc@5  92.19 ( 92.19)\n",
            "Test: [100/782]\tTime  0.247 ( 0.208)\tAcc@1  64.06 ( 66.51)\tAcc@5  82.81 ( 86.77)\n",
            "Test: [200/782]\tTime  0.067 ( 0.165)\tAcc@1  71.88 ( 66.43)\tAcc@5  87.50 ( 86.72)\n",
            "Test: [300/782]\tTime  0.243 ( 0.154)\tAcc@1  64.06 ( 66.52)\tAcc@5  89.06 ( 86.94)\n",
            "Test: [400/782]\tTime  0.114 ( 0.149)\tAcc@1  65.62 ( 66.35)\tAcc@5  81.25 ( 86.87)\n",
            "Test: [500/782]\tTime  0.065 ( 0.146)\tAcc@1  65.62 ( 66.14)\tAcc@5  87.50 ( 86.89)\n",
            "Test: [600/782]\tTime  0.070 ( 0.143)\tAcc@1  67.19 ( 66.30)\tAcc@5  89.06 ( 86.86)\n",
            "Test: [700/782]\tTime  0.066 ( 0.141)\tAcc@1  57.81 ( 66.47)\tAcc@5  82.81 ( 87.00)\n",
            " * Acc@1 66.546 Acc@5 87.080\n",
            "Full quantization (W4A4) accuracy: 66.5459976196289\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --iters_w 100"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qdrop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.031 (rec:0.031, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.037 (rec:0.037, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.029 (rec:0.029, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t80.405 (rec:0.029, round:80.376)\tb=20.00\tcount=2000\n",
            "Total loss:\t37.930 (rec:0.033, round:37.896)\tb=18.88\tcount=2500\n",
            "Total loss:\t34.597 (rec:0.037, round:34.561)\tb=17.75\tcount=3000\n",
            "Total loss:\t32.246 (rec:0.038, round:32.209)\tb=16.62\tcount=3500\n",
            "Total loss:\t30.201 (rec:0.035, round:30.166)\tb=15.50\tcount=4000\n",
            "Total loss:\t27.842 (rec:0.030, round:27.812)\tb=14.38\tcount=4500\n",
            "Total loss:\t25.694 (rec:0.038, round:25.656)\tb=13.25\tcount=5000\n",
            "Total loss:\t22.998 (rec:0.033, round:22.965)\tb=12.12\tcount=5500\n",
            "Total loss:\t20.465 (rec:0.037, round:20.429)\tb=11.00\tcount=6000\n",
            "Total loss:\t17.618 (rec:0.040, round:17.578)\tb=9.88\tcount=6500\n",
            "Total loss:\t14.518 (rec:0.034, round:14.483)\tb=8.75\tcount=7000\n",
            "Total loss:\t11.202 (rec:0.032, round:11.170)\tb=7.62\tcount=7500\n",
            "Total loss:\t7.753 (rec:0.052, round:7.701)\tb=6.50\tcount=8000\n",
            "Total loss:\t4.065 (rec:0.030, round:4.036)\tb=5.38\tcount=8500\n",
            "Total loss:\t0.954 (rec:0.043, round:0.911)\tb=4.25\tcount=9000\n",
            "Total loss:\t0.073 (rec:0.031, round:0.042)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.034 (rec:0.034, round:0.000)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.205 (rec:0.205, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.207 (rec:0.207, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.299 (rec:0.299, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t649.994 (rec:0.248, round:649.747)\tb=20.00\tcount=2000\n",
            "Total loss:\t363.475 (rec:0.189, round:363.286)\tb=18.88\tcount=2500\n",
            "Total loss:\t332.886 (rec:0.222, round:332.664)\tb=17.75\tcount=3000\n",
            "Total loss:\t310.951 (rec:0.207, round:310.744)\tb=16.62\tcount=3500\n",
            "Total loss:\t289.925 (rec:0.214, round:289.711)\tb=15.50\tcount=4000\n",
            "Total loss:\t269.947 (rec:0.202, round:269.745)\tb=14.38\tcount=4500\n",
            "Total loss:\t249.082 (rec:0.282, round:248.801)\tb=13.25\tcount=5000\n",
            "Total loss:\t227.961 (rec:0.227, round:227.733)\tb=12.12\tcount=5500\n",
            "Total loss:\t205.290 (rec:0.204, round:205.087)\tb=11.00\tcount=6000\n",
            "Total loss:\t181.360 (rec:0.187, round:181.173)\tb=9.88\tcount=6500\n",
            "Total loss:\t154.760 (rec:0.225, round:154.536)\tb=8.75\tcount=7000\n",
            "Total loss:\t125.718 (rec:0.239, round:125.479)\tb=7.62\tcount=7500\n",
            "Total loss:\t95.577 (rec:0.189, round:95.388)\tb=6.50\tcount=8000\n",
            "Total loss:\t62.823 (rec:0.223, round:62.600)\tb=5.38\tcount=8500\n",
            "Total loss:\t28.844 (rec:0.199, round:28.645)\tb=4.25\tcount=9000\n",
            "Total loss:\t5.097 (rec:0.268, round:4.829)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.390 (rec:0.222, round:0.168)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.626 (rec:0.626, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.665 (rec:0.665, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.701 (rec:0.701, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t687.963 (rec:0.614, round:687.349)\tb=20.00\tcount=2000\n",
            "Total loss:\t392.667 (rec:0.638, round:392.029)\tb=18.88\tcount=2500\n",
            "Total loss:\t358.568 (rec:0.607, round:357.961)\tb=17.75\tcount=3000\n",
            "Total loss:\t333.924 (rec:0.610, round:333.314)\tb=16.62\tcount=3500\n",
            "Total loss:\t311.089 (rec:0.591, round:310.499)\tb=15.50\tcount=4000\n",
            "Total loss:\t288.114 (rec:0.564, round:287.550)\tb=14.38\tcount=4500\n",
            "Total loss:\t265.796 (rec:0.629, round:265.166)\tb=13.25\tcount=5000\n",
            "Total loss:\t242.291 (rec:0.647, round:241.644)\tb=12.12\tcount=5500\n",
            "Total loss:\t217.619 (rec:0.610, round:217.009)\tb=11.00\tcount=6000\n",
            "Total loss:\t191.748 (rec:0.612, round:191.136)\tb=9.88\tcount=6500\n",
            "Total loss:\t162.958 (rec:0.555, round:162.403)\tb=8.75\tcount=7000\n",
            "Total loss:\t132.426 (rec:0.582, round:131.843)\tb=7.62\tcount=7500\n",
            "Total loss:\t100.087 (rec:0.639, round:99.448)\tb=6.50\tcount=8000\n",
            "Total loss:\t64.441 (rec:0.608, round:63.833)\tb=5.38\tcount=8500\n",
            "Total loss:\t30.637 (rec:0.643, round:29.994)\tb=4.25\tcount=9000\n",
            "Total loss:\t7.307 (rec:0.673, round:6.633)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.045 (rec:0.623, round:0.422)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.371 (rec:0.371, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.368 (rec:0.368, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.359 (rec:0.359, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2154.813 (rec:0.372, round:2154.440)\tb=20.00\tcount=2000\n",
            "Total loss:\t1149.644 (rec:0.385, round:1149.259)\tb=18.88\tcount=2500\n",
            "Total loss:\t1047.321 (rec:0.409, round:1046.912)\tb=17.75\tcount=3000\n",
            "Total loss:\t974.658 (rec:0.395, round:974.263)\tb=16.62\tcount=3500\n",
            "Total loss:\t908.334 (rec:0.375, round:907.959)\tb=15.50\tcount=4000\n",
            "Total loss:\t842.780 (rec:0.388, round:842.392)\tb=14.38\tcount=4500\n",
            "Total loss:\t775.087 (rec:0.367, round:774.720)\tb=13.25\tcount=5000\n",
            "Total loss:\t703.424 (rec:0.402, round:703.022)\tb=12.12\tcount=5500\n",
            "Total loss:\t628.336 (rec:0.391, round:627.945)\tb=11.00\tcount=6000\n",
            "Total loss:\t545.293 (rec:0.367, round:544.926)\tb=9.88\tcount=6500\n",
            "Total loss:\t454.617 (rec:0.416, round:454.201)\tb=8.75\tcount=7000\n",
            "Total loss:\t356.217 (rec:0.378, round:355.839)\tb=7.62\tcount=7500\n",
            "Total loss:\t252.694 (rec:0.411, round:252.283)\tb=6.50\tcount=8000\n",
            "Total loss:\t149.126 (rec:0.433, round:148.693)\tb=5.38\tcount=8500\n",
            "Total loss:\t58.151 (rec:0.426, round:57.725)\tb=4.25\tcount=9000\n",
            "Total loss:\t7.207 (rec:0.410, round:6.796)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.541 (rec:0.423, round:0.118)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.634 (rec:0.634, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.653 (rec:0.653, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.635 (rec:0.635, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2760.662 (rec:0.605, round:2760.057)\tb=20.00\tcount=2000\n",
            "Total loss:\t1454.083 (rec:0.630, round:1453.453)\tb=18.88\tcount=2500\n",
            "Total loss:\t1327.229 (rec:0.601, round:1326.628)\tb=17.75\tcount=3000\n",
            "Total loss:\t1234.601 (rec:0.628, round:1233.973)\tb=16.62\tcount=3500\n",
            "Total loss:\t1152.015 (rec:0.594, round:1151.421)\tb=15.50\tcount=4000\n",
            "Total loss:\t1070.099 (rec:0.581, round:1069.518)\tb=14.38\tcount=4500\n",
            "Total loss:\t984.849 (rec:0.607, round:984.243)\tb=13.25\tcount=5000\n",
            "Total loss:\t895.281 (rec:0.629, round:894.651)\tb=12.12\tcount=5500\n",
            "Total loss:\t800.462 (rec:0.607, round:799.855)\tb=11.00\tcount=6000\n",
            "Total loss:\t697.739 (rec:0.603, round:697.136)\tb=9.88\tcount=6500\n",
            "Total loss:\t586.925 (rec:0.597, round:586.328)\tb=8.75\tcount=7000\n",
            "Total loss:\t466.918 (rec:0.634, round:466.284)\tb=7.62\tcount=7500\n",
            "Total loss:\t338.790 (rec:0.659, round:338.131)\tb=6.50\tcount=8000\n",
            "Total loss:\t205.544 (rec:0.638, round:204.905)\tb=5.38\tcount=8500\n",
            "Total loss:\t80.744 (rec:0.729, round:80.014)\tb=4.25\tcount=9000\n",
            "Total loss:\t8.938 (rec:0.619, round:8.319)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.733 (rec:0.649, round:0.084)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.448 (rec:0.448, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.402 (rec:0.402, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.418 (rec:0.418, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t8648.116 (rec:0.396, round:8647.721)\tb=20.00\tcount=2000\n",
            "Total loss:\t4475.710 (rec:0.408, round:4475.302)\tb=18.88\tcount=2500\n",
            "Total loss:\t4092.504 (rec:0.396, round:4092.108)\tb=17.75\tcount=3000\n",
            "Total loss:\t3821.394 (rec:0.412, round:3820.982)\tb=16.62\tcount=3500\n",
            "Total loss:\t3570.285 (rec:0.426, round:3569.859)\tb=15.50\tcount=4000\n",
            "Total loss:\t3321.339 (rec:0.425, round:3320.913)\tb=14.38\tcount=4500\n",
            "Total loss:\t3061.398 (rec:0.423, round:3060.975)\tb=13.25\tcount=5000\n",
            "Total loss:\t2786.662 (rec:0.417, round:2786.245)\tb=12.12\tcount=5500\n",
            "Total loss:\t2494.799 (rec:0.411, round:2494.387)\tb=11.00\tcount=6000\n",
            "Total loss:\t2180.613 (rec:0.409, round:2180.203)\tb=9.88\tcount=6500\n",
            "Total loss:\t1838.909 (rec:0.411, round:1838.499)\tb=8.75\tcount=7000\n",
            "Total loss:\t1472.013 (rec:0.427, round:1471.585)\tb=7.62\tcount=7500\n",
            "Total loss:\t1081.564 (rec:0.443, round:1081.121)\tb=6.50\tcount=8000\n",
            "Total loss:\t671.157 (rec:0.438, round:670.719)\tb=5.38\tcount=8500\n",
            "Total loss:\t269.662 (rec:0.437, round:269.225)\tb=4.25\tcount=9000\n",
            "Total loss:\t25.957 (rec:0.435, round:25.523)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.696 (rec:0.447, round:0.249)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.620 (rec:0.620, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.619 (rec:0.619, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.540 (rec:0.540, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t11102.456 (rec:0.552, round:11101.904)\tb=20.00\tcount=2000\n",
            "Total loss:\t5665.992 (rec:0.610, round:5665.382)\tb=18.88\tcount=2500\n",
            "Total loss:\t5179.420 (rec:0.595, round:5178.825)\tb=17.75\tcount=3000\n",
            "Total loss:\t4830.079 (rec:0.584, round:4829.495)\tb=16.62\tcount=3500\n",
            "Total loss:\t4500.639 (rec:0.551, round:4500.088)\tb=15.50\tcount=4000\n",
            "Total loss:\t4171.738 (rec:0.581, round:4171.157)\tb=14.38\tcount=4500\n",
            "Total loss:\t3834.448 (rec:0.594, round:3833.853)\tb=13.25\tcount=5000\n",
            "Total loss:\t3478.929 (rec:0.579, round:3478.350)\tb=12.12\tcount=5500\n",
            "Total loss:\t3103.567 (rec:0.586, round:3102.980)\tb=11.00\tcount=6000\n",
            "Total loss:\t2701.901 (rec:0.579, round:2701.322)\tb=9.88\tcount=6500\n",
            "Total loss:\t2275.585 (rec:0.616, round:2274.969)\tb=8.75\tcount=7000\n",
            "Total loss:\t1818.417 (rec:0.593, round:1817.824)\tb=7.62\tcount=7500\n",
            "Total loss:\t1337.368 (rec:0.580, round:1336.789)\tb=6.50\tcount=8000\n",
            "Total loss:\t843.294 (rec:0.605, round:842.689)\tb=5.38\tcount=8500\n",
            "Total loss:\t353.106 (rec:0.576, round:352.531)\tb=4.25\tcount=9000\n",
            "Total loss:\t38.167 (rec:0.595, round:37.572)\tb=3.12\tcount=9500\n",
            "Total loss:\t1.161 (rec:0.604, round:0.557)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.014 (rec:1.014, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.896 (rec:0.896, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.922 (rec:0.922, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t34639.891 (rec:0.823, round:34639.066)\tb=20.00\tcount=2000\n",
            "Total loss:\t18271.744 (rec:0.870, round:18270.875)\tb=18.88\tcount=2500\n",
            "Total loss:\t16750.424 (rec:0.879, round:16749.545)\tb=17.75\tcount=3000\n",
            "Total loss:\t15653.088 (rec:0.842, round:15652.246)\tb=16.62\tcount=3500\n",
            "Total loss:\t14626.370 (rec:0.918, round:14625.452)\tb=15.50\tcount=4000\n",
            "Total loss:\t13592.526 (rec:0.841, round:13591.686)\tb=14.38\tcount=4500\n",
            "Total loss:\t12520.422 (rec:0.901, round:12519.521)\tb=13.25\tcount=5000\n",
            "Total loss:\t11399.300 (rec:0.809, round:11398.491)\tb=12.12\tcount=5500\n",
            "Total loss:\t10212.819 (rec:0.775, round:10212.045)\tb=11.00\tcount=6000\n",
            "Total loss:\t8944.345 (rec:0.827, round:8943.518)\tb=9.88\tcount=6500\n",
            "Total loss:\t7583.037 (rec:0.804, round:7582.233)\tb=8.75\tcount=7000\n",
            "Total loss:\t6129.583 (rec:0.840, round:6128.742)\tb=7.62\tcount=7500\n",
            "Total loss:\t4596.435 (rec:0.775, round:4595.660)\tb=6.50\tcount=8000\n",
            "Total loss:\t3021.009 (rec:0.876, round:3020.134)\tb=5.38\tcount=8500\n",
            "Total loss:\t1474.456 (rec:0.876, round:1473.580)\tb=4.25\tcount=9000\n",
            "Total loss:\t252.339 (rec:0.934, round:251.405)\tb=3.12\tcount=9500\n",
            "Total loss:\t6.888 (rec:0.970, round:5.918)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t36.786 (rec:36.786, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t33.761 (rec:33.761, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t35.233 (rec:35.233, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t44656.312 (rec:33.592, round:44622.719)\tb=20.00\tcount=2000\n",
            "Total loss:\t29913.777 (rec:34.217, round:29879.561)\tb=18.88\tcount=2500\n",
            "Total loss:\t27905.316 (rec:33.502, round:27871.814)\tb=17.75\tcount=3000\n",
            "Total loss:\t26544.229 (rec:29.187, round:26515.041)\tb=16.62\tcount=3500\n",
            "Total loss:\t25329.643 (rec:30.615, round:25299.027)\tb=15.50\tcount=4000\n",
            "Total loss:\t24141.492 (rec:32.103, round:24109.389)\tb=14.38\tcount=4500\n",
            "Total loss:\t22917.469 (rec:37.320, round:22880.148)\tb=13.25\tcount=5000\n",
            "Total loss:\t21614.896 (rec:30.557, round:21584.340)\tb=12.12\tcount=5500\n",
            "Total loss:\t20207.613 (rec:30.840, round:20176.773)\tb=11.00\tcount=6000\n",
            "Total loss:\t18666.346 (rec:29.751, round:18636.594)\tb=9.88\tcount=6500\n",
            "Total loss:\t16957.508 (rec:27.740, round:16929.768)\tb=8.75\tcount=7000\n",
            "Total loss:\t15038.961 (rec:29.464, round:15009.497)\tb=7.62\tcount=7500\n",
            "Total loss:\t12851.600 (rec:30.285, round:12821.314)\tb=6.50\tcount=8000\n",
            "Total loss:\t10356.646 (rec:29.916, round:10326.730)\tb=5.38\tcount=8500\n",
            "Total loss:\t7489.341 (rec:28.915, round:7460.426)\tb=4.25\tcount=9000\n",
            "Total loss:\t4271.771 (rec:30.685, round:4241.086)\tb=3.12\tcount=9500\n",
            "Total loss:\t1202.424 (rec:33.217, round:1169.206)\tb=2.00\tcount=10000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t38.096 (rec:38.096, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t36.374 (rec:36.374, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t43.942 (rec:43.942, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t4858.865 (rec:42.300, round:4816.564)\tb=20.00\tcount=2000\n",
            "Total loss:\t3151.776 (rec:39.392, round:3112.384)\tb=18.88\tcount=2500\n",
            "Total loss:\t2929.153 (rec:38.734, round:2890.419)\tb=17.75\tcount=3000\n",
            "Total loss:\t2773.157 (rec:40.083, round:2733.074)\tb=16.62\tcount=3500\n",
            "Total loss:\t2630.389 (rec:42.772, round:2587.616)\tb=15.50\tcount=4000\n",
            "Total loss:\t2487.827 (rec:44.105, round:2443.722)\tb=14.38\tcount=4500\n",
            "Total loss:\t2333.603 (rec:40.010, round:2293.592)\tb=13.25\tcount=5000\n",
            "Total loss:\t2172.606 (rec:35.492, round:2137.114)\tb=12.12\tcount=5500\n",
            "Total loss:\t2009.431 (rec:40.226, round:1969.205)\tb=11.00\tcount=6000\n",
            "Total loss:\t1823.738 (rec:35.415, round:1788.323)\tb=9.88\tcount=6500\n",
            "Total loss:\t1628.673 (rec:38.394, round:1590.279)\tb=8.75\tcount=7000\n",
            "Total loss:\t1416.488 (rec:38.944, round:1377.544)\tb=7.62\tcount=7500\n",
            "Total loss:\t1181.984 (rec:39.589, round:1142.394)\tb=6.50\tcount=8000\n",
            "Total loss:\t923.799 (rec:37.883, round:885.916)\tb=5.38\tcount=8500\n",
            "Total loss:\t649.500 (rec:44.572, round:604.928)\tb=4.25\tcount=9000\n",
            "Total loss:\t349.467 (rec:38.081, round:311.386)\tb=3.12\tcount=9500\n",
            "Total loss:\t110.548 (rec:40.759, round:69.789)\tb=2.00\tcount=10000\n",
            "Test: [  0/782]\tTime  8.672 ( 8.672)\tAcc@1  71.88 ( 71.88)\tAcc@5  89.06 ( 89.06)\n",
            "Test: [100/782]\tTime  0.067 ( 0.209)\tAcc@1  67.19 ( 66.82)\tAcc@5  85.94 ( 87.21)\n",
            "Test: [200/782]\tTime  0.065 ( 0.168)\tAcc@1  62.50 ( 67.62)\tAcc@5  90.62 ( 87.61)\n",
            "Test: [300/782]\tTime  0.046 ( 0.155)\tAcc@1  70.31 ( 67.58)\tAcc@5  85.94 ( 87.78)\n",
            "Test: [400/782]\tTime  0.047 ( 0.147)\tAcc@1  67.19 ( 67.81)\tAcc@5  84.38 ( 87.80)\n",
            "Test: [500/782]\tTime  0.168 ( 0.144)\tAcc@1  65.62 ( 67.90)\tAcc@5  89.06 ( 87.95)\n",
            "Test: [600/782]\tTime  0.350 ( 0.143)\tAcc@1  64.06 ( 68.00)\tAcc@5  81.25 ( 88.00)\n",
            "Test: [700/782]\tTime  0.056 ( 0.141)\tAcc@1  64.06 ( 68.07)\tAcc@5  90.62 ( 88.05)\n",
            " * Acc@1 68.126 Acc@5 88.078\n",
            "Full quantization (W4A4) accuracy: 68.1259994506836\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Дополнительная квантизация"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для того, чтобы смоделировать дополнительную квантизацию, изменим количество уровней в обычной квантизации. Для 4х бит уровней по умлочанию 16, с дополнительной квантизацией добавляется максимум 15 уровней. Будем варьировать количество уровней от 16 до 31, и посмотрим, как при этом меняется accuracy модели."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обычный способ квантизации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sedov\\dev\\2023-Project-139\\code\\QDrop\\quant\\quant_layer.py:148: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorCompare.cpp:652.)\n",
            "  x_min, x_max = torch._aminmax(y, 1)\n",
            "C:\\Users\\sedov\\dev\\2023-Project-139\\code\\QDrop\\quant\\quant_layer.py:179: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceAllOps.cpp:66.)\n",
            "  x_min, x_max = torch._aminmax(x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test: [  0/782]\tTime  7.819 ( 7.819)\tAcc@1  67.19 ( 67.19)\tAcc@5  84.38 ( 84.38)\n",
            "Test: [100/782]\tTime  0.231 ( 0.181)\tAcc@1  70.31 ( 65.35)\tAcc@5  82.81 ( 85.89)\n",
            "Test: [200/782]\tTime  0.215 ( 0.145)\tAcc@1  65.62 ( 65.40)\tAcc@5  87.50 ( 86.26)\n",
            "Test: [300/782]\tTime  0.211 ( 0.132)\tAcc@1  68.75 ( 65.33)\tAcc@5  90.62 ( 86.10)\n",
            "Test: [400/782]\tTime  0.231 ( 0.128)\tAcc@1  64.06 ( 65.48)\tAcc@5  89.06 ( 86.10)\n",
            "Test: [500/782]\tTime  0.489 ( 0.127)\tAcc@1  65.62 ( 65.47)\tAcc@5  78.12 ( 86.12)\n",
            "Test: [600/782]\tTime  0.302 ( 0.127)\tAcc@1  68.75 ( 65.44)\tAcc@5  84.38 ( 86.19)\n",
            "Test: [700/782]\tTime  0.287 ( 0.128)\tAcc@1  59.38 ( 65.28)\tAcc@5  78.12 ( 86.12)\n",
            " * Acc@1 65.354 Acc@5 86.166\n",
            "Full quantization (W4A4) accuracy: 65.35399627685547\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.929 ( 7.929)\tAcc@1  70.31 ( 70.31)\tAcc@5  82.81 ( 82.81)\n",
            "Test: [100/782]\tTime  0.303 ( 0.200)\tAcc@1  62.50 ( 64.81)\tAcc@5  85.94 ( 85.46)\n",
            "Test: [200/782]\tTime  0.282 ( 0.163)\tAcc@1  68.75 ( 64.67)\tAcc@5  89.06 ( 86.05)\n",
            "Test: [300/782]\tTime  0.235 ( 0.152)\tAcc@1  64.06 ( 64.51)\tAcc@5  89.06 ( 85.78)\n",
            "Test: [400/782]\tTime  0.286 ( 0.146)\tAcc@1  64.06 ( 64.58)\tAcc@5  89.06 ( 85.79)\n",
            "Test: [500/782]\tTime  0.490 ( 0.143)\tAcc@1  59.38 ( 64.51)\tAcc@5  73.44 ( 85.80)\n",
            "Test: [600/782]\tTime  0.079 ( 0.141)\tAcc@1  68.75 ( 64.42)\tAcc@5  82.81 ( 85.87)\n",
            "Test: [700/782]\tTime  0.066 ( 0.140)\tAcc@1  59.38 ( 64.29)\tAcc@5  76.56 ( 85.83)\n",
            " * Acc@1 64.364 Acc@5 85.852\n",
            "Full quantization (W4A4) accuracy: 64.36399841308594\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test: [  0/782]\tTime  7.618 ( 7.618)\tAcc@1  67.19 ( 67.19)\tAcc@5  84.38 ( 84.38)\n",
            "Test: [100/782]\tTime  0.277 ( 0.197)\tAcc@1  64.06 ( 62.28)\tAcc@5  81.25 ( 83.80)\n",
            "Test: [200/782]\tTime  0.273 ( 0.161)\tAcc@1  67.19 ( 61.99)\tAcc@5  89.06 ( 84.29)\n",
            "Test: [300/782]\tTime  0.278 ( 0.149)\tAcc@1  67.19 ( 62.06)\tAcc@5  85.94 ( 84.06)\n",
            "Test: [400/782]\tTime  0.312 ( 0.144)\tAcc@1  54.69 ( 62.26)\tAcc@5  85.94 ( 84.16)\n",
            "Test: [500/782]\tTime  0.439 ( 0.141)\tAcc@1  60.94 ( 62.07)\tAcc@5  75.00 ( 84.12)\n",
            "Test: [600/782]\tTime  0.262 ( 0.139)\tAcc@1  67.19 ( 62.08)\tAcc@5  81.25 ( 84.13)\n",
            "Test: [700/782]\tTime  0.063 ( 0.138)\tAcc@1  56.25 ( 61.93)\tAcc@5  70.31 ( 84.04)\n",
            " * Acc@1 61.994 Acc@5 84.030\n",
            "Full quantization (W4A4) accuracy: 61.99399948120117\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4 --channel_wise --n_bits_a 4  --act_quant --order after --wwq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --use_basic_quantization --quantize_clipped --additional_levels_num 3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AdaRound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t638.825 (rec:0.376, round:638.448)\tb=13.25\tcount=50\n",
            "Total loss:\t437.694 (rec:0.389, round:437.305)\tb=2.00\tcount=100\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t684.027 (rec:1.779, round:682.247)\tb=13.25\tcount=50\n",
            "Total loss:\t472.857 (rec:1.795, round:471.061)\tb=2.00\tcount=100\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t2120.859 (rec:1.308, round:2119.550)\tb=13.25\tcount=50\n",
            "Total loss:\t1452.524 (rec:1.348, round:1451.176)\tb=2.00\tcount=100\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t2724.949 (rec:1.940, round:2723.009)\tb=13.25\tcount=50\n",
            "Total loss:\t1864.160 (rec:1.714, round:1862.446)\tb=2.00\tcount=100\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t8470.561 (rec:1.511, round:8469.050)\tb=13.25\tcount=50\n",
            "Total loss:\t5783.137 (rec:1.474, round:5781.663)\tb=2.00\tcount=100\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t10884.512 (rec:1.629, round:10882.883)\tb=13.25\tcount=50\n",
            "Total loss:\t7414.654 (rec:1.644, round:7413.011)\tb=2.00\tcount=100\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t33862.168 (rec:3.762, round:33858.406)\tb=13.25\tcount=50\n",
            "Total loss:\t23096.453 (rec:3.696, round:23092.756)\tb=2.00\tcount=100\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t43732.180 (rec:219.040, round:43513.141)\tb=13.25\tcount=50\n",
            "Total loss:\t30340.670 (rec:217.050, round:30123.621)\tb=2.00\tcount=100\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Test: [  0/782]\tTime  9.687 ( 9.687)\tAcc@1  70.31 ( 70.31)\tAcc@5  92.19 ( 92.19)\n",
            "Test: [100/782]\tTime  0.284 ( 0.217)\tAcc@1  64.06 ( 67.31)\tAcc@5  82.81 ( 87.41)\n",
            "Test: [200/782]\tTime  0.186 ( 0.173)\tAcc@1  73.44 ( 67.29)\tAcc@5  85.94 ( 87.27)\n",
            "Test: [300/782]\tTime  0.290 ( 0.158)\tAcc@1  64.06 ( 67.35)\tAcc@5  87.50 ( 87.52)\n",
            "Test: [400/782]\tTime  0.212 ( 0.150)\tAcc@1  60.94 ( 67.27)\tAcc@5  79.69 ( 87.50)\n",
            "Test: [500/782]\tTime  0.062 ( 0.145)\tAcc@1  70.31 ( 67.18)\tAcc@5  92.19 ( 87.52)\n",
            "Test: [600/782]\tTime  0.066 ( 0.143)\tAcc@1  70.31 ( 67.24)\tAcc@5  90.62 ( 87.53)\n",
            "Test: [700/782]\tTime  0.065 ( 0.140)\tAcc@1  53.12 ( 67.28)\tAcc@5  81.25 ( 87.64)\n",
            " * Acc@1 67.376 Acc@5 87.718\n",
            "Full quantization (W4A4) accuracy: 67.3759994506836\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 1.0 --prob 1.0 --batch_size 64 --keep_cpu --iters_w 100 --quantize_clipped"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QDrop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using Pytorch Dataset\n",
            "Setting the first and the last layer to 8-bit\n",
            "check the model!\n",
            "QuantModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=False\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): ReLU(inplace=True)\n",
            "    )\n",
            "    (bn1): StraightThrough()\n",
            "    (relu): StraightThrough()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (downsample): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      )\n",
            "      (1): QuantBasicBlock(\n",
            "        (conv1): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=False\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): ReLU(inplace=True)\n",
            "        )\n",
            "        (conv2): QuantModule(\n",
            "          wbit=4, abit=4, disable_act_quant=True\n",
            "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "          (activation_function): StraightThrough()\n",
            "        )\n",
            "        (activation_function): ReLU(inplace=True)\n",
            "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantModule(\n",
            "      wbit=8, abit=4, disable_act_quant=True\n",
            "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
            "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
            "      (activation_function): StraightThrough()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Reconstruction for layer conv1\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.031 (rec:0.031, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.037 (rec:0.037, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.029 (rec:0.029, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t80.340 (rec:0.029, round:80.312)\tb=20.00\tcount=2000\n",
            "Total loss:\t36.018 (rec:0.033, round:35.985)\tb=18.88\tcount=2500\n",
            "Total loss:\t32.396 (rec:0.037, round:32.360)\tb=17.75\tcount=3000\n",
            "Total loss:\t30.439 (rec:0.038, round:30.401)\tb=16.62\tcount=3500\n",
            "Total loss:\t28.335 (rec:0.035, round:28.300)\tb=15.50\tcount=4000\n",
            "Total loss:\t26.221 (rec:0.030, round:26.191)\tb=14.38\tcount=4500\n",
            "Total loss:\t23.863 (rec:0.038, round:23.825)\tb=13.25\tcount=5000\n",
            "Total loss:\t21.366 (rec:0.033, round:21.333)\tb=12.12\tcount=5500\n",
            "Total loss:\t18.846 (rec:0.036, round:18.809)\tb=11.00\tcount=6000\n",
            "Total loss:\t16.189 (rec:0.040, round:16.149)\tb=9.88\tcount=6500\n",
            "Total loss:\t13.376 (rec:0.034, round:13.342)\tb=8.75\tcount=7000\n",
            "Total loss:\t10.504 (rec:0.032, round:10.472)\tb=7.62\tcount=7500\n",
            "Total loss:\t7.115 (rec:0.052, round:7.063)\tb=6.50\tcount=8000\n",
            "Total loss:\t3.385 (rec:0.030, round:3.355)\tb=5.38\tcount=8500\n",
            "Total loss:\t0.831 (rec:0.043, round:0.787)\tb=4.25\tcount=9000\n",
            "Total loss:\t0.078 (rec:0.031, round:0.047)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.034 (rec:0.034, round:0.000)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.205 (rec:0.205, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.208 (rec:0.208, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.301 (rec:0.301, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t648.545 (rec:0.250, round:648.296)\tb=20.00\tcount=2000\n",
            "Total loss:\t350.885 (rec:0.189, round:350.696)\tb=18.88\tcount=2500\n",
            "Total loss:\t320.805 (rec:0.222, round:320.583)\tb=17.75\tcount=3000\n",
            "Total loss:\t298.009 (rec:0.208, round:297.801)\tb=16.62\tcount=3500\n",
            "Total loss:\t276.797 (rec:0.214, round:276.583)\tb=15.50\tcount=4000\n",
            "Total loss:\t255.254 (rec:0.202, round:255.052)\tb=14.38\tcount=4500\n",
            "Total loss:\t233.727 (rec:0.284, round:233.443)\tb=13.25\tcount=5000\n",
            "Total loss:\t210.974 (rec:0.226, round:210.748)\tb=12.12\tcount=5500\n",
            "Total loss:\t187.176 (rec:0.204, round:186.972)\tb=11.00\tcount=6000\n",
            "Total loss:\t163.077 (rec:0.185, round:162.891)\tb=9.88\tcount=6500\n",
            "Total loss:\t137.138 (rec:0.224, round:136.914)\tb=8.75\tcount=7000\n",
            "Total loss:\t110.474 (rec:0.238, round:110.236)\tb=7.62\tcount=7500\n",
            "Total loss:\t81.151 (rec:0.186, round:80.965)\tb=6.50\tcount=8000\n",
            "Total loss:\t50.495 (rec:0.216, round:50.279)\tb=5.38\tcount=8500\n",
            "Total loss:\t21.283 (rec:0.191, round:21.092)\tb=4.25\tcount=9000\n",
            "Total loss:\t3.125 (rec:0.255, round:2.870)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.275 (rec:0.207, round:0.068)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.626 (rec:0.626, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.668 (rec:0.668, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.706 (rec:0.706, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t684.101 (rec:0.620, round:683.481)\tb=20.00\tcount=2000\n",
            "Total loss:\t379.055 (rec:0.634, round:378.421)\tb=18.88\tcount=2500\n",
            "Total loss:\t344.527 (rec:0.606, round:343.921)\tb=17.75\tcount=3000\n",
            "Total loss:\t316.490 (rec:0.606, round:315.883)\tb=16.62\tcount=3500\n",
            "Total loss:\t292.266 (rec:0.586, round:291.679)\tb=15.50\tcount=4000\n",
            "Total loss:\t267.106 (rec:0.555, round:266.551)\tb=14.38\tcount=4500\n",
            "Total loss:\t243.619 (rec:0.625, round:242.994)\tb=13.25\tcount=5000\n",
            "Total loss:\t217.827 (rec:0.644, round:217.183)\tb=12.12\tcount=5500\n",
            "Total loss:\t192.950 (rec:0.607, round:192.343)\tb=11.00\tcount=6000\n",
            "Total loss:\t166.177 (rec:0.604, round:165.573)\tb=9.88\tcount=6500\n",
            "Total loss:\t139.520 (rec:0.540, round:138.980)\tb=8.75\tcount=7000\n",
            "Total loss:\t110.746 (rec:0.569, round:110.177)\tb=7.62\tcount=7500\n",
            "Total loss:\t81.532 (rec:0.620, round:80.912)\tb=6.50\tcount=8000\n",
            "Total loss:\t50.833 (rec:0.587, round:50.246)\tb=5.38\tcount=8500\n",
            "Total loss:\t21.937 (rec:0.612, round:21.325)\tb=4.25\tcount=9000\n",
            "Total loss:\t4.528 (rec:0.628, round:3.900)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.720 (rec:0.580, round:0.140)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.367 (rec:0.367, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.367 (rec:0.367, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.358 (rec:0.358, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2148.409 (rec:0.375, round:2148.033)\tb=20.00\tcount=2000\n",
            "Total loss:\t1108.948 (rec:0.379, round:1108.569)\tb=18.88\tcount=2500\n",
            "Total loss:\t1006.456 (rec:0.402, round:1006.054)\tb=17.75\tcount=3000\n",
            "Total loss:\t928.531 (rec:0.389, round:928.142)\tb=16.62\tcount=3500\n",
            "Total loss:\t857.956 (rec:0.367, round:857.589)\tb=15.50\tcount=4000\n",
            "Total loss:\t789.320 (rec:0.377, round:788.943)\tb=14.38\tcount=4500\n",
            "Total loss:\t718.901 (rec:0.352, round:718.549)\tb=13.25\tcount=5000\n",
            "Total loss:\t645.385 (rec:0.394, round:644.991)\tb=12.12\tcount=5500\n",
            "Total loss:\t568.414 (rec:0.380, round:568.034)\tb=11.00\tcount=6000\n",
            "Total loss:\t486.668 (rec:0.352, round:486.316)\tb=9.88\tcount=6500\n",
            "Total loss:\t400.065 (rec:0.398, round:399.667)\tb=8.75\tcount=7000\n",
            "Total loss:\t311.069 (rec:0.355, round:310.714)\tb=7.62\tcount=7500\n",
            "Total loss:\t218.599 (rec:0.381, round:218.218)\tb=6.50\tcount=8000\n",
            "Total loss:\t125.245 (rec:0.397, round:124.848)\tb=5.38\tcount=8500\n",
            "Total loss:\t43.202 (rec:0.386, round:42.816)\tb=4.25\tcount=9000\n",
            "Total loss:\t3.942 (rec:0.362, round:3.579)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.395 (rec:0.366, round:0.029)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.620 (rec:0.620, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.631 (rec:0.631, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.621 (rec:0.621, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t2749.176 (rec:0.581, round:2748.595)\tb=20.00\tcount=2000\n",
            "Total loss:\t1398.063 (rec:0.604, round:1397.459)\tb=18.88\tcount=2500\n",
            "Total loss:\t1270.574 (rec:0.573, round:1270.002)\tb=17.75\tcount=3000\n",
            "Total loss:\t1174.011 (rec:0.602, round:1173.408)\tb=16.62\tcount=3500\n",
            "Total loss:\t1085.147 (rec:0.566, round:1084.581)\tb=15.50\tcount=4000\n",
            "Total loss:\t995.740 (rec:0.550, round:995.189)\tb=14.38\tcount=4500\n",
            "Total loss:\t906.167 (rec:0.569, round:905.598)\tb=13.25\tcount=5000\n",
            "Total loss:\t813.575 (rec:0.602, round:812.974)\tb=12.12\tcount=5500\n",
            "Total loss:\t718.383 (rec:0.586, round:717.797)\tb=11.00\tcount=6000\n",
            "Total loss:\t617.944 (rec:0.576, round:617.368)\tb=9.88\tcount=6500\n",
            "Total loss:\t512.607 (rec:0.560, round:512.047)\tb=8.75\tcount=7000\n",
            "Total loss:\t401.659 (rec:0.591, round:401.068)\tb=7.62\tcount=7500\n",
            "Total loss:\t287.084 (rec:0.614, round:286.470)\tb=6.50\tcount=8000\n",
            "Total loss:\t170.157 (rec:0.602, round:169.555)\tb=5.38\tcount=8500\n",
            "Total loss:\t60.455 (rec:0.677, round:59.778)\tb=4.25\tcount=9000\n",
            "Total loss:\t4.889 (rec:0.572, round:4.317)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.644 (rec:0.600, round:0.044)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.436 (rec:0.436, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.394 (rec:0.394, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.414 (rec:0.414, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t8625.183 (rec:0.385, round:8624.798)\tb=20.00\tcount=2000\n",
            "Total loss:\t4323.979 (rec:0.392, round:4323.586)\tb=18.88\tcount=2500\n",
            "Total loss:\t3941.125 (rec:0.385, round:3940.740)\tb=17.75\tcount=3000\n",
            "Total loss:\t3663.495 (rec:0.392, round:3663.103)\tb=16.62\tcount=3500\n",
            "Total loss:\t3402.628 (rec:0.408, round:3402.220)\tb=15.50\tcount=4000\n",
            "Total loss:\t3140.829 (rec:0.409, round:3140.420)\tb=14.38\tcount=4500\n",
            "Total loss:\t2873.242 (rec:0.402, round:2872.840)\tb=13.25\tcount=5000\n",
            "Total loss:\t2591.697 (rec:0.402, round:2591.294)\tb=12.12\tcount=5500\n",
            "Total loss:\t2293.826 (rec:0.388, round:2293.438)\tb=11.00\tcount=6000\n",
            "Total loss:\t1980.200 (rec:0.385, round:1979.815)\tb=9.88\tcount=6500\n",
            "Total loss:\t1648.353 (rec:0.386, round:1647.967)\tb=8.75\tcount=7000\n",
            "Total loss:\t1298.827 (rec:0.396, round:1298.431)\tb=7.62\tcount=7500\n",
            "Total loss:\t935.351 (rec:0.417, round:934.934)\tb=6.50\tcount=8000\n",
            "Total loss:\t565.927 (rec:0.398, round:565.528)\tb=5.38\tcount=8500\n",
            "Total loss:\t196.175 (rec:0.395, round:195.780)\tb=4.25\tcount=9000\n",
            "Total loss:\t12.189 (rec:0.382, round:11.807)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.477 (rec:0.398, round:0.078)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t0.607 (rec:0.607, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.614 (rec:0.614, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.519 (rec:0.519, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t11066.537 (rec:0.538, round:11065.999)\tb=20.00\tcount=2000\n",
            "Total loss:\t5448.221 (rec:0.598, round:5447.623)\tb=18.88\tcount=2500\n",
            "Total loss:\t4960.604 (rec:0.570, round:4960.034)\tb=17.75\tcount=3000\n",
            "Total loss:\t4600.480 (rec:0.560, round:4599.919)\tb=16.62\tcount=3500\n",
            "Total loss:\t4258.399 (rec:0.531, round:4257.868)\tb=15.50\tcount=4000\n",
            "Total loss:\t3914.660 (rec:0.557, round:3914.103)\tb=14.38\tcount=4500\n",
            "Total loss:\t3565.045 (rec:0.585, round:3564.460)\tb=13.25\tcount=5000\n",
            "Total loss:\t3203.501 (rec:0.558, round:3202.943)\tb=12.12\tcount=5500\n",
            "Total loss:\t2824.065 (rec:0.566, round:2823.499)\tb=11.00\tcount=6000\n",
            "Total loss:\t2425.780 (rec:0.556, round:2425.223)\tb=9.88\tcount=6500\n",
            "Total loss:\t2004.935 (rec:0.594, round:2004.342)\tb=8.75\tcount=7000\n",
            "Total loss:\t1566.871 (rec:0.566, round:1566.304)\tb=7.62\tcount=7500\n",
            "Total loss:\t1124.349 (rec:0.559, round:1123.789)\tb=6.50\tcount=8000\n",
            "Total loss:\t684.747 (rec:0.578, round:684.169)\tb=5.38\tcount=8500\n",
            "Total loss:\t264.952 (rec:0.543, round:264.410)\tb=4.25\tcount=9000\n",
            "Total loss:\t23.894 (rec:0.561, round:23.332)\tb=3.12\tcount=9500\n",
            "Total loss:\t0.872 (rec:0.561, round:0.311)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 0\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t1.006 (rec:1.006, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t0.885 (rec:0.885, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t0.928 (rec:0.928, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t34603.574 (rec:0.829, round:34602.746)\tb=20.00\tcount=2000\n",
            "Total loss:\t17644.959 (rec:0.880, round:17644.078)\tb=18.88\tcount=2500\n",
            "Total loss:\t16120.008 (rec:0.880, round:16119.128)\tb=17.75\tcount=3000\n",
            "Total loss:\t14996.073 (rec:0.842, round:14995.231)\tb=16.62\tcount=3500\n",
            "Total loss:\t13930.341 (rec:0.941, round:13929.399)\tb=15.50\tcount=4000\n",
            "Total loss:\t12855.846 (rec:0.862, round:12854.984)\tb=14.38\tcount=4500\n",
            "Total loss:\t11751.292 (rec:0.928, round:11750.364)\tb=13.25\tcount=5000\n",
            "Total loss:\t10598.317 (rec:0.802, round:10597.515)\tb=12.12\tcount=5500\n",
            "Total loss:\t9388.929 (rec:0.768, round:9388.161)\tb=11.00\tcount=6000\n",
            "Total loss:\t8113.392 (rec:0.809, round:8112.583)\tb=9.88\tcount=6500\n",
            "Total loss:\t6764.678 (rec:0.795, round:6763.883)\tb=8.75\tcount=7000\n",
            "Total loss:\t5351.026 (rec:0.833, round:5350.193)\tb=7.62\tcount=7500\n",
            "Total loss:\t3904.795 (rec:0.749, round:3904.046)\tb=6.50\tcount=8000\n",
            "Total loss:\t2469.499 (rec:0.871, round:2468.627)\tb=5.38\tcount=8500\n",
            "Total loss:\t1118.848 (rec:0.846, round:1118.003)\tb=4.25\tcount=9000\n",
            "Total loss:\t163.570 (rec:0.874, round:162.696)\tb=3.12\tcount=9500\n",
            "Total loss:\t4.264 (rec:0.895, round:3.370)\tb=2.00\tcount=10000\n",
            "Reconstruction for block 1\n",
            "Init alpha to be FP32\n",
            "Init alpha to be FP32\n",
            "Total loss:\t38.121 (rec:38.121, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t34.828 (rec:34.828, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t37.835 (rec:37.835, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t44602.727 (rec:35.930, round:44566.797)\tb=20.00\tcount=2000\n",
            "Total loss:\t28875.234 (rec:36.242, round:28838.992)\tb=18.88\tcount=2500\n",
            "Total loss:\t26851.262 (rec:36.173, round:26815.090)\tb=17.75\tcount=3000\n",
            "Total loss:\t25443.223 (rec:30.906, round:25412.316)\tb=16.62\tcount=3500\n",
            "Total loss:\t24176.072 (rec:32.416, round:24143.656)\tb=15.50\tcount=4000\n",
            "Total loss:\t22922.861 (rec:34.831, round:22888.031)\tb=14.38\tcount=4500\n",
            "Total loss:\t21632.365 (rec:40.904, round:21591.461)\tb=13.25\tcount=5000\n",
            "Total loss:\t20242.805 (rec:32.343, round:20210.463)\tb=12.12\tcount=5500\n",
            "Total loss:\t18772.844 (rec:33.340, round:18739.504)\tb=11.00\tcount=6000\n",
            "Total loss:\t17169.129 (rec:31.005, round:17138.123)\tb=9.88\tcount=6500\n",
            "Total loss:\t15416.000 (rec:29.731, round:15386.270)\tb=8.75\tcount=7000\n",
            "Total loss:\t13465.798 (rec:32.038, round:13433.760)\tb=7.62\tcount=7500\n",
            "Total loss:\t11275.514 (rec:31.955, round:11243.559)\tb=6.50\tcount=8000\n",
            "Total loss:\t8825.719 (rec:31.026, round:8794.693)\tb=5.38\tcount=8500\n",
            "Total loss:\t6098.391 (rec:28.792, round:6069.600)\tb=4.25\tcount=9000\n",
            "Total loss:\t3189.006 (rec:29.731, round:3159.275)\tb=3.12\tcount=9500\n",
            "Total loss:\t743.710 (rec:31.294, round:712.416)\tb=2.00\tcount=10000\n",
            "Reconstruction for layer fc\n",
            "Init alpha to be FP32\n",
            "Total loss:\t47.103 (rec:47.103, round:0.000)\tb=0.00\tcount=500\n",
            "Total loss:\t42.013 (rec:42.013, round:0.000)\tb=0.00\tcount=1000\n",
            "Total loss:\t49.947 (rec:49.947, round:0.000)\tb=0.00\tcount=1500\n",
            "Total loss:\t4871.191 (rec:45.167, round:4826.024)\tb=20.00\tcount=2000\n",
            "Total loss:\t3178.030 (rec:48.546, round:3129.484)\tb=18.88\tcount=2500\n",
            "Total loss:\t2957.553 (rec:47.657, round:2909.896)\tb=17.75\tcount=3000\n",
            "Total loss:\t2800.592 (rec:46.164, round:2754.428)\tb=16.62\tcount=3500\n",
            "Total loss:\t2665.595 (rec:52.613, round:2612.982)\tb=15.50\tcount=4000\n",
            "Total loss:\t2521.997 (rec:48.807, round:2473.189)\tb=14.38\tcount=4500\n",
            "Total loss:\t2375.510 (rec:48.155, round:2327.355)\tb=13.25\tcount=5000\n",
            "Total loss:\t2217.581 (rec:45.001, round:2172.580)\tb=12.12\tcount=5500\n",
            "Total loss:\t2057.107 (rec:49.213, round:2007.894)\tb=11.00\tcount=6000\n",
            "Total loss:\t1875.919 (rec:46.616, round:1829.303)\tb=9.88\tcount=6500\n",
            "Total loss:\t1680.311 (rec:44.932, round:1635.378)\tb=8.75\tcount=7000\n",
            "Total loss:\t1464.900 (rec:41.516, round:1423.385)\tb=7.62\tcount=7500\n",
            "Total loss:\t1235.214 (rec:46.995, round:1188.219)\tb=6.50\tcount=8000\n",
            "Total loss:\t974.873 (rec:49.419, round:925.454)\tb=5.38\tcount=8500\n",
            "Total loss:\t680.183 (rec:42.765, round:637.419)\tb=4.25\tcount=9000\n",
            "Total loss:\t380.980 (rec:47.856, round:333.124)\tb=3.12\tcount=9500\n",
            "Total loss:\t125.571 (rec:48.002, round:77.568)\tb=2.00\tcount=10000\n",
            "Test: [  0/782]\tTime  9.825 ( 9.825)\tAcc@1  71.88 ( 71.88)\tAcc@5  89.06 ( 89.06)\n",
            "Test: [100/782]\tTime  0.111 ( 0.217)\tAcc@1  67.19 ( 67.25)\tAcc@5  85.94 ( 87.64)\n",
            "Test: [200/782]\tTime  0.106 ( 0.172)\tAcc@1  71.88 ( 67.96)\tAcc@5  87.50 ( 87.84)\n",
            "Test: [300/782]\tTime  0.064 ( 0.158)\tAcc@1  67.19 ( 67.83)\tAcc@5  89.06 ( 87.98)\n",
            "Test: [400/782]\tTime  0.069 ( 0.151)\tAcc@1  68.75 ( 68.07)\tAcc@5  84.38 ( 88.06)\n",
            "Test: [500/782]\tTime  0.063 ( 0.148)\tAcc@1  65.62 ( 68.20)\tAcc@5  89.06 ( 88.16)\n",
            "Test: [600/782]\tTime  0.064 ( 0.146)\tAcc@1  64.06 ( 68.22)\tAcc@5  79.69 ( 88.18)\n",
            "Test: [700/782]\tTime  0.061 ( 0.145)\tAcc@1  60.94 ( 68.23)\tAcc@5  90.62 ( 88.22)\n",
            " * Acc@1 68.246 Acc@5 88.258\n",
            "Full quantization (W4A4) accuracy: 68.2459945678711\n"
          ]
        }
      ],
      "source": [
        "%run -i main_imagenet.py --data_path D:\\data\\ILSVRC\\Data\\CLS-LOC --arch resnet18 --n_bits_w 4  --channel_wise --n_bits_a 4  --act_quant --order together --wwq --waq --awq --aaq \\\n",
        "--weight 0.01 --input_prob 0.5 --prob 0.5 --batch_size 64 --keep_cpu --iters_w 10000 --quantize_clipped"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml-course-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "b2d9c01f2b82f2d13cd87b7a6e7c575ae2d1fb70678e14201d3043ea189d77ff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
