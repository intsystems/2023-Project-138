\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{color}
\usepackage{xcolor}

\newcommand{\TODO}[1]{\textcolor{purple}{ToDo: #1.}}


\title{Post Training Quantization. Flexible continuous modification for SOTA post training quantization methods to make them lossless.}

\author{ Zharikov Ilya \\
        Moscow Institute of Physics and Technology
	%% examples of more authors
	\And
	  Sedova Anna \\
	Moscow Institute of Physics and Technology\\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Neural network quantization gives the opportunity to inference large models on resource constrained devices. Post-Training Quantization(PTQ) methods have became popular, as they are simple and fast to use. They do not require whole model retraining and use only small calibration set to calculate quantization parameters. However, these methods show significant accuracy decrease on low-bit setting. There are methods that allow to increase the accuracy of model by increasing its computational complexity. In this paper, we propose a continuous modification for these methods and find a reasonable trade-off between computational complexity and performance.
\end{abstract}


\keywords{Deep Learning \and Model Compression \and Post-Training Quantization}

\section{Introduction}

\TODO{Need to be extended in terms related works further}

Deep Neural Networks~(DNN) are applicable to wide range of tasks nowadays. Despite showing the great performance on these tasks, state-of-the-art models require high computational resources. There is a need to run large models on power-limited devices such as smartphones. Many different methods were proposed for model compression. In this paper, we concentrate on quantization method.

The quantization is a process of  mapping real numbers to the low-precision discrete values. There are two main types of quantization methods: quantization aware training~(QAT) and post-training quantization~(PTQ). Quantization-aware training shows great results, however, it requires the whole model retraining. Hence, this method is not applicable in some real-life cases, as if training data is not available or computational resources are limited. Unlike QAT, post-training quantization usually uses only an unlabeled calibration set for setting up quantization. Current post-training quantization methods are not such efficient as quantization aware training. However, post-training quantization is a promising technique and therefore should be explored further.

The goal of post-training quantization is to find optimal quantization parameters having only small set of data. The main problem of this technique is that quantization errors of layers can be amplified by deeper layers.  Quantization errors can accumulate layer by layer and lead to accuracy degradation. Quantization accuracy degradation is explored in the paper~\cite{loss-aware-quantization} article, which explaines why  low-bit post-training quantization is a quite challenging task.  

Most of post-training quantization methods quantize model parameters and data by minimizing the error between quantized and the original model layers outputs. The recent post-training quantization techniques  ~\citep{adaquant, BreQ} made a progress towards low-bit post-training quantization, considering previous layers errors during quantization. However, these methods leave model structure 
without changes and don't consider improving accuracy of quantized model by complicating its structure.  

In this work, we study ways to improve quantized model accuracy by making model more complex. Paper~\citep{multiple_points} uses the idea of approximating model weights as a sum of low-precision values. Our paper suggests a modification to this method.  There are two main goals of this work. Firstly, we would like to propose a method to make post-training quantization lossless. This is relevant to situations when computational device support only low bit data types. Second approach of this paper is to find a trade-off between model complexity and quantization bits, allowing to compress model for resource constrained devices.  


\section{Problem statement}

\TODO{Need to be slightly reformulated during theory week}

\label{sec:headings}
In this article, we use uniform quantization. Given value to quantize $v$ , the maximum and minimum quantization value $Q_{max}$ and $Q_{min}$ and quantization step size $\alpha$, quantizer computes integer representation of a data $\bar{v}$:
$$
 \bar{v} =
\begin{cases}
    -Q_{min}, \text{if} \frac{v}{\alpha} \leq -Q_{min}\\
    \lfloor\frac{v}{\alpha}\rceil, \text{if} \frac{v}{\alpha} \in [-Q_{min}, Q_{max}]\\
    Q_{max}, \text{if} \frac{v}{\alpha} \geq Q_{max}\\
\end{cases}.
$$
To get representation of the same scale, $\bar{v}$ is multiplied by $\alpha$:
$$
\hat{v} = \bar{v} * \alpha.
$$
Let's suppose that model has $n$ parameters $W_1, ..., W_n $, then let's denote the model consisting of these parameters as $M(W_1, ..., W_n)$. 
Also let quantized model parameters be denoted as $Q(W_1, \alpha_1), ..., Q(W_n, \alpha_n)$.

The goal of our work is to quantize model M without significant performance degradation. We will achieve this by making  outputs of $M(Q(W_1, \alpha_1), ..., Q(W_n, \alpha_n))$ similar to the outputs of $M(W_1, ..., W_n )$.
Let's denote model M complexity as $P(M)$, model quality as $F(M)$.

Then, we want to maximize $F(M(Q(W_1, \alpha_1), ..., Q(W_n, \alpha_n))) $  for given model $M(W_1, ..., W_n)$ and some complexity limit $P_0$:

$\arg\max\limits_{\alpha_1, ..., \alpha_n} \{F(M(Q(W_1, \alpha_1), ..., Q(W_n, \alpha_n)))\ , P(M(Q(W_1, \alpha_1), ..., Q(W_n, \alpha_n))) \leq P_0\}$






\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
